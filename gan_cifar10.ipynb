{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook Configuration\n",
    "GPU_COUNT = 1\n",
    "WINDOWS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas\n",
    "!pip install -q seaborn\n",
    "!pip install -q matplotlib\n",
    "!pip install -q neptune-client"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cifar10.PNG\" width=700>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks - CIFAR10\n",
    "> Can we create a strong generative model for CIFAR10 using the GAN architecture?\n",
    "\n",
    "## Background Research\n",
    "\n",
    "### Introduction\n",
    "The ultimate goal of Deep Learning is to be able to create a function that can <strong>effectively</strong> model any form of data distribution. History has time and time again displayed the impressive success of discriminators, models that learn to divide the data distribution/map a high dimensional vector to one that is lower (Goodfellow et al., 2014). \n",
    "\n",
    "<ul>\n",
    "\t<li>Generative Adversarial Networks</li>\n",
    "\t<li>Diffusion Models</li>\n",
    "\t<li>Variational AutoEncoders</li>\n",
    "</ul>\n",
    "\n",
    "### The makings of GANs\n",
    "\n",
    "### Types of GANs\n",
    "\n",
    "### The difficulty with GANs\n",
    "Mode collapse\n",
    "Stability - hyperparamter tuning is very important versus discriminator models where hyperparameters more often than not determine .\n",
    "\n",
    "### Loss Functions\n",
    "Different Loss functions, main one being KL Divergence.\n",
    "Explain how KL Divergence works.\n",
    "There are papers claiming that novel loss functions improve stability.\n",
    "There are also papers https://arxiv.org/abs/1811.09567 that claim that loss functions don't really matter.\n",
    "This is an area of research I will attempt to explore in the notebook as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing the GAN\n",
    "### Objectives\n",
    "<ol>\n",
    "\t<li>Explore the CIFAR10 dataset</li>\n",
    "\t<li>Implement and evaluate to find the best performing model</li>\n",
    "\t<li>Analyse the final model</li>\n",
    "</ol>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "We define some utility functions below that will ease and help us with our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc_data(data, loc):\n",
    "\tdatacopy = copy.deepcopy(data)\n",
    "\tarr = np.array(datacopy.loc[loc].drop('label'))\n",
    "\tlabel = datacopy.loc[loc]['label']\n",
    "\troot = int(len(arr) ** 0.5)\n",
    "\tarr.resize((root, root))\n",
    "\treturn label, arr\n",
    "\n",
    "def imshow(arr: list, label: list = None, figsize=None, shape = (32, 32, 3), is_int = None):\n",
    "\tif is_int == None:\n",
    "\t\tif type(arr[0]) == torch.Tensor:\n",
    "\t\t\tis_int = (arr[0].detach().cpu().numpy() > 1).sum() > 0\n",
    "\t\telse:\n",
    "\t\t\tis_int = (arr[0] > 1).sum() > 0\n",
    "\tif label == None:\n",
    "\t\tlabel = [''] * len(arr)\n",
    "\n",
    "\theight = int(len(arr) ** 0.5)\n",
    "\twidth = math.ceil(len(arr) / height)\n",
    "\n",
    "\tif figsize == None:\n",
    "\t\tfig = plt.figure()\n",
    "\telse:\n",
    "\t\tfig = plt.figure(figsize=figsize)\n",
    "\tfor i in range(height):\n",
    "\t\tfor j in range(width):\n",
    "\t\t\tax = fig.add_subplot(height, width, i * height + j + 1)\n",
    "\t\t\tax.grid(False)\n",
    "\t\t\tax.set_xticks([])\n",
    "\t\t\tax.set_yticks([])\n",
    "\t\t\tshow = arr[i * height + j]\n",
    "\t\t\tif type(arr[i * height + j]) != torch.Tensor:\n",
    "\t\t\t\tshow = torch.Tensor(show)\n",
    "\t\t\t\t# ax.imshow((arr[i * height + j].squeeze(0).cpu().permute(1, 2, 0) / 255).type(torch.uint8 if is_int else float))\n",
    "\t\t\t# if (show.shape[0] == 1):\n",
    "\t\t\t# \tax.imshow((show.squeeze(0).cpu()).type(torch.uint8 if is_int else torch.float), cmap='gray')\n",
    "\t\t\t# else:\n",
    "\t\t\tif len(show.squeeze(0).cpu().shape) == 2:\n",
    "\t\t\t\tax.imshow((show.squeeze(0).detach().cpu()).type(torch.uint8 if is_int else torch.float), cmap='gray')\n",
    "\t\t\telse:\n",
    "\t\t\t\tax.imshow((show.squeeze(0).detach().cpu().permute(1,2,0)).type(torch.uint8 if is_int else torch.float))\n",
    "\t\t\tax.set_title(label[i * height + j])\n",
    "\n",
    "def df_to_tensor(df, shape = (28, 28)):\n",
    "\treturn torch.tensor(df.values.reshape((-1, *shape)), dtype=torch.float32)\n",
    "\n",
    "def preprocess(df):\n",
    "\treturn df.copy() / 255\n",
    "\n",
    "def mse(t1, t2, shape=(28, 28)):\n",
    "\tloss = nn.MSELoss(reduction='none')\n",
    "\tloss_result = torch.sum(loss(t1, t2), dim=2)\n",
    "\tloss_result = torch.sum(loss_result, dim=2)\n",
    "\tloss_result = loss_result / np.prod([*shape])\n",
    "\treturn loss_result"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
