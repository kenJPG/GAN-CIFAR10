{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== #\n",
    "# Author: Kenneth Chen #\n",
    "# Student ID: 2100072  #\n",
    "# ==================== #\n",
    "\n",
    "# Notebook Configuration\n",
    "GPU_COUNT = 1\n",
    "WINDOWS = True\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch-fidelity in c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages (from torch-fidelity) (9.2.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages (from torch-fidelity) (0.14.0)\n",
      "Requirement already satisfied: torch in c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages (from torch-fidelity) (1.13.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages (from torch-fidelity) (4.64.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages (from torch-fidelity) (1.23.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages (from torch-fidelity) (1.9.3)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages (from torch->torch-fidelity) (4.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages (from torchvision->torch-fidelity) (2.28.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages (from tqdm->torch-fidelity) (0.4.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages (from requests->torchvision->torch-fidelity) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages (from requests->torchvision->torch-fidelity) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages (from requests->torchvision->torch-fidelity) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages (from requests->torchvision->torch-fidelity) (2.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages (from torchmetrics) (1.23.4)\n",
      "Requirement already satisfied: torch>=1.8.1 in c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages (from torchmetrics) (1.13.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages (from torchmetrics) (21.3)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages (from torch>=1.8.1->torchmetrics) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages (from packaging->torchmetrics) (3.0.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\p2100072\\.conda\\envs\\pytorch\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pandas\n",
    "!pip install -q seaborn\n",
    "!pip install -q matplotlib\n",
    "!pip install -q neptune-client\n",
    "!pip install torch-fidelity\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cifar10.PNG\" width=700>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References and Credits\n",
    "I would like to attribute a lot of credit towards OpenAI and StudioGAN for providing quite a bit of base reference code to work with. Nevertheless, I suffered way too many headaches trying to adapt their python code structure into a Jupyter Notebook format. <a href=\"https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/\">GitHub</a>. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks - CIFAR10\n",
    "> Can we create a strong generative model for CIFAR10 using the GAN architecture?\n",
    "\n",
    "## Background Research üìñ\n",
    "\n",
    "### Introduction üí°\n",
    "The ultimate goal of Deep Learning is to be able to create a function that can <strong>effectively model any form of data distribution</strong>. History has time and time again displayed the impressive success of discriminators, models that learn to divide the data distribution/map a high dimensional vector to one that is lower (Goodfellow et al., 2014). For instance, tasks such as Image Classification are one type of discriminative modelling, as the high dimensional images are mapped into low dimensional probabilities of labels.\n",
    "\n",
    "What about generative modelling? In generative modelling, the goal is instead given a data distribution to learn from, to produce or <strong>generate new examples</strong> that follow this distribution but still aim to be unique. Thus, a high performing generative model should be able to create examples that are both <strong>plausible</strong> (in that one can recognize what the generated example is supposed to be of) <strong>and indistinguishable</strong> from real data examples (Brownlee, 2019). Generative models can be Unsupervised or Semi-Supervised, depending on the exact task that one is trying to tackle. \n",
    "\n",
    "There are different approaches to network architectures when it comes to trying to achieve Generative Models:\n",
    "<ul>\n",
    "\t<li>Generative Adversarial Networks (GAN) </li>\n",
    "\t<li>Diffusion Models</li>\n",
    "\t<li>Variational Auto Encoders (VAE)</li>\n",
    "</ul>\n",
    "\n",
    "<strong>GANs ‚öîÔ∏è</strong> <br />\n",
    "GANs are the main focus of this notebook. Proposed by Ian Goodfellow in 2014, it became one of the more popular types of Generative Models used. For instance, the commonly known website <a href=\"thispersondoesnotexist.com\">thispersondoesnotexist.com</a> uses the StyleGAN2 architecture (Karras et al., 2020), to generate high fidelity images of humans. The idea for GANs is that <strong>there are two networks that work against each other in a game</strong>, where they try to one up each other. Thus, this leads to improvement in both networks. More details are discussed under \"What's inside a GAN? üîç\".\n",
    "\n",
    "<strong>Diffusion Models ‚ú®</strong><br /> \n",
    "These are the models that have been not only been successful, but widely popular as well. For instance, OpenAI Dall-E, Google Imagen, Stable Diffusion, Midjourney are models that fall under the category of Diffusion Models. (Muppalla and Hendryx, 2022). From a high level, it works like so:\n",
    "<ul>\n",
    "\t<li>Noise is added to original images</li>\n",
    "\t<li>Noise is procedurally added until image is all noise</li>\n",
    "\t<li>The model then learns to remove the noise</li>\n",
    "\t<li>Guidance can be added in the form of e.g. text-to-image, to provide direction of the generation process</li>\n",
    "</ul>\n",
    "\n",
    "<img src=\"images/diffusion1.jpg\" width=400/><br />\n",
    "*Noise is procedurally added to the image. Image Credit: (Muppalla and Hendryx, 2022)*\n",
    "\n",
    "<img src=\"images/diffusion2.jpg\" width=400/><br />\n",
    "*Model attempts to recreate the image. Image Credit: (Muppalla and Hendryx, 2022)*\n",
    "\n",
    "<strong>Variational Auto Encoders üé≤</strong><br />\n",
    "We first take a look at what Auto Encoders are. An Auto Encoder is trained for it to learn to copy the input to the output (Goodfellow, Bengio and Courville, 2016). This is done by having an <strong>encoder map the image</strong> to a compressed representation of the image (the inner nodes), to which the decoder <strong>uses this compressed representation</strong> to generate an image similar to the original. Note that the trick is to <strong>restrict the number of inner nodes</strong> inside the network, such that it is <strong>not able to generate a 1-to-1 copy</strong>. This way, it's forced to learn the most promiment of features to recognize, using the limited number of nodes.\n",
    "\n",
    "The idea with Variational Auto Encoders is that <strong>instead of the encoder just mapping the image to a compressed representation</strong> (aka latent vector), we instead <strong>learn the distribution that the latent vector can take on</strong>. Using this, we can then randomly sample from the learned latent distribution, for the decoder to give us a newly and controlled generated image.\n",
    "\n",
    "<img src=\"images/vae.PNG\" width=400/><br />\n",
    "*VAE Architecture. We note the learning of the latent distribution in the form of $\\mu$ (mean) and $\\sigma$ (standard deviation). Image Credit: (Rocca, 2019)*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What's inside a GAN? üîç\n",
    "In a GAN, there are in fact two networks, a <strong>generator</strong> and a <strong>discriminator</strong> that improve each other by competing in a game scenario (Goodfellow, Bengio and Courville, 2016). The aim is to use the well established field of discriminators to assist the generator. The goal of the generator is to <strong>create realistic images</strong> that appear to be from the distribution of the training images, where as the goal of the discriminator is to determine <strong>if a given image is from the data distribution</strong>. The process goes as follows:\n",
    "<ol>\n",
    "\t<li>Generator creates images</li>\n",
    "\t<li>Discriminator learns to distinguish real vs fake from a set of real images and these newly generated images</li>\n",
    "\t<li>Using the updated Discriminator, Generator learns to trick the Discriminator</li>\n",
    "</ol>\n",
    "\n",
    "Mathematically speaking, the process of GANs can be described as so. \n",
    "Suppose we have some a random distribution or a \"prior\", which we can sample from. We can denote this as $p_z(z)$, where $z$ represents a vector of a specific size. This vector acts as the input to our generative model $G$. The model is ultimately described as $G(z; \\theta_g)$ where $\\theta_g$ represents the parameters of the generative model.\n",
    "\n",
    "On the other hand, our discriminator model $D$, takes in an input $x$, an image, which can be fully described as $D(x; \\theta_d)$, where likewise, $\\theta_d$ represents the parameters of the discriminative model. Being the discriminator, $D(x)$ returns the probability that an input $x$ is from the.\n",
    "\n",
    "This \"game\" between the two models can be thought of as trying to optimize of minimax function of:\n",
    "\n",
    "$$\\underset{G}{\\text{min}}\\underset{D}{\\text{max}} V(D, G) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} [\\text{log}D(x)] + \\mathbb{E}_{z \\sim p_{\\text{z}}(z)} [\\text{log}(1 - D(G(z)))]$$\n",
    "\n",
    "<em style=\"text: center\">(Goodfellow et al., 2014)</em>\n",
    "\n",
    "Essentially, the function $V$ takes in two inputs, our models $D$ and $G$, and returns an output that has two parts. The left hand part of the sum $\\mathbb{E}_{x \\thicksim p_{\\text{data}}(x)} [\\text{log}D(x)]$ represents \"the expected value that the discriminator model predicts real data is real\". The right hand part of the sum $\\mathbb{E}_{z \\thicksim p_{\\text{z}}(z)} [\\text{log}(1 - D(G(z)))]$ looks at given some random vector $z$, \"what is the expected value that the discriminator model predicts fake data is fake\", in that the value of the right hand part is maximum when discriminator model is successful at labeling the fake images of a generator as fake.\n",
    "\n",
    "$\\underset{G}{\\text{min}}\\underset{D}{\\text{max}}$ aims to do two things. Firstly, what is the generator model $G$ that *minimizes the value*, which means *the discriminator labels generator's images as real*. Secondly what is the disciminator model $D$ that will maximize this value, which means **discriminator model predicts real images as real and fake images as fake**. These two perfectly optimize each other, when the best approach the discriminator can take is to <strong>just guess randomly</strong> as the generator images are on the same realism as the real data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of GANs üçê\n",
    "There many different types of GANs, however I believe the most differing pair is the Vanilla GAN and the Conditional GAN. The Vanilla GAN is what was proposed by Ian Goodfellow in 2014, which consists of the basic architecture with multi-layer perceptrons (MLPs). Conditional GANs are different in the aspect that one can provide additional information to the model, which could be thought of as a form of guidance similar to diffusion models.\n",
    "\n",
    "<img src=\"images/vanilla.jpg\" width=400><br/>\n",
    "*Vanilla GAN architecture (Tewari, N.d.)*\n",
    "\n",
    "<img src=\"images/conditional.jpg\" width=400><br/>\n",
    "\n",
    "*Conditional GAN architecture (Tewari, N.d.)*\n",
    "\n",
    "We observe that there is an extra component of `y`, which represents the extra information presented both to the Generator and Discriminator. This extra information is usually in the form of class labels to allow one to possess control over the output, but it can be extended to different modal data, even something such as text (in which case it needs some sort of text processor).\n",
    "\n",
    "One may think of Conditional GAN as a more *\"supervised\" version* of Vanilla GANs, as some condition techniques (in the form of perhaps label information) are provided **to support the adversarial training**. Conditional GANs have in fact become **the go-to method of generating high-quality images** (Kang et al., 2021). This may be due to the fact that **when image sizes become larger** and there is **no sense of direction**, there can be a **significant** amount of overlapping generation that occurs.\n",
    "\n",
    "What's more, is that even among Conditional GANs, the category can be further broken down into different methods of **inputing this conditional information**. The common methods are:\n",
    "<ol>\n",
    "\t<li>Conditioning by concatenation</li>\n",
    "\t<li>Conditioning with projection discriminator</li>\n",
    "\t<li>Conditioning with auxiliary classifier</li>\n",
    "\t<li>Conditioning with batch normalization</li>\n",
    "</ol>\n",
    "\n",
    "In this notebook, we utilize and explore the above 4 techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Uses of GANs üß§\n",
    "\n",
    "As a GAN is a generative model, there are a large number of applications of GANs (Brownlee, 2019). Personally, I find it interesting how the idea of GANs can be adjusted for any modal of data, as long as the architecture for the encoder and decoders are adjusted accordingly. Here are a few areas of GANs I believe are quite intriguing:\n",
    "<ul>\n",
    "\t<li>Time Series</li>\n",
    "\t<li>Image Generation</li>\n",
    "\t<li>Music Generation</li>\n",
    "\t<li>Audio Generation</li>\n",
    "\t<li>Style Transfer (e.g. winter photo to summer photo, jazz to classical music)</li>\n",
    "</ul>\n",
    "\n",
    "Among them, I think Audio Generation stands out to me the most. The idea of using GANs in music composition sounds like a difficult challenge, but also an impressive feat if one could pull it off."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The difficulty with GANs üß©\n",
    "Despite having achieved great success at generating realistic and sharp looking images, they are still remarkably difficult to train (Arjovsky and Bottou, 2017). Different symptoms/signs may appear as a result of non-optimal training process or architecture. Below, we attempt to discuss what such that we are able to know what to pay attention to, when developing our GANs as well.\n",
    "\n",
    "#### Mode collapse\n",
    "One of the largest and continuing issues with the GAN training process is mode collapse. Mode collapse is said to occur if the generator **maps multiple different vectors** $z$ to the same output of $x$, where $x$ is a vector which the <strong>discriminator assigns high probability of being real</strong> to. In other words, the generator **exploits the discovery** that a particular output can fool the generator and thus ultimately produces, <strong>decreasing the diversity of one's outputs</strong>.\n",
    "\n",
    "#### Mode Dropping\n",
    "This is another case of decreasing the diversity of one's outputs, however not in the same way as Mode Collapse. In Mode Collapse, diversity is decreased within the classes. However, in Mode Dropping, diversity is decreased **among the classes**. Specifically, the generator may learn that for certain classes, it is difficult to generate an image, sufficiently realistic enough to trick the generator. Thus the generator may learn to simply just ignore these difficult classes, to be the most effective in the game against discriminator."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Lipschitz Continuity\n",
    "A real-valued function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ possesses the property of Lipschitz continuity if\n",
    "\n",
    "$$ |f(x_1) - f(x_2) | \\leq K|x_1 - x_2| $$\n",
    "\n",
    "where $K$ is some constant and $x$ represents the input. Intuitively, this means that when we change the value of $x$, we can expect to be <strong>relatively small</strong>, in that the change can be described as a result of linear growth rather than <strong>uncontrolled growth</strong>.\n",
    "\n",
    "**What does this mean in the context of neural networks?** Suppose we have some network that **does not** possess the property of being Lipschitz continuous (this is possible because neural networks are essentially a composition of functions). As there is no bound of $K$, it means the output of the function can grow uncontrollably and a small change in the input **can possibly lead to large changes in the output**. This is bad for GANs, as we lose our sense of control and also means that moving along one dimension may result in drastically different changes in the output.\n",
    "\n",
    "If we can enforce Lipschitz continuity in GANs, it means that a small change in the input, guarantees a relative small change in the output as well, which is what we want. One method of achieving this is to utilize a different loss function in GANs such as Wasserstein Distance. We explore approaches to tackling this issue further in the *\"Developing GANs\"* section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GAN Metrics**: what does it mean to be good?\n",
    "\n",
    "The most straight forward method of telling whether the output of a GAN is good, is to well, use our human eyes. On a smaller-sized experiment or if one is assessing a few images, this may be a valid approach. However, when the amount of data scale up to a very large quantity (which naturally is the case with Deep Learning), the act of visually inspecting every output is **simply infeasible**. Besides, using our eye as a metric means our feedback is **qualitative**. This means if it comes down to comparing the performances of two incredibly well-performing models, it's also difficult for one to tell which one is better. Thus, we need to define a set of quantitative metrics that we can use to substitute the use of our human eyes.\n",
    "\n",
    "There is no one metric that is the *\"best\"* (Borji, 2018). Each metric has its own advantages and disadvantages, but ideally an efficient evaluation metric should:\n",
    "<ol>\n",
    "\t<li>high quality</li>\n",
    "\t<li>diverse</li>\n",
    "\t<li>disentangled latent space</li>\n",
    "</ol>\n",
    "<em>(Borji, 2018)</em>\n",
    "\n",
    "What is **Disentangled Latent Space**?\n",
    "Latent space is straight forward, it's the typically smaller representation that in the context of GANs, acts as the input to the generator network. Disentangled latent space means that going along one dimension of the latent space representation, the changes reflected in the output should have only an appropriate level of overlap with outputs achieved by going along a different dimension. Thus, there are a few cases:\n",
    "\n",
    "![VAE Disentanglement](images/disentangled.JPG)<br />\n",
    "*(Mathieu et al., 2019)*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the context in Variational Auto Encoders, however as the problem statement of generator images to mimic the real distribution is the same, thus the evaluation metrics being the same as well, I use the image to explain how the concept of disentangled latent space in the context of GANs.\n",
    "\n",
    "We see there are a total of 3 possible cases:\n",
    "<ul>\n",
    "\t<li>Insufficient overlap</li>\n",
    "\t<li>Appropriate Overlap</li>\n",
    "\t<li>Too Much Overlap</li>\n",
    "</ul>\n",
    "\n",
    "In the case of insufficient overlap, we see that moving along the latent distribution $p(z)$ provides the property of independence, in that there are no two latent spaces that affect two features of the output. However, the problem is that the generated image distribution is not, does not take up much of our ideal output distribution $p_{\\theta}(x)$. In the case of too much overlap, as indicated by the large overlapping regions, it tells us that moving along one dimension of the latent space leads to a change in many different images among the distribution (which in this 2d representation can be interpretted as changing too many of the same features). Thus, the ideal or appropriate level of overlap here, as seen in the center image, is where there is a good level of independence, in that moving along one dimension of the latent space vector results to changes associated only with the latent space vector, and where the covered area is also large, taking up most of the space of our distribution $p_{\\theta}(x)$.\n",
    "\n",
    "How about the two others -- quality and diversity? \n",
    "\n",
    "##### **Measuring Quality**  \n",
    "What we mean by quality here is not the resolution of the image (doesn't make sense for the generator to output high-res images when the training data is low resolution). Here, quality means how well one is able to recognize an image, as the class it is intended to be. The two most popular metrics are **Inception Score** and **Frechet Inception Distance**. The term **inception** comes from the fact that these two metrics **actually use** a pretrained Inception classifier to aid in producing the metric values.\n",
    "<ul>\n",
    "\t<li>Inception Score <strong>(IS)</strong></li>\n",
    "\t<li>Frechet Inception Distance <strong>(FID)</strong></li>\n",
    "</ul>\n",
    "\n",
    "##### **Measuring Diversity**  \n",
    "If we simply **focused on quality-measuring metrics**, our models may still be susceptible to something like Mode Collapse or Mode Dropping. We try to aim for a quantitative metric such as:\n",
    "<ul>\n",
    "\t<li>Learned Perceptual Image Patch Similarity <strong>(LPIPS)</strong></li>\n",
    "\t<li>Number of statistically-different bins <strong>(NDB)</strong></li>\n",
    "\t<li><strong>I-Variance</strong></li>\n",
    "</ul>\n",
    "\n",
    "Further details of these metrics are discussed in later sections, when we evaluate the models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CIFAR10 Dataset\n",
    "<img src=\"images/cifar_random.png\" width=300 />\n",
    "\n",
    "As per our assignment, we are instructed to utilize the CIFAR10 dataset to **generate 1000 small colour images**. \n",
    "\n",
    "The CIFAR-10 dataset consists of **60000** `32x32` colour images in a total of 10 classes, with 6000 images per class (Krizhevsky, 2009)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing GAN üíª\n",
    "> Let's get to building our Conditional GAN.\n",
    "\n",
    "Why Conditional GANs? As mentioned during the above background research section, Conditional GANs are. On top of this, Conditional GANs allow the user to <strong>control</strong> the output, where as Va.\n",
    "\n",
    "Besides, if one appreciates the idea of not knowing what a Vanilla GAN may output, a simple random module can be attached as the conditional information to somewhat *hack* the Conditional GAN into being a normal GAN.\n",
    "\n",
    "### Objectives üñäÔ∏è\n",
    "We identify the tasks and objectives we want to meet, which will be used as a guide throughout the development of our Conditional GAN.\n",
    "<ol>\n",
    "\t<li>Explore the CIFAR10 dataset</li>\n",
    "\t<li>Implement and evaluate to find the best performing model</li>\n",
    "\t<li>Analyse the final model</li>\n",
    "</ol>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "The necessary libraries are imported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, autograd\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "import torch.distributed as dist\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import config\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from os.path import dirname, exists, join\n",
    "\n",
    "import scipy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions üî®\n",
    "We define some utility functions below that will ease and help us with our analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining **Network Components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(x):\n",
    "\treturn x\n",
    "\n",
    "def loc_data(data, loc):\n",
    "\tdatacopy = copy.deepcopy(data)\n",
    "\tarr = np.array(datacopy.loc[loc].drop('label'))\n",
    "\tlabel = datacopy.loc[loc]['label']\n",
    "\troot = int(len(arr) ** 0.5)\n",
    "\tarr.resize((root, root))\n",
    "\treturn label, arr\n",
    "\n",
    "def reshape_weight_to_matrix(weight):\n",
    "    weight_mat = weight\n",
    "    dim = 0\n",
    "    if dim != 0:\n",
    "        weight_mat = weight_mat.permute(dim, *[d for d in range(weight_mat.dim()) if d != dim])\n",
    "    height = weight_mat.size(0)\n",
    "    return weight_mat.reshape(height, -1)\n",
    "\n",
    "def make_logger(save_dir, run_name, log_output):\n",
    "    if log_output is not None:\n",
    "        run_name = log_output.split('/')[-1].split('.')[0]\n",
    "    logger = logging.getLogger(run_name)\n",
    "    logger.propagate = False\n",
    "    log_filepath = log_output if log_output is not None else join(save_dir, \"logs\", run_name + \".log\")\n",
    "\n",
    "    log_dir = dirname(log_filepath)\n",
    "    if not exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    if not logger.handlers:  # execute only if logger doesn't already exist\n",
    "        file_handler = logging.FileHandler(log_filepath, 'a', 'utf-8')\n",
    "        stream_handler = logging.StreamHandler(os.sys.stdout)\n",
    "\n",
    "        formatter = logging.Formatter('[%(levelname)s] %(asctime)s > %(message)s', datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        file_handler.setFormatter(formatter)\n",
    "        stream_handler.setFormatter(formatter)\n",
    "\n",
    "        logger.addHandler(file_handler)\n",
    "        logger.addHandler(stream_handler)\n",
    "        logger.setLevel(logging.INFO)\n",
    "    return logger\n",
    "\n",
    "def accm_values_convert_dict(list_dict, value_dict, step, interval):\n",
    "    for name, value_list in list_dict.items():\n",
    "        if step is None:\n",
    "            value_list += [value_dict[name]]\n",
    "        else:\n",
    "            try:\n",
    "                value_list[step // interval - 1] = value_dict[name]\n",
    "            except IndexError:\n",
    "                try:\n",
    "                    value_list += [value_dict[name]]\n",
    "                except:\n",
    "                    raise KeyError\n",
    "        list_dict[name] = value_list\n",
    "    return list_dict\n",
    "\n",
    "def imshow(arr: list, label: list = None, figsize = None, shape = (32, 32, 3), is_int = None, fig_shape = None):\n",
    "\tif is_int == None:\n",
    "\t\tif type(arr[0]) == torch.Tensor:\n",
    "\t\t\tis_int = (arr[0].detach().cpu().numpy() > 1).sum() > 0\n",
    "\t\telse:\n",
    "\t\t\tis_int = (arr[0] > 1).sum() > 0\n",
    "\tif len(label) == 0:\n",
    "\t\tlabel = [''] * len(arr)\n",
    "\n",
    "\tif fig_shape == None:\n",
    "\t\theight = int(len(arr) ** 0.5)\n",
    "\t\twidth = math.ceil(len(arr) / height)\n",
    "\telse:\n",
    "\t\theight = fig_shape[0]\n",
    "\t\twidth = fig_shape[1]\n",
    "\n",
    "\tX_means = np.array([0.4919, 0.4827, 0.4472])\n",
    "\tX_stds = np.array([0.2470, 0.2434, 0.2616])\n",
    "\tunnormalize = transforms.Normalize((-X_means / X_stds).tolist(), (1.0 / X_stds).tolist())\n",
    "\n",
    "\tif figsize == None:\n",
    "\t\tfig = plt.figure()\n",
    "\telse:\n",
    "\t\tfig = plt.figure(figsize=figsize)\n",
    "\tfor i in range(height):\n",
    "\t\tfor j in range(width):\n",
    "\t\t\tif (i * width + j >= len(arr)):\n",
    "\t\t\t\tbreak\n",
    "\t\t\tax = fig.add_subplot(height, width, i * width + j + 1)\n",
    "\t\t\tax.grid(False)\n",
    "\t\t\tax.set_xticks([])\n",
    "\t\t\tax.set_yticks([])\n",
    "\t\t\tshow = arr[i * width + j]\n",
    "\t\t\tif type(arr[i * width + j]) != torch.Tensor:\n",
    "\t\t\t\tshow = torch.Tensor(show)\n",
    "\t\t\t\n",
    "\t\t\tshow = (show + 1) / 2\n",
    "\t\t\tif len(show.squeeze(0).cpu().shape) == 2:\n",
    "\t\t\t\tax.imshow((show.squeeze(0).detach().cpu()).type(torch.uint8 if is_int else torch.float), cmap='gray')\n",
    "\t\t\telse:\n",
    "\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tax.imshow((show.squeeze(0).detach().cpu().permute(1,2,0)).type(torch.uint8 if is_int else torch.float))\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tax.imshow((show.squeeze(0).detach().cpu()).type(torch.uint8 if is_int else torch.float))\n",
    "\t\t\tax.set_title(label[i * width + j])\n",
    "\n",
    "\treturn fig, ax\n",
    "\n",
    "def df_to_tensor(df, shape = (32, 32)):\n",
    "\treturn torch.tensor(df.values.reshape((-1, *shape)), dtype=torch.float32)\n",
    "\n",
    "def preprocess(df):\n",
    "\treturn df.copy() / 255\n",
    "\n",
    "def count_parameters(module):\n",
    "    return \"Number of parameters: {num}\".format(num=sum([p.data.nelement() for p in module.parameters()]))\n",
    "\n",
    "# Credits to StudioGAN for function reference\n",
    "def set_deterministic_op_trainable(m):\n",
    "\tif isinstance(m, torch.nn.modules.conv.Conv2d):\n",
    "\t\tm.train()\n",
    "\tif isinstance(m, torch.nn.modules.conv.ConvTranspose2d):\n",
    "\t\tm.train()\n",
    "\tif isinstance(m, torch.nn.modules.linear.Linear):\n",
    "\t\tm.train()\n",
    "\tif isinstance(m, torch.nn.modules.Embedding):\n",
    "\t\tm.train()\n",
    "\n",
    "def untrack_bn_statistics(m):\n",
    "\tif isinstance(m, torch.nn.modules.batchnorm._BatchNorm):\n",
    "\t\tm.track_running_stats = False\n",
    "\n",
    "def track_bn_statistics(m):\n",
    "\tif isinstance(m, torch.nn.modules.batchnorm._BatchNorm):\n",
    "\t\tm.track_running_stats = True\n",
    "\n",
    "class dummy_context_mgr():\n",
    "\tdef __enter__(self):\n",
    "\t\treturn None\n",
    "\n",
    "\tdef __exit__(self, exc_type, exc_value, traceback):\n",
    "\t\treturn False\n",
    "\n",
    "\n",
    "def sample_y(y_sampler, batch_size, num_classes, device):\n",
    "\tif y_sampler == \"totally_random\":\n",
    "\t\ty_fake = torch.randint(low=0, high=num_classes, size=(batch_size, ), dtype=torch.long, device=device)\n",
    "\n",
    "\telif y_sampler == \"acending_some\":\n",
    "\t\tassert batch_size % 8 == 0, \"The size of batches should be a multiple of 8.\"\n",
    "\t\tnum_classes_plot = batch_size // 8\n",
    "\t\tindices = np.random.permutation(num_classes)[:num_classes_plot]\n",
    "\n",
    "\telif y_sampler == \"acending_all\":\n",
    "\t\tbatch_size = num_classes * 8\n",
    "\t\tindices = [c for c in range(num_classes)]\n",
    "\n",
    "\telif isinstance(y_sampler, int):\n",
    "\t\ty_fake = torch.tensor([y_sampler] * batch_size, dtype=torch.long).to(device)\n",
    "\telse:\n",
    "\t\ty_fake = None\n",
    "\n",
    "\tif y_sampler in [\"acending_some\", \"acending_all\"]:\n",
    "\t\ty_fake = []\n",
    "\t\tfor idx in indices:\n",
    "\t\t\ty_fake += [idx] * 8\n",
    "\t\ty_fake = torch.tensor(y_fake, dtype=torch.long).to(device)\n",
    "\treturn y_fake\n",
    "\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "def truncated_normal(size, threshold=1.):\n",
    "\tvalues = truncnorm.rvs(-threshold, threshold, size=size)\n",
    "\treturn values\n",
    "\n",
    "def sample_normal(batch_size, z_dim, truncation_factor, device):\n",
    "\tif truncation_factor == -1.0:\n",
    "\t\tlatents = torch.randn(batch_size, z_dim, device=device)\n",
    "\telif truncation_factor > 0:\n",
    "\t\tlatents = torch.FloatTensor(truncated_normal([batch_size, z_dim], truncation_factor)).to(device)\n",
    "\telse:\n",
    "\t\traise ValueError(\"truncated_factor must be positive.\")\n",
    "\treturn latents\n",
    "\n",
    "def sample_zy(z_prior, batch_size, z_dim, num_classes, truncation_factor, y_sampler, radius, device, random_gen = None):\n",
    "\tfake_labels = sample_y(y_sampler=y_sampler, batch_size=batch_size, num_classes=num_classes, device=device)\n",
    "\tbatch_size = fake_labels.shape[0]\n",
    "\n",
    "\n",
    "\tif z_prior == \"gaussian\":\n",
    "\t\tzs = sample_normal(batch_size=batch_size, z_dim=z_dim, truncation_factor=truncation_factor, device=device)\n",
    "\telif z_prior == \"uniform\":\n",
    "\t\tzs = torch.FloatTensor(batch_size, z_dim).uniform_(-1.0, 1.0).to(device)\n",
    "\telif z_prior == \"fixed\":\n",
    "\t\tassert random_gen != None, \"generator must not be none\"\n",
    "\t\tsaved_state = random_gen.get_state()\n",
    "\t\tzs = torch.FloatTensor(batch_size, z_dim).uniform_(-1.0, 1.0, generator = random_gen).to(device)\n",
    "\t\trandom_gen.set_state(saved_state)\n",
    "\telse:\n",
    "\t\traise NotImplementedError\n",
    "\n",
    "\tif isinstance(radius, float) and radius > 0.0:\n",
    "\t\tif z_prior == \"gaussian\":\n",
    "\t\t\tzs_eps = zs + radius * sample_normal(batch_size, z_dim, -1.0, device)\n",
    "\t\telif z_prior == \"uniform\":\n",
    "\t\t\tzs_eps = zs + radius * torch.FloatTensor(batch_size, z_dim).uniform_(-1.0, 1.0).to(device)\n",
    "\telse:\n",
    "\t\tzs_eps = None\n",
    "\n",
    "\treturn zs, fake_labels, zs_eps\n",
    "\n",
    "\n",
    "def make_GAN_trainable(Gen, Gen_ema, Dis):\n",
    "\tGen.train()\n",
    "\tGen.apply(track_bn_statistics)\n",
    "\tif Gen_ema is not None:\n",
    "\t\tGen_ema.train()\n",
    "\t\tGen_ema.apply(track_bn_statistics)\n",
    "\n",
    "\tDis.train()\n",
    "\tDis.apply(track_bn_statistics)\n",
    "\n",
    "def peel_model(model):\n",
    "\tif isinstance(model, nn.DataParallel):\n",
    "\t\tmodel = model.module\n",
    "\treturn model\n",
    "\n",
    "def toggle_grad(model, grad, num_freeze_layers=-1, is_stylegan=False):\n",
    "\tmodel = peel_model(model)\n",
    "\tif is_stylegan:\n",
    "\t\tfor name, param in model.named_parameters():\n",
    "\t\t\tparam.requires_grad = grad\n",
    "\telse:\n",
    "\t\ttry:\n",
    "\t\t\tnum_blocks = len(model.in_dims)\n",
    "\t\t\tassert num_freeze_layers < num_blocks,\\\n",
    "\t\t\t\t\"cannot freeze the {nfl}th block > total {nb} blocks.\".format(nfl=num_freeze_layers,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnb=num_blocks)\n",
    "\t\texcept:\n",
    "\t\t\tpass\n",
    "\n",
    "\t\tif num_freeze_layers == -1:\n",
    "\t\t\tfor name, param in model.named_parameters():\n",
    "\t\t\t\tparam.requires_grad = grad\n",
    "\t\telse:\n",
    "\t\t\tassert grad, \"cannot freeze the model when grad is False\"\n",
    "\t\t\tfor name, param in model.named_parameters():\n",
    "\t\t\t\tparam.requires_grad = True\n",
    "\t\t\t\tfor layer in range(num_freeze_layers):\n",
    "\t\t\t\t\tblock_name = \"blocks.{layer}\".format(layer=layer)\n",
    "\t\t\t\t\tif block_name in name:\n",
    "\t\t\t\t\t\tparam.requires_grad = False\n",
    "\n",
    "def calculate_all_sn(model, prefix):\n",
    "    sigmas = {}\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            operations = model\n",
    "            if \"weight_orig\" in name:\n",
    "                splited_name = name.split(\".\")\n",
    "                for name_element in splited_name[:-1]:\n",
    "                    operations = getattr(operations, name_element)\n",
    "                weight_orig = reshape_weight_to_matrix(operations.weight_orig)\n",
    "                weight_u = operations.weight_u\n",
    "                weight_v = operations.weight_v\n",
    "                sigmas[prefix + \"_\" + name] = torch.dot(weight_u, torch.mv(weight_orig, weight_v)).item()\n",
    "    return sigmas\n",
    "\n",
    "def cal_deriv(inputs, outputs, device):\n",
    "\tgrads = autograd.grad(outputs=outputs,\n",
    "\t\t\t\t\t\t  inputs=inputs,\n",
    "\t\t\t\t\t\t  grad_outputs=torch.ones(outputs.size()).to(device),\n",
    "\t\t\t\t\t\t  create_graph=True,\n",
    "\t\t\t\t\t\t  retain_graph=True,\n",
    "\t\t\t\t\t\t  only_inputs=True)[0]\n",
    "\treturn grads\n",
    "\n",
    "def cal_r1_reg(adv_output, images, device):\n",
    "\tbatch_size = images.size(0)\n",
    "\tgrad_dout = cal_deriv(inputs=images, outputs=adv_output.sum(), device=device)\n",
    "\tgrad_dout2 = grad_dout.pow(2)\n",
    "\tassert (grad_dout2.size() == images.size())\n",
    "\tr1_reg = 0.5 * grad_dout2.contiguous().view(batch_size, -1).sum(1).mean(0) + images[:,0,0,0].mean()*0\n",
    "\treturn r1_reg\n",
    "\n",
    "def cal_maxgrad_penalty(real_images, real_labels, fake_images, discriminator, device):\n",
    "\tbatch_size, c, h, w = real_images.shape\n",
    "\talpha = torch.rand(batch_size, 1)\n",
    "\talpha = alpha.expand(batch_size, real_images.nelement() // batch_size).contiguous().view(batch_size, c, h, w)\n",
    "\talpha = alpha.to(device)\n",
    "\n",
    "\treal_images = real_images.to(device)\n",
    "\tinterpolates = alpha * real_images + ((1 - alpha) * fake_images)\n",
    "\tinterpolates = interpolates.to(device)\n",
    "\tinterpolates = autograd.Variable(interpolates, requires_grad=True)\n",
    "\tfake_dict = discriminator(interpolates, real_labels, eval=False)\n",
    "\tgrads = cal_deriv(inputs=interpolates, outputs=fake_dict[\"adv_output\"], device=device)\n",
    "\tgrads = grads.view(grads.size(0), -1)\n",
    "\n",
    "\tmaxgrad_penalty = torch.max(grads.norm(2, dim=1)**2) + interpolates[:,0,0,0].mean()*0\n",
    "\treturn maxgrad_penalty\n",
    "\n",
    "def generate_images(z_prior, truncation_factor, batch_size, z_dim, num_classes, y_sampler, radius, generator, discriminator,\n",
    "\t\t\t\t\tis_train, LOSS, RUN, MODEL, device, is_stylegan, cal_trsp_cost, random_gen = None):\n",
    "\tif is_train:\n",
    "\t\ttruncation_factor = -1.0\n",
    "\t\tlo_steps = LOSS.lo_steps4train\n",
    "\t\tapply_langevin = False\n",
    "\telse:\n",
    "\t\tlo_steps = LOSS.lo_steps4eval\n",
    "\t\tif truncation_factor != -1:\n",
    "\t\t\tif is_stylegan:\n",
    "\t\t\t\tassert 0 <= truncation_factor <= 1, \"Stylegan truncation_factor must lie btw 0(strong truncation) ~ 1(no truncation)\"\n",
    "\t\t\telse:\n",
    "\t\t\t\tassert 0 <= truncation_factor, \"truncation_factor must lie btw 0(strong truncation) ~ inf(no truncation)\"\n",
    "\n",
    "\tzs, fake_labels, zs_eps = sample_zy(z_prior=z_prior,\n",
    "\t\t\t\t\t\t\t\t\t\tbatch_size=batch_size,\n",
    "\t\t\t\t\t\t\t\t\t\tz_dim=z_dim,\n",
    "\t\t\t\t\t\t\t\t\t\tnum_classes=num_classes,\n",
    "\t\t\t\t\t\t\t\t\t\ttruncation_factor=-1 if is_stylegan else truncation_factor,\n",
    "\t\t\t\t\t\t\t\t\t\ty_sampler=y_sampler,\n",
    "\t\t\t\t\t\t\t\t\t\tradius=radius,\n",
    "\t\t\t\t\t\t\t\t\t\tdevice=device,\n",
    "\t\t\t\t\t\t\t\t\t\trandom_gen = random_gen)\n",
    "\tbatch_size = fake_labels.shape[0]\n",
    "\tinfo_discrete_c, info_conti_c = None, None\n",
    "\tif MODEL.info_type in [\"discrete\", \"both\"]:\n",
    "\t\tinfo_discrete_c = torch.randint(MODEL.info_dim_discrete_c,(batch_size, MODEL.info_num_discrete_c), device=device)\n",
    "\t\tzs = torch.cat((zs, F.one_hot(info_discrete_c, MODEL.info_dim_discrete_c).view(batch_size, -1)), dim=1)\n",
    "\tif MODEL.info_type in [\"continuous\", \"both\"]:\n",
    "\t\tinfo_conti_c = torch.rand(batch_size, MODEL.info_num_conti_c, device=device) * 2 - 1\n",
    "\t\tzs = torch.cat((zs, info_conti_c), dim=1)\n",
    "\n",
    "\ttrsp_cost = None\n",
    "\tfake_images = generator(zs, fake_labels, eval=not is_train)\n",
    "\tws = None\n",
    "\n",
    "\tif zs_eps is not None:\n",
    "\t\tfake_images_eps = generator(zs_eps, fake_labels, eval=not is_train)\n",
    "\telse:\n",
    "\t\tfake_images_eps = None\n",
    "\treturn fake_images, fake_labels, fake_images_eps, trsp_cost, ws, info_discrete_c, info_conti_c\n",
    "\n",
    "# Exponential Moving Average\n",
    "class Ema(object):\n",
    "    def __init__(self, source, target, decay=0.9999, start_iter=0):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.decay = decay\n",
    "        self.start_iter = start_iter\n",
    "        self.source_dict = self.source.state_dict()\n",
    "        self.target_dict = self.target.state_dict()\n",
    "        print(\"Initialize the copied generator's parameters to be source parameters.\")\n",
    "        with torch.no_grad():\n",
    "            for p_ema, p in zip(self.target.parameters(), self.source.parameters()):\n",
    "                p_ema.copy_(p)\n",
    "            for b_ema, b in zip(self.target.buffers(), self.source.buffers()):\n",
    "                b_ema.copy_(b)\n",
    "\n",
    "    def update(self, iter=None):\n",
    "        if iter >= 0 and iter < self.start_iter:\n",
    "            decay = 0.0\n",
    "        else:\n",
    "            decay = self.decay\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for p_ema, p in zip(self.target.parameters(), self.source.parameters()):\n",
    "                p_ema.copy_(p.lerp(p_ema, decay))\n",
    "            for (b_ema_name, b_ema), (b_name, b) in zip(self.target.named_buffers(), self.source.named_buffers()):\n",
    "                if \"num_batches_tracked\" in b_ema_name:\n",
    "                    b_ema.copy_(b)\n",
    "                else:\n",
    "                    b_ema.copy_(b.lerp(b_ema, decay))\n",
    "\n",
    "def load_generator_discriminator(DATA, OPTIMIZATION, MODEL, STYLEGAN, MODULES, RUN, device, logger, Generator, Discriminator):\n",
    "\t# if device == 0:\n",
    "\t# \tlogger.info(\"Build a Generative Adversarial Network.\")\n",
    "\t# module = __import__(\"models.{backbone}\".format(backbone=MODEL.backbone), fromlist=[\"something\"])\n",
    "\t# if device == 0:\n",
    "\t# \tlogger.info(\"Modules are located on './src/models.{backbone}'.\".format(backbone=MODEL.backbone))\n",
    "\n",
    "\tGen = Generator(z_dim=MODEL.z_dim,\n",
    "\t\t\t\t\t\t\tg_shared_dim=MODEL.g_shared_dim,\n",
    "\t\t\t\t\t\t\timg_size=DATA.img_size,\n",
    "\t\t\t\t\t\t\tg_conv_dim=MODEL.g_conv_dim,\n",
    "\t\t\t\t\t\t\tapply_attn=MODEL.apply_attn,\n",
    "\t\t\t\t\t\t\tattn_g_loc=MODEL.attn_g_loc,\n",
    "\t\t\t\t\t\t\tg_cond_mtd=MODEL.g_cond_mtd,\n",
    "\t\t\t\t\t\t\tnum_classes=DATA.num_classes,\n",
    "\t\t\t\t\t\t\tg_init=MODEL.g_init,\n",
    "\t\t\t\t\t\t\tg_depth=MODEL.g_depth,\n",
    "\t\t\t\t\t\t\tMODULES=MODULES,\n",
    "\t\t\t\t\t\t\tMODEL=MODEL).to(device)\n",
    "\n",
    "\tDis = Discriminator(img_size=DATA.img_size,\n",
    "\t\t\t\t\t\t\t\td_conv_dim=MODEL.d_conv_dim,\n",
    "\t\t\t\t\t\t\t\tapply_d_sn=MODEL.apply_d_sn,\n",
    "\t\t\t\t\t\t\t\tapply_attn=MODEL.apply_attn,\n",
    "\t\t\t\t\t\t\t\tattn_d_loc=MODEL.attn_d_loc,\n",
    "\t\t\t\t\t\t\t\td_cond_mtd=MODEL.d_cond_mtd,\n",
    "\t\t\t\t\t\t\t\taux_cls_type=MODEL.aux_cls_type,\n",
    "\t\t\t\t\t\t\t\td_embed_dim=MODEL.d_embed_dim,\n",
    "\t\t\t\t\t\t\t\tnum_classes=DATA.num_classes,\n",
    "\t\t\t\t\t\t\t\tnormalize_d_embed=MODEL.normalize_d_embed,\n",
    "\t\t\t\t\t\t\t\td_init=MODEL.d_init,\n",
    "\t\t\t\t\t\t\t\td_depth=MODEL.d_depth,\n",
    "\t\t\t\t\t\t\t\tMODULES=MODULES,\n",
    "\t\t\t\t\t\t\t\tMODEL=MODEL).to(device)\n",
    "\tif MODEL.apply_g_ema:\n",
    "\t\t# if device == 0:\n",
    "\t\t# \tlogger.info(\"Prepare exponential moving average generator with decay rate of {decay}.\"\\\n",
    "\t\t# \t\t\t\t.format(decay=MODEL.g_ema_decay))\n",
    "\t\tGen_ema = copy.deepcopy(Gen)\n",
    "\n",
    "\t\tema = Ema(source=Gen, target=Gen_ema, decay=MODEL.g_ema_decay, start_iter=MODEL.g_ema_start)\n",
    "\telse:\n",
    "\t\tGen_ema, ema = None, None\n",
    "\n",
    "\treturn Gen, Dis, Gen_ema, ema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis üó∫Ô∏è\n",
    "To start off, let's try to get a better feel for the dataset.\n",
    "We download the data from the Kaggle link: <a href=\"https://www.kaggle.com/datasets/pankrzysiu/cifar10-python\">link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "\t<tr>\n",
    "\t\t<th>\n",
    "\t\t\tColumn Name\n",
    "\t\t</th>\n",
    "\t\t<th>\n",
    "\t\t\tDescription\n",
    "\t\t</th>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>\n",
    "\t\t\tpixel 1<br />\n",
    "\t\t\t...<br />\n",
    "\t\t\tpixel 3072\n",
    "\t\t</td>\n",
    "\t\t<td>\n",
    "\t\t\tPixels representing the image, each pixel ranging from 0 to 255. Each image has a dimension of 32x32x3\n",
    "\t\t</td>\n",
    "\t</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (60000, 3072)\n",
      "y shape: (60000, 1)\n"
     ]
    }
   ],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "files = [f'data_batch_{i}' for i in range(1, 6)] + ['test_batch']\n",
    "\n",
    "X = pd.DataFrame()\n",
    "y = pd.DataFrame()\n",
    "for file in files:\n",
    "\tX = pd.concat([X, pd.DataFrame(unpickle(f'data/{file}')[b'data'])])\n",
    "\ty = pd.concat([y, pd.DataFrame(unpickle(f'data/{file}')[b'labels'])])\n",
    "\n",
    "print(\"X shape:\",X.shape)\n",
    "print(\"y shape:\",y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are a total of `60000` rows in both the `X DataFrame` and `y DataFrame`\n",
    "\n",
    "Next, we open the metadata and see all the different labels we have. We see that <strong>there are a total of 10 classes</strong>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 10\n",
      "['airplane' 'automobile' 'bird' 'cat' 'deer' 'dog' 'frog' 'horse' 'ship'\n",
      " 'truck']\n"
     ]
    }
   ],
   "source": [
    "classes = np.array(\n",
    "\tlist(map(\n",
    "\t\tlambda x: x.decode('utf-8'),\n",
    "\t\tunpickle('data/batches.meta')[b'label_names']\n",
    "\t))\n",
    ")\n",
    "print(\"Number of classes:\", len(classes))\n",
    "print(classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "Before we move into EDA, we transform the dataframe to tensors, using the following transformations:\n",
    "<ul>\n",
    "\t<li>Change from 3072 pixels to <code>3x32x32</code> images</li>\n",
    "\t<li>Normalize the values</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we reshape the 3072 pixels accordingly into 3 channels of 32x32 images\n",
    "X_tensor = torch.Tensor(np.transpose(X.values.reshape((-1, 3, 32, 32)), axes=(0, 1, 2, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to normalize the values. To do this, we firstly want to calculate <strong>the mean and standard deviations</strong> in the image. We note that this is done within each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 3, 32, 32])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means:  tensor([0.4919, 0.4827, 0.4472])\n",
      "Standard Deviations: tensor([0.2470, 0.2434, 0.2616])\n"
     ]
    }
   ],
   "source": [
    "X_means = X_tensor.mean(axis=(0, 2, 3)) / 255\n",
    "X_stds = X_tensor.std(axis=(0, 2, 3)) / 255\n",
    "print(\"Means: \", X_means)\n",
    "print(\"Standard Deviations:\", X_stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the mean and standard deviations calculated, we then move onto dividing the pixel values by 255, to make the range between `0 - 1`. Only **after** we do this, do we apply the normalize transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(X_means, X_stds)\n",
    "X_tensor = normalize(X_tensor / 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signs of Class Imbalance\n",
    "We try to discover if there are any signs of class imbalance. If so, this may indicate that our data processing pipeline is incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airplane      6000\n",
       "automobile    6000\n",
       "bird          6000\n",
       "cat           6000\n",
       "deer          6000\n",
       "dog           6000\n",
       "frog          6000\n",
       "horse         6000\n",
       "ship          6000\n",
       "truck         6000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(classes[y]).value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe:\n",
    "<ul>\n",
    "\t<li>For the `10` classes in the dataset, we see that each of the classes has a count of `6000`.</li>\n",
    "\t<li>Thus, we see there is no sign of class imbalance.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Null values?\n",
    "Next, we perform a simple `.isnull()` check to see if there are any null values in any of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null counts among ALL the data: 0\n"
     ]
    }
   ],
   "source": [
    "null_counts = 0\n",
    "for iter_data in [X, y]:\n",
    "\tnull_counts += iter_data.isnull().sum().sum()\n",
    "\n",
    "print(\"Null counts among ALL the data:\", null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we have downloaded and used present no null values. Thus, we can proceed with further data exploration.\n",
    "\n",
    "#### What does the average image look like?\n",
    "First, we take a look at a few images from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<Figure size 640x480 with 10 Axes>, <AxesSubplot: title={'center': 'cat'}>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(X_tensor[0:10], label = classes[y[0:10]].flatten().tolist(), is_int = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe:\n",
    "<ul>\n",
    "\t<li>The objects in question such as <code>frog</code> and <code>deer</code>, do have some sort of background</li>\n",
    "\t<li>\n",
    "\t\tVisually, although the images are <strong>quite blurry</strong> due to the image size being <code>32x32</code>, it is still easy to tell the diffference between the classes.\n",
    "\t</li>\n",
    "</ul>\n",
    "\n",
    "What about the <strong>average of all images</strong>?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Figure size 640x480 with 1 Axes>,\n",
       " <AxesSubplot: title={'center': 'Average of all'}>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow([\n",
    "\tX.mean().apply(lambda x: x).values.reshape(3, 32, 32)\n",
    "], ['Average of all'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there is no significance in the average of all images.\n",
    "\n",
    "<strong>Average image among the classes</strong><br />\n",
    "We then split by the classes and find the average among each of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Figure size 1700x1000 with 10 Axes>,\n",
       " <AxesSubplot: title={'center': 'truck'}>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1700x1000 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = []\n",
    "titles = []\n",
    "\n",
    "for i in range(10):\n",
    "\tidx = np.where((y.values == i))[0].flatten()\n",
    "\timages.append(X.iloc[idx].values.reshape(-1, 3, 32, 32).mean(axis=0))\n",
    "\ttitles.append(classes[i])\n",
    "\n",
    "imshow(images, titles, figsize = (17, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe:\n",
    "<ul>\n",
    "\t<li>\n",
    "\t\tWe manage to <em>slightly</em> make out the shape of the <code>horse</code>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t\tIt appears that the average <code>horse</code> seems to have a more dominant precence when it faces the left, which is quite an interesting note.\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t\tFor most of the classes, we manage to make out some sort of contour in the center, however it still is pixelated and blurry.\n",
    "\t</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_prob_dist(labels: torch.tensor, size = 100):\n",
    "\tarr = np.full((len(labels), size), 0)\n",
    "\tfor i, label in enumerate(labels):\n",
    "\t\tarr[i][int(label.item())] = 1\n",
    "\treturn torch.tensor(arr, dtype=torch.float32)\n",
    "\n",
    "class TorchDataset(torch.utils.data.Dataset):\n",
    "\tdef __init__(self, X, y, X_shape = (3, 32, 32), y_shape = (1, ), y_size = 10, transform = None):\n",
    "\t\tif (type(X) == torch.Tensor):\n",
    "\t\t\tself.x = X\n",
    "\t\telse:\n",
    "\t\t\tself.x = df_to_tensor(X, X_shape)\n",
    "\t\t\tself.x /= 255\n",
    "\t\t\tself.x *= 2\n",
    "\t\t\tself.x -= 1\n",
    "\n",
    "\t\tself.x = self.x.type(torch.float32)\n",
    "\t\tself.x = self.x.to(torch.device('cuda'))\n",
    "\n",
    "\t\tself.transform = transform\n",
    "            \n",
    "\t\tif (type(y) == torch.Tensor):\n",
    "\t\t\tself.y = y\n",
    "\t\telse:\n",
    "\t\t\tself.y = df_to_tensor(y, y_shape)\n",
    "\t\t\tself.y = to_prob_dist(self.y, y_size)\n",
    "\n",
    "\t\t# Transform labels to probability distributions\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.y)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tif torch.is_tensor(idx):\n",
    "\t\t\tidx = idx.tolist()\n",
    "\n",
    "\t\tselected = self.x[idx]\n",
    "            \n",
    "\t\tif self.transform:\n",
    "\t\t\tselected = self.transform(selected)\n",
    "        \n",
    "\t\treturn selected, self.y[idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application and Evaluation of GAN Models\n",
    "In this section, we attempt to make and apply our GAN models to tackle the CIFAR10 dataset. Additionally, for each model we try, we will also have a set of evaluation metrics and will analyze the outputs to gain a better understand of our progress.\n",
    "\n",
    "The modeling process will go as such:\n",
    "<ol>\n",
    "\t<li>Train the generator and discriminator</li>\n",
    "\t<li>Assess the progress</li>\n",
    "\t<li>Evaluate on a series of metrics</li>\n",
    "\t<li>Discover potential improvements and repeat</li>\n",
    "</ol>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are our <strong>evaluation metrics</strong>?\n",
    "As our primary measure for **quality**, we will utilize both **Inception Score** and **Frechet Inception Distance**.\n",
    "\n",
    "**I**\n",
    "\n",
    "Frechet Inception Distance\n",
    "Inception Score (Can't detect ModeCollapse)\n",
    "Wasserstein Critic\n",
    "Mode Collapse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining **Loss Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatherLayer(torch.autograd.Function):\n",
    "\t\"\"\"\n",
    "\tThis file is copied from\n",
    "\thttps://github.com/open-mmlab/OpenSelfSup/blob/master/openselfsup/models/utils/gather_layer.py\n",
    "\tGather tensors from all process, supporting backward propagation\n",
    "\t\"\"\"\n",
    "\t@staticmethod\n",
    "\tdef forward(ctx, input):\n",
    "\t\tctx.save_for_backward(input)\n",
    "\t\toutput = [torch.zeros_like(input) for _ in range(dist.get_world_size())]\n",
    "\t\tdist.all_gather(output, input)\n",
    "\t\treturn tuple(output)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef backward(ctx, *grads):\n",
    "\t\tinput, = ctx.saved_tensors\n",
    "\t\tgrad_out = torch.zeros_like(input)\n",
    "\t\tgrad_out[:] = grads[dist.get_rank()]\n",
    "\t\treturn grad_out\n",
    "\n",
    "\n",
    "class CrossEntropyLoss(torch.nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(CrossEntropyLoss, self).__init__()\n",
    "\t\tself.ce_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\tdef forward(self, cls_output, label, **_):\n",
    "\t\treturn self.ce_loss(cls_output, label).mean()\n",
    "\n",
    "class Data2DataCrossEntropyLoss(torch.nn.Module):\n",
    "\tdef __init__(self, num_classes, temperature, m_p, master_rank, DDP):\n",
    "\t\tsuper(Data2DataCrossEntropyLoss, self).__init__()\n",
    "\t\tself.num_classes = num_classes\n",
    "\t\tself.temperature = temperature\n",
    "\t\tself.m_p = m_p\n",
    "\t\tself.master_rank = master_rank\n",
    "\t\tself.DDP = DDP\n",
    "\t\tself.calculate_similarity_matrix = self._calculate_similarity_matrix()\n",
    "\t\tself.cosine_similarity = torch.nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "\tdef _calculate_similarity_matrix(self):\n",
    "\t\treturn self._cosine_simililarity_matrix\n",
    "\n",
    "\tdef _cosine_simililarity_matrix(self, x, y):\n",
    "\t\tv = self.cosine_similarity(x.unsqueeze(1), y.unsqueeze(0))\n",
    "\t\treturn v\n",
    "\n",
    "\tdef make_index_matrix(self, labels):\n",
    "\t\tlabels = labels.detach().cpu().numpy()\n",
    "\t\tnum_samples = labels.shape[0]\n",
    "\t\tmask_multi, target = np.ones([self.num_classes, num_samples]), 0.0\n",
    "\n",
    "\t\tfor c in range(self.num_classes):\n",
    "\t\t\tc_indices = np.where(labels==c)\n",
    "\t\t\tmask_multi[c, c_indices] = target\n",
    "\t\treturn torch.tensor(mask_multi).type(torch.long).to(self.master_rank)\n",
    "\n",
    "\tdef remove_diag(self, M):\n",
    "\t\th, w = M.shape\n",
    "\t\tassert h==w, \"h and w should be same\"\n",
    "\t\tmask = np.ones((h, w)) - np.eye(h)\n",
    "\t\tmask = torch.from_numpy(mask)\n",
    "\t\tmask = (mask).type(torch.bool).to(self.master_rank)\n",
    "\t\treturn M[mask].view(h, -1)\n",
    "\n",
    "\tdef forward(self, embed, proxy, label, **_):\n",
    "\t\t# If train a GAN throuh DDP, gather all data on the master rank\n",
    "\t\tif self.DDP:\n",
    "\t\t\tembed = torch.cat(GatherLayer.apply(embed), dim=0)\n",
    "\t\t\tproxy = torch.cat(GatherLayer.apply(proxy), dim=0)\n",
    "\t\t\tlabel = torch.cat(GatherLayer.apply(label), dim=0)\n",
    "\n",
    "\t\t# calculate similarities between sample embeddings\n",
    "\t\tsim_matrix = self.calculate_similarity_matrix(embed, embed) + self.m_p - 1\n",
    "\t\t# remove diagonal terms\n",
    "\t\tsim_matrix = self.remove_diag(sim_matrix/self.temperature)\n",
    "\t\t# for numerical stability\n",
    "\t\tsim_max, _ = torch.max(sim_matrix, dim=1, keepdim=True)\n",
    "\t\tsim_matrix = F.relu(sim_matrix) - sim_max.detach()\n",
    "\n",
    "\t\t# calculate similarities between sample embeddings and the corresponding proxies\n",
    "\t\tsmp2proxy = self.cosine_similarity(embed, proxy)\n",
    "\t\t# make false negative removal\n",
    "\t\tremoval_fn = self.remove_diag(self.make_index_matrix(label)[label])\n",
    "\t\t# apply the negative removal to the similarity matrix\n",
    "\t\timproved_sim_matrix = removal_fn*torch.exp(sim_matrix)\n",
    "\n",
    "\t\t# compute positive attraction term\n",
    "\t\tpos_attr = F.relu((self.m_p - smp2proxy)/self.temperature)\n",
    "\t\t# compute negative repulsion term\n",
    "\t\tneg_repul = torch.log(torch.exp(-pos_attr) + improved_sim_matrix.sum(dim=1))\n",
    "\t\t# compute data to data cross-entropy criterion\n",
    "\t\tcriterion = pos_attr + neg_repul\n",
    "\t\treturn criterion.mean()\n",
    "\n",
    "def enable_allreduce(dict_):\n",
    "\tloss = 0\n",
    "\tfor key, value in dict_.items():\n",
    "\t\tif value is not None and key != \"label\":\n",
    "\t\t\tloss += value.mean()*0\n",
    "\treturn loss\n",
    "\n",
    "\n",
    "def d_vanilla(d_logit_real, d_logit_fake, DDP):\n",
    "\td_loss = torch.mean(F.softplus(-d_logit_real)) + torch.mean(F.softplus(d_logit_fake))\n",
    "\treturn d_loss\n",
    "\n",
    "\n",
    "def g_vanilla(d_logit_fake, DDP):\n",
    "\treturn torch.mean(F.softplus(-d_logit_fake))\n",
    "\n",
    "\n",
    "def d_logistic(d_logit_real, d_logit_fake, DDP):\n",
    "\td_loss = F.softplus(-d_logit_real) + F.softplus(d_logit_fake)\n",
    "\treturn d_loss.mean()\n",
    "\n",
    "\n",
    "def g_logistic(d_logit_fake, DDP):\n",
    "\t# basically same as g_vanilla.\n",
    "\treturn F.softplus(-d_logit_fake).mean()\n",
    "\n",
    "\n",
    "def d_ls(d_logit_real, d_logit_fake, DDP):\n",
    "\td_loss = 0.5 * (d_logit_real - torch.ones_like(d_logit_real))**2 + 0.5 * (d_logit_fake)**2\n",
    "\treturn d_loss.mean()\n",
    "\n",
    "\n",
    "def g_ls(d_logit_fake, DDP):\n",
    "\tgen_loss = 0.5 * (d_logit_fake - torch.ones_like(d_logit_fake))**2\n",
    "\treturn gen_loss.mean()\n",
    "\n",
    "\n",
    "def d_hinge(d_logit_real, d_logit_fake, DDP):\n",
    "\treturn torch.mean(F.relu(1. - d_logit_real)) + torch.mean(F.relu(1. + d_logit_fake))\n",
    "\n",
    "\n",
    "def g_hinge(d_logit_fake, DDP):\n",
    "\treturn -torch.mean(d_logit_fake)\n",
    "\n",
    "\n",
    "def d_wasserstein(d_logit_real, d_logit_fake, DDP):\n",
    "\treturn torch.mean(d_logit_fake - d_logit_real)\n",
    "\n",
    "\n",
    "def g_wasserstein(d_logit_fake, DDP):\n",
    "\treturn -torch.mean(d_logit_fake)\n",
    "\n",
    "\n",
    "def crammer_singer_loss(adv_output, label, DDP, **_):\n",
    "\t# https://github.com/ilyakava/BigGAN-PyTorch/blob/master/train_fns.py\n",
    "\t# crammer singer criterion\n",
    "\tnum_real_classes = adv_output.shape[1] - 1\n",
    "\tmask = torch.ones_like(adv_output).to(adv_output.device)\n",
    "\tmask.scatter_(1, label.unsqueeze(-1), 0)\n",
    "\twrongs = torch.masked_select(adv_output, mask.bool()).reshape(adv_output.shape[0], num_real_classes)\n",
    "\tmax_wrong, _ = wrongs.max(1)\n",
    "\tmax_wrong = max_wrong.unsqueeze(-1)\n",
    "\ttarget = adv_output.gather(1, label.unsqueeze(-1))\n",
    "\treturn torch.mean(F.relu(1 + max_wrong - target))\n",
    "\n",
    "\n",
    "def feature_matching_loss(real_embed, fake_embed):\n",
    "\t# https://github.com/ilyakava/BigGAN-PyTorch/blob/master/train_fns.py\n",
    "\t# feature matching criterion\n",
    "\tfm_loss = torch.mean(torch.abs(torch.mean(fake_embed, 0) - torch.mean(real_embed, 0)))\n",
    "\treturn fm_loss\n",
    "\n",
    "\n",
    "def lecam_reg(d_logit_real, d_logit_fake, ema):\n",
    "\treg = torch.mean(F.relu(d_logit_real - ema.D_fake).pow(2)) + \\\n",
    "\t\t  torch.mean(F.relu(ema.D_real - d_logit_fake).pow(2))\n",
    "\treturn reg\n",
    "\n",
    "\n",
    "def cal_deriv(inputs, outputs, device):\n",
    "\tgrads = autograd.grad(outputs=outputs,\n",
    "\t\t\t\t\t\t  inputs=inputs,\n",
    "\t\t\t\t\t\t  grad_outputs=torch.ones(outputs.size()).to(device),\n",
    "\t\t\t\t\t\t  create_graph=True,\n",
    "\t\t\t\t\t\t  retain_graph=True,\n",
    "\t\t\t\t\t\t  only_inputs=True)[0]\n",
    "\treturn grads\n",
    "\n",
    "\n",
    "def latent_optimise(zs, fake_labels, generator, discriminator, batch_size, lo_rate, lo_steps, lo_alpha, lo_beta, eval,\n",
    "\t\t\t\t\tcal_trsp_cost, device):\n",
    "\tfor step in range(lo_steps - 1):\n",
    "\t\tdrop_mask = (torch.FloatTensor(batch_size, 1).uniform_() > 1 - lo_rate).to(device)\n",
    "\n",
    "\t\tzs = autograd.Variable(zs, requires_grad=True)\n",
    "\t\tfake_images = generator(zs, fake_labels, eval=eval)\n",
    "\t\tfake_dict = discriminator(fake_images, fake_labels, eval=eval)\n",
    "\t\tz_grads = cal_deriv(inputs=zs, outputs=fake_dict[\"adv_output\"], device=device)\n",
    "\t\tz_grads_norm = torch.unsqueeze((z_grads.norm(2, dim=1)**2), dim=1)\n",
    "\t\tdelta_z = lo_alpha * z_grads / (lo_beta + z_grads_norm)\n",
    "\t\tzs = torch.clamp(zs + drop_mask * delta_z, -1.0, 1.0)\n",
    "\n",
    "\t\tif cal_trsp_cost:\n",
    "\t\t\tif step == 0:\n",
    "\t\t\t\ttrsf_cost = (delta_z.norm(2, dim=1)**2).mean()\n",
    "\t\t\telse:\n",
    "\t\t\t\ttrsf_cost += (delta_z.norm(2, dim=1)**2).mean()\n",
    "\t\telse:\n",
    "\t\t\ttrsf_cost = None\n",
    "\t\treturn zs, trsf_cost\n",
    "\n",
    "\n",
    "def cal_grad_penalty(real_images, real_labels, fake_images, discriminator, device):\n",
    "\tbatch_size, c, h, w = real_images.shape\n",
    "\talpha = torch.rand(batch_size, 1)\n",
    "\talpha = alpha.expand(batch_size, real_images.nelement() // batch_size).contiguous().view(batch_size, c, h, w)\n",
    "\talpha = alpha.to(device)\n",
    "\n",
    "\treal_images = real_images.to(device)\n",
    "\tinterpolates = alpha * real_images + ((1 - alpha) * fake_images)\n",
    "\tinterpolates = interpolates.to(device)\n",
    "\tinterpolates = autograd.Variable(interpolates, requires_grad=True)\n",
    "\tfake_dict = discriminator(interpolates, real_labels, eval=False)\n",
    "\tgrads = cal_deriv(inputs=interpolates, outputs=fake_dict[\"adv_output\"], device=device)\n",
    "\tgrads = grads.view(grads.size(0), -1)\n",
    "\n",
    "\tgrad_penalty = ((grads.norm(2, dim=1) - 1)**2).mean() + interpolates[:,0,0,0].mean()*0\n",
    "\treturn grad_penalty\n",
    "\n",
    "\n",
    "def cal_dra_penalty(real_images, real_labels, discriminator, device):\n",
    "\tbatch_size, c, h, w = real_images.shape\n",
    "\talpha = torch.rand(batch_size, 1, 1, 1)\n",
    "\talpha = alpha.to(device)\n",
    "\n",
    "\treal_images = real_images.to(device)\n",
    "\tdifferences = 0.5 * real_images.std() * torch.rand(real_images.size()).to(device)\n",
    "\tinterpolates = real_images + (alpha * differences)\n",
    "\tinterpolates = interpolates.to(device)\n",
    "\tinterpolates = autograd.Variable(interpolates, requires_grad=True)\n",
    "\tfake_dict = discriminator(interpolates, real_labels, eval=False)\n",
    "\tgrads = cal_deriv(inputs=interpolates, outputs=fake_dict[\"adv_output\"], device=device)\n",
    "\tgrads = grads.view(grads.size(0), -1)\n",
    "\n",
    "\tgrad_penalty = ((grads.norm(2, dim=1) - 1)**2).mean() + interpolates[:,0,0,0].mean()*0\n",
    "\treturn grad_penalty\n",
    "\n",
    "\n",
    "def cal_maxgrad_penalty(real_images, real_labels, fake_images, discriminator, device):\n",
    "\tbatch_size, c, h, w = real_images.shape\n",
    "\talpha = torch.rand(batch_size, 1)\n",
    "\talpha = alpha.expand(batch_size, real_images.nelement() // batch_size).contiguous().view(batch_size, c, h, w)\n",
    "\talpha = alpha.to(device)\n",
    "\n",
    "\treal_images = real_images.to(device)\n",
    "\tinterpolates = alpha * real_images + ((1 - alpha) * fake_images)\n",
    "\tinterpolates = interpolates.to(device)\n",
    "\tinterpolates = autograd.Variable(interpolates, requires_grad=True)\n",
    "\tfake_dict = discriminator(interpolates, real_labels, eval=False)\n",
    "\tgrads = cal_deriv(inputs=interpolates, outputs=fake_dict[\"adv_output\"], device=device)\n",
    "\tgrads = grads.view(grads.size(0), -1)\n",
    "\n",
    "\tmaxgrad_penalty = torch.max(grads.norm(2, dim=1)**2) + interpolates[:,0,0,0].mean()*0\n",
    "\treturn maxgrad_penalty\n",
    "\n",
    "\n",
    "def cal_r1_reg(adv_output, images, device):\n",
    "\tbatch_size = images.size(0)\n",
    "\tgrad_dout = cal_deriv(inputs=images, outputs=adv_output.sum(), device=device)\n",
    "\tgrad_dout2 = grad_dout.pow(2)\n",
    "\tassert (grad_dout2.size() == images.size())\n",
    "\tr1_reg = 0.5 * grad_dout2.contiguous().view(batch_size, -1).sum(1).mean(0) + images[:,0,0,0].mean()*0\n",
    "\treturn r1_reg\n",
    "\n",
    "\n",
    "def adjust_k(current_k, topk_gamma, inf_k):\n",
    "\tcurrent_k = max(current_k * topk_gamma, inf_k)\n",
    "\treturn current_k\n",
    "\n",
    "\n",
    "def normal_nll_loss(x, mu, var):\n",
    "\t# https://github.com/Natsu6767/InfoGAN-PyTorch/blob/master/utils.py\n",
    "\t# Calculate the negative log likelihood of normal distribution.\n",
    "\t# Needs to be minimized in InfoGAN. (Treats Q(c]x) as a factored Gaussian)\n",
    "\tlogli = -0.5 * (var.mul(2 * np.pi) + 1e-6).log() - (x - mu).pow(2).div(var.mul(2.0) + 1e-6)\n",
    "\tnll = -(logli.sum(1).mean())\n",
    "\treturn nll"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining **Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copyright (c) 2020, Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han\n",
    "All rights reserved.\n",
    "\n",
    "Redistribution and use in source and binary forms, with or without\n",
    "modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "* Redistributions of source code must retain the above copyright notice, this\n",
    "  list of conditions and the following disclaimer.\n",
    "\n",
    "* Redistributions in binary form must reproduce the above copyright notice,\n",
    "  this list of conditions and the following disclaimer in the documentation\n",
    "  and/or other materials provided with the distribution.\n",
    "\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
    "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
    "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "\"\"\"\n",
    "\n",
    "### Differentiable Augmentation for Data-Efficient GAN Training (https://arxiv.org/abs/2006.10738)\n",
    "### Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han\n",
    "### https://github.com/mit-han-lab/data-efficient-gans\n",
    "\n",
    "def apply_diffaug(x, policy=\"color,translation,cutout\", channels_first=True):\n",
    "    if policy:\n",
    "        if not channels_first:\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "        for p in policy.split(\",\"):\n",
    "            for f in AUGMENT_FNS[p]:\n",
    "                x = f(x)\n",
    "        if not channels_first:\n",
    "            x = x.permute(0, 2, 3, 1)\n",
    "        x = x.contiguous()\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_brightness(x):\n",
    "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_saturation(x):\n",
    "    x_mean = x.mean(dim=1, keepdim=True)\n",
    "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_contrast(x):\n",
    "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
    "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_translation(x, ratio=0.125):\n",
    "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
    "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
    "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
    "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
    "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
    "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
    "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
    "    )\n",
    "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
    "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
    "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
    "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_cutout(x, ratio=0.5):\n",
    "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
    "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
    "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
    "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
    "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
    "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
    "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
    "    )\n",
    "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
    "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
    "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
    "    mask[grid_batch, grid_x, grid_y] = 0\n",
    "    x = x * mask.unsqueeze(1)\n",
    "    return x\n",
    "\n",
    "\n",
    "AUGMENT_FNS = {\n",
    "    \"color\": [rand_brightness, rand_saturation, rand_contrast],\n",
    "    \"translation\": [rand_translation],\n",
    "    \"cutout\": [rand_cutout],\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining **Nework Components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import spectral_norm\n",
    "\n",
    "class empty_object(object):\n",
    "\tpass\n",
    "\n",
    "class ConditionalBatchNorm2d(nn.Module):\n",
    "    # https://github.com/voletiv/self-attention-GAN-pytorch\n",
    "    def __init__(self, in_features, out_features, MODULES):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.bn = batchnorm_2d(out_features, eps=1e-4, momentum=0.1, affine=False)\n",
    "\n",
    "        self.gain = MODULES.g_linear(in_features=in_features, out_features=out_features, bias=False)\n",
    "        self.bias = MODULES.g_linear(in_features=in_features, out_features=out_features, bias=False)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        gain = (1 + self.gain(y)).view(y.size(0), -1, 1, 1)\n",
    "        bias = self.bias(y).view(y.size(0), -1, 1, 1)\n",
    "        out = self.bn(x)\n",
    "        return out * gain + bias\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    https://github.com/voletiv/self-attention-GAN-pytorch\n",
    "    MIT License\n",
    "    Copyright (c) 2019 Vikram Voleti\n",
    "    Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "    of this software and associated documentation files (the \"Software\"), to deal\n",
    "    in the Software without restriction, including without limitation the rights\n",
    "    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "    copies of the Software, and to permit persons to whom the Software is\n",
    "    furnished to do so, subject to the following conditions:\n",
    "    The above copyright notice and this permission notice shall be included in all\n",
    "    copies or substantial portions of the Software.\n",
    "    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "    SOFTWARE.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, is_generator, MODULES):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        if is_generator:\n",
    "            self.conv1x1_theta = MODULES.g_conv2d(in_channels=in_channels, out_channels=in_channels // 8, kernel_size=1,\n",
    "                                                  stride=1, padding=0, bias=False)\n",
    "            self.conv1x1_phi = MODULES.g_conv2d(in_channels=in_channels, out_channels=in_channels // 8, kernel_size=1,\n",
    "                                                stride=1, padding=0, bias=False)\n",
    "            self.conv1x1_g = MODULES.g_conv2d(in_channels=in_channels, out_channels=in_channels // 2, kernel_size=1,\n",
    "                                              stride=1, padding=0, bias=False)\n",
    "            self.conv1x1_attn = MODULES.g_conv2d(in_channels=in_channels // 2, out_channels=in_channels, kernel_size=1,\n",
    "                                                 stride=1, padding=0, bias=False)\n",
    "        else:\n",
    "            self.conv1x1_theta = MODULES.d_conv2d(in_channels=in_channels, out_channels=in_channels // 8, kernel_size=1,\n",
    "                                                  stride=1, padding=0, bias=False)\n",
    "            self.conv1x1_phi = MODULES.d_conv2d(in_channels=in_channels, out_channels=in_channels // 8, kernel_size=1,\n",
    "                                                stride=1, padding=0, bias=False)\n",
    "            self.conv1x1_g = MODULES.d_conv2d(in_channels=in_channels, out_channels=in_channels // 2, kernel_size=1,\n",
    "                                              stride=1, padding=0, bias=False)\n",
    "            self.conv1x1_attn = MODULES.d_conv2d(in_channels=in_channels // 2, out_channels=in_channels, kernel_size=1,\n",
    "                                                 stride=1, padding=0, bias=False)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2, stride=2, padding=0)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.sigma = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, ch, h, w = x.size()\n",
    "        # Theta path\n",
    "        theta = self.conv1x1_theta(x)\n",
    "        theta = theta.view(-1, ch // 8, h * w)\n",
    "        # Phi path\n",
    "        phi = self.conv1x1_phi(x)\n",
    "        phi = self.maxpool(phi)\n",
    "        phi = phi.view(-1, ch // 8, h * w // 4)\n",
    "        # Attn map\n",
    "        attn = torch.bmm(theta.permute(0, 2, 1), phi)\n",
    "        attn = self.softmax(attn)\n",
    "        # g path\n",
    "        g = self.conv1x1_g(x)\n",
    "        g = self.maxpool(g)\n",
    "        g = g.view(-1, ch // 2, h * w // 4)\n",
    "        # Attn_g\n",
    "        attn_g = torch.bmm(g, attn.permute(0, 2, 1))\n",
    "        attn_g = attn_g.view(-1, ch // 2, h, w)\n",
    "        attn_g = self.conv1x1_attn(attn_g)\n",
    "        return x + self.sigma * attn_g\n",
    "\n",
    "def init_weights(modules, initialize):\n",
    "    for module in modules():\n",
    "        if (isinstance(module, nn.Conv2d) or isinstance(module, nn.ConvTranspose2d) or isinstance(module, nn.Linear)):\n",
    "            if initialize == \"ortho\":\n",
    "                init.orthogonal_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.fill_(0.)\n",
    "            elif initialize == \"N02\":\n",
    "                init.normal_(module.weight, 0, 0.02)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.fill_(0.)\n",
    "            elif initialize in [\"glorot\", \"xavier\"]:\n",
    "                init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.fill_(0.)\n",
    "            else:\n",
    "                pass\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            if initialize == \"ortho\":\n",
    "                init.orthogonal_(module.weight)\n",
    "            elif initialize == \"N02\":\n",
    "                init.normal_(module.weight, 0, 0.02)\n",
    "            elif initialize in [\"glorot\", \"xavier\"]:\n",
    "                init.xavier_uniform_(module.weight)\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "def conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "    return nn.Conv2d(in_channels=in_channels,\n",
    "                     out_channels=out_channels,\n",
    "                     kernel_size=kernel_size,\n",
    "                     stride=stride,\n",
    "                     padding=padding,\n",
    "                     dilation=dilation,\n",
    "                     groups=groups,\n",
    "                     bias=bias)\n",
    "\n",
    "\n",
    "def deconv2d(in_channels, out_channels, kernel_size, stride=2, padding=0, dilation=1, groups=1, bias=True):\n",
    "    return nn.ConvTranspose2d(in_channels=in_channels,\n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=kernel_size,\n",
    "                              stride=stride,\n",
    "                              padding=padding,\n",
    "                              dilation=dilation,\n",
    "                              groups=groups,\n",
    "                              bias=bias)\n",
    "\n",
    "\n",
    "def linear(in_features, out_features, bias=True):\n",
    "    return nn.Linear(in_features=in_features, out_features=out_features, bias=bias)\n",
    "\n",
    "\n",
    "def embedding(num_embeddings, embedding_dim):\n",
    "    return nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim)\n",
    "\n",
    "\n",
    "def snconv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "    return spectral_norm(nn.Conv2d(in_channels=in_channels,\n",
    "                                   out_channels=out_channels,\n",
    "                                   kernel_size=kernel_size,\n",
    "                                   stride=stride,\n",
    "                                   padding=padding,\n",
    "                                   dilation=dilation,\n",
    "                                   groups=groups,\n",
    "                                   bias=bias),\n",
    "                         eps=1e-6)\n",
    "\n",
    "\n",
    "def sndeconv2d(in_channels, out_channels, kernel_size, stride=2, padding=0, dilation=1, groups=1, bias=True):\n",
    "    return spectral_norm(nn.ConvTranspose2d(in_channels=in_channels,\n",
    "                                            out_channels=out_channels,\n",
    "                                            kernel_size=kernel_size,\n",
    "                                            stride=stride,\n",
    "                                            padding=padding,\n",
    "                                            dilation=dilation,\n",
    "                                            groups=groups,\n",
    "                                            bias=bias),\n",
    "                         eps=1e-6)\n",
    "\n",
    "\n",
    "def snlinear(in_features, out_features, bias=True):\n",
    "    return spectral_norm(nn.Linear(in_features=in_features, out_features=out_features, bias=bias), eps=1e-6)\n",
    "\n",
    "\n",
    "def sn_embedding(num_embeddings, embedding_dim):\n",
    "    return spectral_norm(nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim), eps=1e-6)\n",
    "\n",
    "\n",
    "def batchnorm_2d(in_features, eps=1e-4, momentum=0.1, affine=True):\n",
    "    return nn.BatchNorm2d(in_features, eps=eps, momentum=momentum, affine=affine, track_running_stats=True)\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, lr_org, epoch, total_epoch, dataset):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    if dataset in [\"CIFAR10\", \"CIFAR100\"]:\n",
    "        lr = lr_org * (0.1 ** (epoch // (total_epoch * 0.5))) * (0.1 ** (epoch // (total_epoch * 0.75)))\n",
    "    elif dataset in [\"Tiny_ImageNet\", \"ImageNet\"]:\n",
    "        if total_epoch == 300:\n",
    "            lr = lr_org * (0.1 ** (epoch // 75))\n",
    "        else:\n",
    "            lr = lr_org * (0.1 ** (epoch // 30))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def quantize_images(x):\n",
    "    x = (x + 1)/2\n",
    "    x = (255.0*x + 0.5).clamp(0.0, 255.0)\n",
    "    x = x.detach().cpu().numpy().astype(np.uint8)\n",
    "    return x\n",
    "\n",
    "def make_GAN_untrainable(Gen, Gen_ema, Dis):\n",
    "    Gen.eval()\n",
    "    Gen.apply(set_deterministic_op_trainable)\n",
    "    if Gen_ema is not None:\n",
    "        Gen_ema.eval()\n",
    "        Gen_ema.apply(set_deterministic_op_trainable)\n",
    "\n",
    "    Dis.eval()\n",
    "    Dis.apply(set_deterministic_op_trainable)\n",
    "\n",
    "def resize_images(x, resizer, ToTensor, mean, std, device):\n",
    "    x = x.transpose((0, 2, 3, 1))\n",
    "    x = list(map(lambda x: ToTensor(resizer(x)), list(x)))\n",
    "    x = torch.stack(x, 0).to(device)\n",
    "    x = (x/255.0 - mean)/std\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Environment\n",
    "We develop a training environment that will assist us in both training the GAN and also evaluating the performance and progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "class FIDCalculator:\n",
    "\tdef __init__(self, dataloader):\n",
    "\t\tself.fid = FrechetInceptionDistance(feature = 64, reset_real_features=False)\n",
    "\t\tself.dataloader = dataloader\n",
    "\n",
    "\tdef setup(self):\n",
    "\t\ttotal_instance = len(self.dataloader.dataset)\n",
    "\t\tdata_iter = iter(self.dataloader)\n",
    "\t\tacts = []\n",
    "\t\twhile True:\n",
    "\t\t\ttry:\n",
    "\t\t\t\timages, labels = next(data_iter)\n",
    "\t\t\texcept StopIteration:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tacts.append(images)\n",
    "\n",
    "\t\tacts = torch.cat(acts, dim = 0)\n",
    "\t\tself.fid.update(acts[:total_instance], real = True)\n",
    "\t\t\n",
    "\tdef calc(self, generator, cfgs):\n",
    "\t\ttotal_instance = len(self.dataloader.dataset)\n",
    "\t\tgenerator.eval()\n",
    "\t\tnum_batches = math.ceil(float(total_instance) / float(cfgs.OPTIMIZATION.batch_size))\n",
    "\n",
    "\t\tacts = []\n",
    "\t\tfor i in tqdm(range(0, num_batches)):\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\tmake_GAN_untrainable(generator)\n",
    "\n",
    "\t\t\t\tzs, fake_labels, zs_eps = sample_zy(z_prior = \"gaussian\",\n",
    "\t      \t\t\t\t  batch_size = cfgs.OPTIMIZATION.batch_size,\n",
    "\t\t\t\t\t\t  z_dim = cfgs.MODEL.z_dim,\n",
    "\t\t\t\t\t\t  num_classes = cfgs.DATA.num_classes,\n",
    "\t\t\t\t\t\t  truncation_factor = -1,\n",
    "\t\t\t\t\t\t  y_sampler = \"totally_random\",\n",
    "\t\t\t\t\t\t  radius = \"N/A\",\n",
    "\t\t\t\t\t\t  device = torch.device('cuda')\n",
    "\t\t\t\t\t)\n",
    "\n",
    "\t\t\t\tfake_images = generator(zs, fake_labels, eval = True)\n",
    "\n",
    "\t\t\t\tacts.append(fake_images)\n",
    "\n",
    "\t\tacts = torch.cat(acts, dim = 0)\n",
    "\t\tacts = acts.detach().cpu().numpy()[:total_instance]\n",
    "\t\tself.fid.update(acts[:total_instance], real = False)\n",
    "\t\tfid_score = self.fid.compute()\n",
    "\t\tself.fid.reset()\n",
    "\n",
    "\t\treturn fid_score\n",
    "\n",
    "class Trainer:\n",
    "\tdef __init__(self):\n",
    "\t\tself.dis = None\n",
    "\t\tself.gen = None\n",
    "\t\tself.real_label = 1\n",
    "\t\tself.fake_label = 0\n",
    "\t\tself.G_losses = []\n",
    "\t\tself.D_losses = []\n",
    "\n",
    "\t\tself.real_image_basket = None\n",
    "\t\tself.real_label_basket = None\n",
    "\n",
    "\n",
    "\t\tself.start_time = datetime.now()\n",
    "\t\tself.device = torch.device('cuda')\n",
    "\n",
    "\tdef prepare_train_iter(self, epoch_counter):\n",
    "\t\tself.epoch_counter = epoch_counter\n",
    "\t\tself.train_iter = iter(self.train_dataloader)\n",
    "\n",
    "\tdef sample_data_basket(self):\n",
    "\t\ttry:\n",
    "\t\t\treal_image_basket, real_label_basket = next(self.train_iter)\n",
    "\t\texcept StopIteration:\n",
    "\t\t\tself.epoch_counter += 1\n",
    "\t\t\tself.train_iter = iter(self.train_dataloader)\n",
    "\t\t\treal_image_basket, real_label_basket = next(self.train_iter)\n",
    "\n",
    "\n",
    "\t\treal_image_basket = torch.split(real_image_basket, self.OPTIMIZATION.batch_size)\n",
    "\t\treal_label_basket = torch.split(real_label_basket, self.OPTIMIZATION.batch_size)\n",
    "\n",
    "\t\treturn real_image_basket, real_label_basket\n",
    "\n",
    "\tdef train_discriminator(self, current_step):\n",
    "\t\tbatch_counter = 0\n",
    "\n",
    "\t\tmake_GAN_trainable(self.Gen, self.Gen_ema, self.Dis)\n",
    "\t\ttoggle_grad(self.Gen, grad=False, num_freeze_layers=-1, is_stylegan=False)\n",
    "\t\ttoggle_grad(self.Dis, grad=True, num_freeze_layers=-1, is_stylegan=False)\n",
    "\n",
    "\t\treal_image_basket, real_label_basket = self.sample_data_basket()\n",
    "\n",
    "\t\tfor step_index in range(self.OPTIMIZATION.d_updates_per_step):\n",
    "\t\t\tself.OPTIMIZATION.d_optimizer.zero_grad()\n",
    "\n",
    "\t\t\twith torch.cuda.amp.autocast() as mpc:\n",
    "\t\t\t\treal_images = real_image_basket[batch_counter].to(self.device)\n",
    "\t\t\t\treal_labels = real_label_basket[batch_counter].to(self.device)\n",
    "\n",
    "\t\t\t\tfake_images, fake_labels, fake_images_eps, trsp_cost, ws, _, _ = generate_images(\n",
    "\t\t\t\t\tz_prior = self.MODEL.z_prior,\n",
    "\t\t\t\t\ttruncation_factor = -1.0,\n",
    "\t\t\t\t\tbatch_size = self.OPTIMIZATION.batch_size,\n",
    "\t\t\t\t\tz_dim = self.MODEL.z_dim,\n",
    "\t\t\t\t\tnum_classes = self.DATA.num_classes,\n",
    "\t\t\t\t\ty_sampler = \"totally_random\",\n",
    "\t\t\t\t\tradius = self.LOSS.radius,\n",
    "\t\t\t\t\tgenerator = self.Gen,\n",
    "\t\t\t\t\tdiscriminator = self.Dis,\n",
    "\t\t\t\t\tis_train = True,\n",
    "\t\t\t\t\tLOSS = self.LOSS,\n",
    "\t\t\t\t\tRUN=self.RUN,\n",
    "\t\t\t\t\tMODEL = self.MODEL,\n",
    "\t\t\t\t\tdevice = self.device,\n",
    "\t\t\t\t\tis_stylegan = False,\n",
    "\t\t\t\t\tcal_trsp_cost = True if self.LOSS.apply_lo else False\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\t\t\tif self.LOSS.apply_r1_reg:\n",
    "\t\t\t\t\treal_images.requires_grad_(True)\n",
    "\n",
    "\t\t\t\treal_images_ = self.AUG.series_augment(real_images)\n",
    "\t\t\t\tfake_images_ = self.AUG.series_augment(fake_images)\n",
    "\n",
    "\t\t\t\treal_dict = self.Dis(real_images_, real_labels)\n",
    "\t\t\t\tfake_dict = self.Dis(fake_images_, fake_labels, adc_fake = self.adc_fake)\n",
    "\n",
    "\t\t\t\tif self.AUG.apply_ada:\n",
    "\t\t\t\t\tself.dis_sign_real += torch.tensor((real_dict[\"adv_output\"].sign().sum().item(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tself.OPTIMIZATION.batch_size),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tdevice=self.device)\n",
    "\t\t\t\t\tself.dis_sign_fake += torch.tensor((fake_dict[\"adv_output\"].sign().sum().item(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tself.OPTIMIZATION.batch_size),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tdevice=self.device)\n",
    "\t\t\t\t\tself.dis_logit_real += torch.tensor((real_dict[\"adv_output\"].sum().item(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tself.OPTIMIZATION.batch_size),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tdevice=self.device)\n",
    "\t\t\t\t\tself.dis_logit_fake += torch.tensor((fake_dict[\"adv_output\"].sum().item(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tself.OPTIMIZATION.batch_size),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tdevice=self.device)\n",
    "\n",
    "\t\t\t\t# Multi-hinge loss\n",
    "\t\t\t\tif self.LOSS.adv_loss == \"MH\":\n",
    "\t\t\t\t\tdis_acml_loss = self.LOSS.d_loss(DDP=self.DDP, **real_dict)\n",
    "\t\t\t\t\tdis_acml_loss += self.LOSS.d_loss(fake_dict[\"adv_output\"], self.lossy, DDP=self.DDP)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tdis_acml_loss = self.LOSS.d_loss(real_dict[\"adv_output\"], fake_dict[\"adv_output\"], DDP=self.DDP)\n",
    "\n",
    "\t\t\t\t# Class conditional loss\n",
    "\t\t\t\tif self.MODEL.d_cond_mtd in self.MISC.classifier_based_GAN:\n",
    "\t\t\t\t\treal_cond_loss = self.cond_loss(**real_dict)\n",
    "\t\t\t\t\tdis_acml_loss += self.LOSS.cond_lambda * real_cond_loss\n",
    "\t\t\t\t\tif self.MODEL.aux_cls_type == \"TAC\":\n",
    "\t\t\t\t\t\ttac_dis_loss = self.cond_loss_mi(**fake_dict)\n",
    "\t\t\t\t\t\tdis_acml_loss += self.LOSS.tac_dis_lambda * tac_dis_loss\n",
    "\t\t\t\t\telif self.MODEL.aux_cls_type == \"ADC\":\n",
    "\t\t\t\t\t\tfake_cond_loss = self.cond_loss(**fake_dict)\n",
    "\t\t\t\t\t\tdis_acml_loss += self.LOSS.cond_lambda * fake_cond_loss\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tpass\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\treal_cond_loss = \"N/A\"\n",
    "\n",
    "\t\t\t\t# Gradient Penalty Regularization for Lipschitz GAN\n",
    "\t\t\t\tif self.LOSS.apply_maxgp:\n",
    "\t\t\t\t\tmaxgp_loss = cal_maxgrad_penalty(real_images=real_images,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\treal_labels=real_labels,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfake_images=fake_images,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdiscriminator=self.Dis,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdevice=self.device)\n",
    "\t\t\t\t\tdis_acml_loss += self.LOSS.maxgp_lambda * maxgp_loss\n",
    "\n",
    "\t\t\t\tif self.LOSS.apply_r1_reg and not self.is_stylegan:\n",
    "\t\t\t\t\tself.r1_penalty = cal_r1_reg(adv_output=real_dict[\"adv_output\"], images=real_images, device=self.device)\n",
    "\t\t\t\t\tdis_acml_loss += self.LOSS.r1_lambda*self.r1_penalty\n",
    "\n",
    "\t\t\t\t# adjust gradients for applying gradient accumluation trick\n",
    "\t\t\t\tdis_acml_loss = dis_acml_loss / self.OPTIMIZATION.acml_steps\n",
    "\t\t\t\tbatch_counter += 1\n",
    "\n",
    "\t\t\t# accumulate gradients of the discriminator\n",
    "\t\t\tif self.RUN.mixed_precision and not self.is_stylegan:\n",
    "\t\t\t\tself.scaler.scale(dis_acml_loss).backward()\n",
    "\t\t\telse:\n",
    "\t\t\t\tdis_acml_loss.backward()\n",
    "\n",
    "\t\t\t# update the discriminator using the pre-defined optimizer\n",
    "\t\t\tif self.RUN.mixed_precision and not self.is_stylegan:\n",
    "\t\t\t\tself.scaler.step(self.OPTIMIZATION.d_optimizer)\n",
    "\t\t\t\tself.scaler.update()\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.OPTIMIZATION.d_optimizer.step()\n",
    "\n",
    "\t\t\t# weight clipping for discriminator 1-lipschitz constraint\n",
    "\t\t\tif self.LOSS.apply_wc:\n",
    "\t\t\t\tfor p in self.Dis.parameters():\n",
    "\t\t\t\t\tp.data.clamp_(-self.LOSS.wc_bound, self.LOSS.wc_bound)\n",
    "\n",
    "\t\t# empty cache to discard used memory\n",
    "\t\t# if self.RUN.empty_cache:\n",
    "\t\t# \ttorch.cuda.empty_cache()\n",
    "\t\treturn real_cond_loss, dis_acml_loss\n",
    "\n",
    "\tdef train_generator(self, current_step):\n",
    "\t\tmake_GAN_trainable(self.Gen, self.Gen_ema, self.Dis)\n",
    "\n",
    "\t\ttoggle_grad(model=self.Dis, grad=False, num_freeze_layers=-1, is_stylegan=False)\n",
    "\t\ttoggle_grad(model=self.Gen, grad=True, num_freeze_layers=-1, is_stylegan=False)\n",
    "\n",
    "\t\tif self.MODEL.info_type in [\"discrete\", \"both\"]:\n",
    "\t\t\ttoggle_grad(getattr(peel_model(self.Dis), self.MISC.info_params[0]), grad=True, num_freeze_layers=-1, is_stylegan=False)\n",
    "\t\tif self.MODEL.info_type in [\"continuous\", \"both\"]:\n",
    "\t\t\ttoggle_grad(getattr(peel_model(self.Dis), self.MISC.info_params[1]), grad=True, num_freeze_layers=-1, is_stylegan=False)\n",
    "\t\t\ttoggle_grad(getattr(peel_model(self.Dis), self.MISC.info_params[2]), grad=True, num_freeze_layers=-1, is_stylegan=False)\n",
    "\n",
    "\n",
    "\t\tself.Gen.apply(track_bn_statistics)\n",
    "\t\tfor step_index in range(self.OPTIMIZATION.g_updates_per_step):\n",
    "\t\t\tself.OPTIMIZATION.g_optimizer.zero_grad()\n",
    "\t\t\twith torch.cuda.amp.autocast() as mpc:\n",
    "\n",
    "\t\t\t\tfake_images, fake_labels, fake_images_eps, trsp_cost, ws, info_discrete_c, info_conti_c = generate_images(\n",
    "\t\t\t\t\tz_prior = self.MODEL.z_prior,\n",
    "\t\t\t\t\ttruncation_factor = -1.0,\n",
    "\t\t\t\t\tbatch_size = self.OPTIMIZATION.batch_size,\n",
    "\t\t\t\t\tz_dim = self.MODEL.z_dim,\n",
    "\t\t\t\t\tnum_classes = self.DATA.num_classes,\n",
    "\t\t\t\t\ty_sampler = \"totally_random\",\n",
    "\t\t\t\t\tradius = self.LOSS.radius,\n",
    "\t\t\t\t\tgenerator = self.Gen,\n",
    "\t\t\t\t\tdiscriminator = self.Dis,\n",
    "\t\t\t\t\tis_train = True,\n",
    "\t\t\t\t\tLOSS = self.LOSS,\n",
    "\t\t\t\t\tRUN=self.RUN,\n",
    "\t\t\t\t\tMODEL = self.MODEL,\n",
    "\t\t\t\t\tdevice = self.device,\n",
    "\t\t\t\t\tis_stylegan = False,\n",
    "\t\t\t\t\tcal_trsp_cost = True if self.LOSS.apply_lo else False\n",
    "\t\t\t\t)\n",
    "\n",
    "\n",
    "\t\t\t\t# Diff augment\n",
    "\t\t\t\tfake_images_ = self.AUG.series_augment(fake_images)\n",
    "\n",
    "\t\t\t\tfake_dict = self.Dis(fake_images_, fake_labels)\n",
    "\n",
    "\t\t\t\tif self.AUG.apply_ada:\n",
    "\t\t\t\t\tself.dis_sign_fake += torch.tensor((fake_dict[\"adv_output\"].sign().sum().item(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tself.OPTIMIZATION.batch_size),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tdevice=self.device)\n",
    "\t\t\t\t\tself.dis_logit_fake += torch.tensor((fake_dict[\"adv_output\"].sum().item(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tself.OPTIMIZATION.batch_size),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tdevice=self.device)\n",
    "\n",
    "\t\t\t\t# Multi-hinge loss\n",
    "\t\t\t\tif self.LOSS.adv_loss == \"MH\":\n",
    "\t\t\t\t\tgen_acml_loss = self.LOSS.mh_lambda * self.LOSS.g_loss(DDP=self.DDP, **fake_dict, )\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tgen_acml_loss = self.LOSS.g_loss(fake_dict[\"adv_output\"], DDP=self.DDP)\n",
    "\n",
    "\t\t\t\t# calculate class conditioning loss defined by \"MODEL.d_cond_mtd\"\n",
    "\t\t\t\tif self.MODEL.d_cond_mtd in self.MISC.classifier_based_GAN:\n",
    "\t\t\t\t\tfake_cond_loss = self.cond_loss(**fake_dict)\n",
    "\t\t\t\t\tgen_acml_loss += self.LOSS.cond_lambda * fake_cond_loss\n",
    "\t\t\t\t\tif self.MODEL.aux_cls_type == \"TAC\":\n",
    "\t\t\t\t\t\ttac_gen_loss = -self.cond_loss_mi(**fake_dict)\n",
    "\t\t\t\t\t\tgen_acml_loss += self.LOSS.tac_gen_lambda * tac_gen_loss\n",
    "\t\t\t\t\telif self.MODEL.aux_cls_type == \"ADC\":\n",
    "\t\t\t\t\t\tadc_fake_dict = self.Dis(fake_images_, fake_labels, adc_fake=self.adc_fake)\n",
    "\t\t\t\t\t\tadc_fake_cond_loss = -self.cond_loss(**adc_fake_dict)\n",
    "\t\t\t\t\t\tgen_acml_loss += self.LOSS.cond_lambda * adc_fake_cond_loss\n",
    "\t\t\t\t\tpass\n",
    "\n",
    "\t\t\t# accumulate gradients of the generator\n",
    "\t\t\tif self.RUN.mixed_precision and not self.is_stylegan:\n",
    "\t\t\t\tself.scaler.scale(gen_acml_loss).backward()\n",
    "\t\t\telse:\n",
    "\t\t\t\tgen_acml_loss.backward()\n",
    "\n",
    "\t\t\t# update the generator using the pre-defined optimizer\n",
    "\t\t\tif self.RUN.mixed_precision and not self.is_stylegan:\n",
    "\t\t\t\tself.scaler.step(self.OPTIMIZATION.g_optimizer)\n",
    "\t\t\t\tself.scaler.update()\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.OPTIMIZATION.g_optimizer.step()\n",
    "\n",
    "\t\t\t# EMA\n",
    "\t\t\tif self.MODEL.apply_g_ema:\n",
    "\t\t\t\tself.ema.update(current_step)\n",
    "\t\n",
    "\t\t# if self.RUN.empty_cache:\n",
    "\t\t# \ttorch.cuda.empty_cache()\n",
    "\n",
    "\t\treturn gen_acml_loss\n",
    "\n",
    "\tdef load(self, cfgs, run_name, backbone):\n",
    "\n",
    "\t\tself.random_gen = torch.Generator()\n",
    "\t\tself.random_gen.manual_seed(1337)\n",
    "\n",
    "\t\ttime = str(datetime.now()).replace(' ', '_').replace(':', '-')\n",
    "\t\tself.model_name = run_name\n",
    "\t\tself.run_name = f\"{run_name}_{time}\"\n",
    "\n",
    "\t\tcfgs.RUN.save_dir = 'saves'\n",
    "\t\tlogger = make_logger(cfgs.RUN.save_dir, self.run_name, None)\n",
    "\t\tcfgs.RUN.num_workers = 1\n",
    "\t\tcfgs.RUN.print_freq = 500\n",
    "\n",
    "\t\tGen, Dis, Gen_ema, ema = load_generator_discriminator(\n",
    "\t\t\tcfgs.DATA,\n",
    "\t\t\tcfgs.OPTIMIZATION,\n",
    "\t\t\tcfgs.MODEL,\n",
    "\t\t\tcfgs.STYLEGAN,\n",
    "\t\t\tcfgs.MODULES,\n",
    "\t\t\tcfgs.RUN,\n",
    "\t\t\ttorch.device('cuda'),\n",
    "\t\t\tNone,\n",
    "\t\t\tbackbone.Generator,\n",
    "\t\t\tbackbone.Discriminator\n",
    "\t\t)\n",
    "\n",
    "\t\tcfgs.define_optimizer(Gen, Dis)\n",
    "\n",
    "\n",
    "\t\tcfgs.OPTIMIZATION.basket_size = cfgs.OPTIMIZATION.batch_size*\\\n",
    "\t\t\t\t\t\t\t\t\t\tcfgs.OPTIMIZATION.acml_steps*\\\n",
    "\t\t\t\t\t\t\t\t\t\tcfgs.OPTIMIZATION.d_updates_per_step\n",
    "\n",
    "\t\tself.device = torch.device('cuda')\n",
    "\t\tself.logger = logger\n",
    "\n",
    "\t\tself.cfgs = cfgs\n",
    "\t\tself.Gen = Gen\n",
    "\t\tself.Dis = Dis\n",
    "\t\tself.Gen_ema = Gen_ema\n",
    "\t\tself.ema = ema\n",
    "\n",
    "\t\tself.load_dataset()\n",
    "\n",
    "\t\tself.cfgs.define_augments(self.device)\n",
    "\t\tself.cfgs.define_losses()\n",
    "\t\tself.DATA = cfgs.DATA\n",
    "\t\tself.MODEL = cfgs.MODEL\n",
    "\t\tself.LOSS = cfgs.LOSS\n",
    "\t\tself.STYLEGAN = cfgs.STYLEGAN\n",
    "\t\tself.OPTIMIZATION = cfgs.OPTIMIZATION\n",
    "\t\tself.PRE = cfgs.PRE\n",
    "\t\tself.AUG = cfgs.AUG\n",
    "\t\tself.RUN = cfgs.RUN\n",
    "\t\tself.MISC = cfgs.MISC\n",
    "\t\tself.is_stylegan = False\n",
    "\t\tself.effective_batch_size = self.OPTIMIZATION.batch_size * self.OPTIMIZATION.acml_steps\n",
    "\t\tself.DDP = self.RUN.distributed_data_parallel\n",
    "\t\tself.adc_fake = False\n",
    "\n",
    "\t\tnum_classes = self.DATA.num_classes\n",
    "\n",
    "\n",
    "\t\tself.l2_loss = torch.nn.MSELoss()\n",
    "\t\tself.ce_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\t\tif self.LOSS.adv_loss == \"MH\":\n",
    "\t\t\tself.lossy = torch.LongTensor(self.OPTIMIZATION.batch_size).to(self.device)\n",
    "\t\t\tself.lossy.data.fill_(self.DATA.num_classes)\n",
    "\n",
    "\t\tif self.AUG.apply_ada:\n",
    "\t\t\tif self.AUG.apply_ada:\n",
    "\t\t\t\tself.AUG.series_augment.p.copy_(torch.as_tensor(self.aa_p))\n",
    "\t\t\tself.aa_interval = self.AUG.ada_interval if self.AUG.ada_interval != \"N/A\" else self.AUG.apa_interval\n",
    "\t\t\tself.aa_target = self.AUG.ada_target if self.AUG.ada_target != \"N/A\" else self.AUG.apa_target\n",
    "\t\t\tself.aa_kimg = self.AUG.ada_kimg if self.AUG.ada_kimg != \"N/A\" else self.AUG.apa_kimg\n",
    "\t\t\tself.dis_sign_real, self.dis_sign_fake = torch.zeros(2, device=self.device), torch.zeros(2, device=self.device)\n",
    "\t\t\tself.dis_logit_real, self.dis_logit_fake = torch.zeros(2, device=self.device), torch.zeros(2, device=self.device)\n",
    "\t\t\tself.dis_sign_real_log, self.dis_sign_fake_log = torch.zeros(2, device=self.device), torch.zeros(2, device=self.device)\n",
    "\t\t\tself.dis_logit_real_log, self.dis_logit_fake_log = torch.zeros(2, device=self.device), torch.zeros(2, device=self.device)\n",
    "\n",
    "\t\tif self.MODEL.aux_cls_type == \"ADC\":\n",
    "\t\t\tnum_classes = num_classes * 2\n",
    "\t\t\tself.adc_fake = True\n",
    "\n",
    "\t\tif self.MODEL.d_cond_mtd == \"AC\":\n",
    "\t\t\tself.cond_loss = CrossEntropyLoss()\n",
    "\t\telif self.MODEL.d_cond_mtd == \"D2DCE\":\n",
    "\t\t\tself.cond_loss = Data2DataCrossEntropyLoss(\n",
    "\t\t\t\tnum_classes = num_classes,\n",
    "\t\t\t\ttemperature = self.LOSS.temperature,\n",
    "\t\t\t\tm_p = self.LOSS.m_p,\n",
    "\t\t\t\tmaster_rank = 'cuda',\n",
    "\t\t\t\tDDP = self.DDP\n",
    "\t\t\t)\n",
    "\n",
    "\t\tif self.MODEL.aux_cls_type == \"TAC\":\n",
    "\t\t\tself.cond_loss_mi = copy.deepcopy(self.cond_loss)\n",
    "\n",
    "\t\tself.gen_eval = self.Gen_ema if self.MODEL.apply_g_ema else self.Gen\n",
    "\n",
    "\t\tif self.DDP:\n",
    "\t\t\tself.group = dist.new_group([n for n in range(self.OPTIMIZATION.world_size)])\n",
    "\n",
    "\t\tif self.RUN.mixed_precision and not self.is_stylegan:\n",
    "\t\t\tself.scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "\t\t# if self.global_rank == 0:\n",
    "\t\t# \tresume = False if self.RUN.freezeD > -1 else True\n",
    "\t\t\t# wandb.init(project=self.RUN.project,\n",
    "\t\t\t# \t\t\tentity=self.RUN.entity,\n",
    "\t\t\t# \t\t\tname=self.run_name,\n",
    "\t\t\t# \t\t\tdir=self.RUN.save_dir,\n",
    "\t\t\t# \t\t\tresume=self.best_step > 0 and resume)\n",
    "\n",
    "\t\tself.start_time = datetime.now()\n",
    "\n",
    "\tdef load_dataset(self):\n",
    "\t\tpreprop = transforms.Compose([\n",
    "\t\t\ttransforms.ToTensor(),\n",
    "\t\t\ttransforms.Lambda(lambda x: x * 2 - 1)\n",
    "\t\t])\n",
    "\t\ttrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tdownload=True, transform=preprop)\n",
    "\n",
    "\n",
    "\t\tself.train_dataloader = torch.utils.data.DataLoader(trainset, batch_size=self.cfgs.OPTIMIZATION.basket_size, drop_last=True)\n",
    "\n",
    "\t\n",
    "\tdef log_train_statistics(self, current_step, real_cond_loss, gen_acml_loss, dis_acml_loss):\n",
    "\t\tself.log_step = current_step + 1\n",
    "\t\tif self.MODEL.d_cond_mtd in self.MISC.classifier_based_GAN:\n",
    "\t\t\tcls_loss = real_cond_loss.item()\n",
    "\t\telse:\n",
    "\t\t\tcls_loss = \"N/A\"\n",
    "\n",
    "\t\tLOG_FORMAT = (\n",
    "\t\t\t\"Epoch: {epoch} | \"\n",
    "\t\t\t\"Step: {step:>6} \"\n",
    "\t\t\t\"Progress: {progress:<.1%} \"\n",
    "\t\t\t\"Elapsed: {elapsed} \"\n",
    "\t\t\t\"Gen_loss: {gen_loss:<.4} \"\n",
    "\t\t\t\"Dis_loss: {dis_loss:<.4} \"\n",
    "\t\t\t\"Cls_loss: {cls_loss:<.4} \"\n",
    "\t\t)\n",
    "\n",
    "\t\tlog_message = LOG_FORMAT.format(\n",
    "\t\t\tepoch = self.epoch_counter,\n",
    "\t\t\tstep = self.log_step,\n",
    "\t\t\tprogress = (current_step + 1) / self.OPTIMIZATION.total_steps,\n",
    "\t\t\telapsed = self.elapsed_time(),\n",
    "\t\t\tgen_loss = gen_acml_loss.item(),\n",
    "\t\t\tdis_loss = dis_acml_loss.item(),\n",
    "\t\t\tcls_loss = cls_loss\n",
    "\t\t)\n",
    "\t\tself.logger.info(log_message)\n",
    "\t\t\n",
    "\t\tloss_dict = {\n",
    "\t\t\t\"gen_loss\": gen_acml_loss.item(),\n",
    "\t\t\t\"dis_loss\": dis_acml_loss.item(),\n",
    "\t\t\t\"cls_loss\": 0.0 if cls_loss == \"N/A\" else cls_loss\n",
    "\t\t}\n",
    "\n",
    "\t\t# wandb.log(loss_dict, step = self.log_step)\n",
    "\n",
    "\t\tinfoGAN_dict = {}\n",
    "\t\tif self.MODEL.info_type in ['discrete', 'both']:\n",
    "\t\t\tinfoGAN_dict[\"info_discrete_loss\"] + self.info_discrete_loss.item()\n",
    "\n",
    "\t\tif self.MODEL.info_type in ['continuous', 'both']:\n",
    "\t\t\tinfoGAN_dict[\"info_conti_loss\"] = self.info_conti_loss.item()\n",
    "\n",
    "\t\t# if self.LOSS.apply_r1_reg:\n",
    "\t\t# \twandb.log({\"r1_reg_loss\": self.r1_penalty.item()}, step=self.log_step)\n",
    "\n",
    "\t\tif self.MODEL.apply_g_sn:\n",
    "\t\t\tgen_sigmas = calculate_all_sn(self.Gen, prefix=\"Gen\")\n",
    "\n",
    "\t\tif self.MODEL.apply_d_sn:\n",
    "\t\t\tdis_sigmas = calculate_all_sn(self.Dis, prefix=\"Dis\")\n",
    "\n",
    "\tdef train(self):\n",
    "\n",
    "\t\tstep = 0\n",
    "\t\tself.training = True\n",
    "\t\tself.prepare_train_iter(epoch_counter = 0)\n",
    "\n",
    "\t\twhile step <= self.cfgs.OPTIMIZATION.total_steps:\n",
    "\t\t\tif self.cfgs.OPTIMIZATION.d_first:\n",
    "\t\t\t\treal_cond_loss, dis_acml_loss = self.train_discriminator(current_step=step)\n",
    "\t\t\t\tgen_acml_loss = self.train_generator(current_step=step)\n",
    "\t\t\telse:\n",
    "\t\t\t\tgen_acml_loss = self.train_generator(current_step=step)\n",
    "\t\t\t\treal_cond_loss, dis_acml_loss = self.train_discriminator(current_step=step)\n",
    "\n",
    "\t\t\tif (step + 1) % self.cfgs.RUN.print_freq == 0:\n",
    "\t\t\t\tself.log_train_statistics(\n",
    "\t\t\t\t\tcurrent_step = step,\n",
    "\t\t\t\t\treal_cond_loss = real_cond_loss,\n",
    "\t\t\t\t\tgen_acml_loss=gen_acml_loss,\n",
    "\t\t\t\t\tdis_acml_loss=dis_acml_loss\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\t\tif (step) % 1000 == 0:\n",
    "\t\t\t\tself.visualize_fake(10, step, self.run_name)\n",
    "\t\t\tstep += 1\n",
    "\n",
    "\n",
    "\tdef elapsed_time(self):\n",
    "\t\telapsed = datetime.now() - self.start_time\n",
    "\t\treturn str(elapsed).split(\".\")[0]  # remove milliseconds \n",
    "\n",
    "\tdef plot_img_canvas(self, images, save_path, num_cols, logging=True):\n",
    "\t\tif self.logger is None:\n",
    "\t\t\tlogging = False\n",
    "\t\tdirectory = dirname(save_path)\n",
    "\n",
    "\t\tif not exists(directory):\n",
    "\t\t\tos.makedirs(directory)\n",
    "\n",
    "\t\tsave_image(((images + 1)/2).clamp(0.0, 1.0), save_path, padding=0, nrow=num_cols)\n",
    "\n",
    "\t\tif logging:\n",
    "\t\t\tself.logger.info(\"Save image canvas to {}\".format(save_path))\n",
    "\n",
    "\tdef visualize_fake(self, num_cols, current_step, run_name):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tmake_GAN_untrainable(self.Gen, self.Gen_ema, self.Dis)\n",
    "\n",
    "\t\t\tfake_images, fake_labels, _, _, _, _, _ = generate_images(\n",
    "\t\t\t\tz_prior = \"fixed\",\n",
    "\t\t\t\ttruncation_factor = -1.0,\n",
    "\t\t\t\tbatch_size = self.OPTIMIZATION.batch_size,\n",
    "\t\t\t\tz_dim = self.MODEL.z_dim,\n",
    "\t\t\t\tnum_classes = self.DATA.num_classes,\n",
    "\t\t\t\ty_sampler = \"acending_all\",\n",
    "\t\t\t\tradius = \"N/A\",\n",
    "\t\t\t\tgenerator = self.gen_eval,\n",
    "\t\t\t\tdiscriminator = self.Dis,\n",
    "\t\t\t\tis_train = False,\n",
    "\t\t\t\tLOSS = self.LOSS,\n",
    "\t\t\t\tRUN=self.RUN,\n",
    "\t\t\t\tMODEL = self.MODEL,\n",
    "\t\t\t\tdevice = self.device,\n",
    "\t\t\t\tis_stylegan = False,\n",
    "\t\t\t\tcal_trsp_cost = True if self.LOSS.apply_lo else False,\n",
    "\t\t\t\trandom_gen = self.random_gen\n",
    "\t\t\t)\n",
    "\n",
    "\t\tnamed_fake_labels = classes[fake_labels.detach().cpu().numpy()]\n",
    "\t\tfig, ax = imshow(fake_images, named_fake_labels, figsize=(10, 15), fig_shape=(10, 8))\n",
    "\t\tfig.suptitle(f'{self.model_name} | Step {current_step + 1}')\n",
    "\n",
    "\t\tdir_path = join(self.RUN.save_dir, \"figures/{run_name}\".format(run_name=self.model_name))\n",
    "\t\tos.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "\t\tsave_path = join(self.RUN.save_dir, \"figures/{run_name}/generated_canvas_{step}.png\".format(run_name=self.model_name, step=current_step))\n",
    "\n",
    "\t\tfig.savefig(save_path)\n",
    "\t\tplt.close(fig)\n",
    "\n",
    "\t\tmake_GAN_trainable(self.Gen, self.Gen_ema, self.Dis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Configuration\n",
    "The configuration below, will allow us to determine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch StudioGAN: https://github.com/POSTECH-CVLab/PyTorch-StudioGAN\n",
    "# The MIT License (MIT)\n",
    "# See license file or visit https://github.com/POSTECH-CVLab/PyTorch-StudioGAN for details\n",
    "\n",
    "from itertools import chain\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class empty_object(object):\n",
    "\tpass\n",
    "\n",
    "class Configurations(object):\n",
    "\tdef __init__(self, cfg_file):\n",
    "\t\tself.cfg_file = cfg_file\n",
    "\t\tself.load_base_cfgs()\n",
    "\t\tself._overwrite_cfgs(self.cfg_file)\n",
    "\t\tself.define_modules()\n",
    "\n",
    "\tdef load_base_cfgs(self):\n",
    "\t\t# -----------------------------------------------------------------------------\n",
    "\t\t# Data settings\n",
    "\t\t# -----------------------------------------------------------------------------\n",
    "\t\tself.DATA = empty_object()\n",
    "\n",
    "\t\t# dataset name \\in [\"CIFAR10\", \"CIFAR100\", \"Tiny_ImageNet\", \"CUB200\", \"ImageNet\", \"MY_DATASET\"]\n",
    "\t\tself.DATA.name = \"CIFAR10\"\n",
    "\t\t# image size for training\n",
    "\t\tself.DATA.img_size = 32\n",
    "\t\t# number of classes in training dataset, if there is no explicit class label, DATA.num_classes = 1\n",
    "\t\tself.DATA.num_classes = 10\n",
    "\t\t# number of image channels in dataset. //image_shape[0]\n",
    "\t\tself.DATA.img_channels = 3\n",
    "\n",
    "\t\t# -----------------------------------------------------------------------------\n",
    "\t\t# Model settings\n",
    "\t\t# -----------------------------------------------------------------------------\n",
    "\t\tself.MODEL = empty_object()\n",
    "\n",
    "\t\t# type of backbone architectures of the generator and discriminator \\in\n",
    "\t\t# [\"deep_conv\", \"resnet\", \"big_resnet\", \"big_resnet_deep_legacy\", \"big_resnet_deep_studiogan\", \"stylegan2\", \"stylegan3\"]\n",
    "\t\tself.MODEL.backbone = \"resnet\"\n",
    "\t\t# conditioning method of the generator \\in [\"W/O\", \"cBN\", \"cAdaIN\"]\n",
    "\t\tself.MODEL.g_cond_mtd = \"W/O\"\n",
    "\t\t# conditioning method of the discriminator \\in [\"W/O\", \"AC\", \"PD\", \"MH\", \"MD\", \"2C\",\"D2DCE\", \"SPD\"]\n",
    "\t\tself.MODEL.d_cond_mtd = \"W/O\"\n",
    "\t\t# type of auxiliary classifier \\in [\"W/O\", \"TAC\", \"ADC\"]\n",
    "\t\tself.MODEL.aux_cls_type = \"W/O\"\n",
    "\t\t# whether to normalize feature maps from the discriminator or not\n",
    "\t\tself.MODEL.normalize_d_embed = False\n",
    "\t\t# dimension of feature maps from the discriminator\n",
    "\t\t# only appliable when MODEL.d_cond_mtd \\in [\"2C, D2DCE\"]\n",
    "\t\tself.MODEL.d_embed_dim = \"N/A\"\n",
    "\t\t# whether to apply spectral normalization on the generator\n",
    "\t\tself.MODEL.apply_g_sn = False\n",
    "\t\t# whether to apply spectral normalization on the discriminator\n",
    "\t\tself.MODEL.apply_d_sn = False\n",
    "\t\t# type of activation function in the generator \\in [\"ReLU\", \"Leaky_ReLU\", \"ELU\", \"GELU\"]\n",
    "\t\tself.MODEL.g_act_fn = \"ReLU\"\n",
    "\t\t# type of activation function in the discriminator \\in [\"ReLU\", \"Leaky_ReLU\", \"ELU\", \"GELU\"]\n",
    "\t\tself.MODEL.d_act_fn = \"ReLU\"\n",
    "\t\t# whether to apply self-attention proposed by zhang et al. (SAGAN)\n",
    "\t\tself.MODEL.apply_attn = False\n",
    "\t\t# locations of the self-attention layer in the generator (should be list type)\n",
    "\t\tself.MODEL.attn_g_loc = [\"N/A\"]\n",
    "\t\t# locations of the self-attention layer in the discriminator (should be list type)\n",
    "\t\tself.MODEL.attn_d_loc = [\"N/A\"]\n",
    "\t\t# prior distribution for noise sampling \\in [\"gaussian\", \"uniform\"]\n",
    "\t\tself.MODEL.z_prior = \"gaussian\"\n",
    "\t\t# dimension of noise vectors\n",
    "\t\tself.MODEL.z_dim = 128\n",
    "\t\t# dimension of intermediate latent (W) dimensionality used only for StyleGAN\n",
    "\t\tself.MODEL.w_dim = \"N/A\"\n",
    "\t\t# dimension of a shared latent embedding\n",
    "\t\tself.MODEL.g_shared_dim = \"N/A\"\n",
    "\t\t# base channel for the resnet style generator architecture\n",
    "\t\tself.MODEL.g_conv_dim = 64\n",
    "\t\t# base channel for the resnet style discriminator architecture\n",
    "\t\tself.MODEL.d_conv_dim = 64\n",
    "\t\t# generator's depth for \"models/big_resnet_deep_*.py\"\n",
    "\t\tself.MODEL.g_depth = \"N/A\"\n",
    "\t\t# discriminator's depth for \"models/big_resnet_deep_*.py\"\n",
    "\t\tself.MODEL.d_depth = \"N/A\"\n",
    "\t\t# whether to apply moving average update for the generator\n",
    "\t\tself.MODEL.apply_g_ema = False\n",
    "\t\t# decay rate for the ema generator\n",
    "\t\tself.MODEL.g_ema_decay = \"N/A\"\n",
    "\t\t# starting step for g_ema update\n",
    "\t\tself.MODEL.g_ema_start = \"N/A\"\n",
    "\t\t# weight initialization method for the generator \\in [\"ortho\", \"N02\", \"glorot\", \"xavier\"]\n",
    "\t\tself.MODEL.g_init = \"ortho\"\n",
    "\t\t# weight initialization method for the discriminator \\in [\"ortho\", \"N02\", \"glorot\", \"xavier\"]\n",
    "\t\tself.MODEL.d_init = \"ortho\"\n",
    "\t\t# type of information for infoGAN training \\in [\"N/A\", \"discrete\", \"continuous\", \"both\"]\n",
    "\t\tself.MODEL.info_type = \"N/A\"\n",
    "\t\t# way to inject information into Generator \\in [\"N/A\", \"concat\", \"cBN\"]\n",
    "\t\tself.MODEL.g_info_injection = \"N/A\"\n",
    "\t\t# number of discrete c to use in InfoGAN\n",
    "\t\tself.MODEL.info_num_discrete_c = \"N/A\"\n",
    "\t\t# number of continuous c to use in InfoGAN\n",
    "\t\tself.MODEL.info_num_conti_c = \"N/A\"\n",
    "\t\t# dimension of discrete c to use in InfoGAN (one-hot)\n",
    "\t\tself.MODEL.info_dim_discrete_c = \"N/A\"\n",
    "\n",
    "\t\t# -----------------------------------------------------------------------------\n",
    "\t\t# loss settings\n",
    "\t\t# -----------------------------------------------------------------------------\n",
    "\t\tself.LOSS = empty_object()\n",
    "\n",
    "\t\t# type of adversarial loss \\in [\"vanilla\", \"least_squere\", \"wasserstein\", \"hinge\", \"MH\"]\n",
    "\t\tself.LOSS.adv_loss = \"vanilla\"\n",
    "\t\t# balancing hyperparameter for conditional image generation\n",
    "\t\tself.LOSS.cond_lambda = \"N/A\"\n",
    "\t\t# strength of conditioning loss induced by twin auxiliary classifier for generator training\n",
    "\t\tself.LOSS.tac_gen_lambda = \"N/A\"\n",
    "\t\t# strength of conditioning loss induced by twin auxiliary classifier for discriminator training\n",
    "\t\tself.LOSS.tac_dis_lambda = \"N/A\"\n",
    "\t\t# strength of multi-hinge loss (MH) for the generator training\n",
    "\t\tself.LOSS.mh_lambda = \"N/A\"\n",
    "\t\t# whether to apply feature matching regularization\n",
    "\t\tself.LOSS.apply_fm = False\n",
    "\t\t# strength of feature matching regularization\n",
    "\t\tself.LOSS.fm_lambda = \"N/A\"\n",
    "\t\t# whether to apply r1 regularization used in multiple-discriminator (FUNIT)\n",
    "\t\tself.LOSS.apply_r1_reg = False\n",
    "\t\t# a place to apply the R1 regularization \\in [\"N/A\", \"inside_loop\", \"outside_loop\"]\n",
    "\t\tself.LOSS.r1_place = \"N/A\"\n",
    "\t\t# strength of r1 regularization (it does not apply to r1_reg in StyleGAN2\n",
    "\t\tself.LOSS.r1_lambda = \"N/A\"\n",
    "\t\t# positive margin for D2DCE\n",
    "\t\tself.LOSS.m_p = \"N/A\"\n",
    "\t\t# temperature scalar for [2C, D2DCE]\n",
    "\t\tself.LOSS.temperature = \"N/A\"\n",
    "\t\t# whether to apply weight clipping regularization to let the discriminator satisfy Lipschitzness\n",
    "\t\tself.LOSS.apply_wc = False\n",
    "\t\t# clipping bound for weight clippling regularization\n",
    "\t\tself.LOSS.wc_bound = \"N/A\"\n",
    "\t\t# whether to apply gradient penalty regularization\n",
    "\t\tself.LOSS.apply_gp = False\n",
    "\t\t# strength of the gradient penalty regularization\n",
    "\t\tself.LOSS.gp_lambda = \"N/A\"\n",
    "\t\t# whether to apply deep regret analysis regularization\n",
    "\t\tself.LOSS.apply_dra = False\n",
    "\t\t# strength of the deep regret analysis regularization\n",
    "\t\tself.LOSS.dra_lambda = \"N/A\"\n",
    "\t\t# whther to apply max gradient penalty to let the discriminator satisfy Lipschitzness\n",
    "\t\tself.LOSS.apply_maxgp = False\n",
    "\t\t# strength of the maxgp regularization\n",
    "\t\tself.LOSS.maxgp_lambda = \"N/A\"\n",
    "\t\t# whether to apply consistency regularization\n",
    "\t\tself.LOSS.apply_cr = False\n",
    "\t\t# strength of the consistency regularization\n",
    "\t\tself.LOSS.cr_lambda = \"N/A\"\n",
    "\t\t# whether to apply balanced consistency regularization\n",
    "\t\tself.LOSS.apply_bcr = False\n",
    "\t\t# attraction strength between logits of real and augmented real samples\n",
    "\t\tself.LOSS.real_lambda = \"N/A\"\n",
    "\t\t# attraction strength between logits of fake and augmented fake samples\n",
    "\t\tself.LOSS.fake_lambda = \"N/A\"\n",
    "\t\t# whether to apply latent consistency regularization\n",
    "\t\tself.LOSS.apply_zcr = False\n",
    "\t\t# radius of ball to generate an fake image G(z + radius)\n",
    "\t\tself.LOSS.radius = \"N/A\"\n",
    "\t\t# repulsion strength between fake images (G(z), G(z + radius))\n",
    "\t\tself.LOSS.g_lambda = \"N/A\"\n",
    "\t\t# attaction strength between logits of fake images (G(z), G(z + radius))\n",
    "\t\tself.LOSS.d_lambda = \"N/A\"\n",
    "\t\t# whether to apply latent optimization for stable training\n",
    "\t\tself.LOSS.apply_lo = False\n",
    "\t\t# latent step size for latent optimization\n",
    "\t\tself.LOSS.lo_alpha = \"N/A\"\n",
    "\t\t# damping factor for calculating Fisher Information matrix\n",
    "\t\tself.LOSS.lo_beta = \"N/A\"\n",
    "\t\t# portion of z for latent optimization (c)\n",
    "\t\tself.LOSS.lo_rate = \"N/A\"\n",
    "\t\t# strength of latent optimization (w_{r})\n",
    "\t\tself.LOSS.lo_lambda = \"N/A\"\n",
    "\t\t# number of latent optimization iterations for a single sample during training\n",
    "\t\tself.LOSS.lo_steps4train = \"N/A\"\n",
    "\t\t# number of latent optimization iterations for a single sample during evaluation\n",
    "\t\tself.LOSS.lo_steps4eval = \"N/A\"\n",
    "\t\t# whether to apply topk training for the generator update\n",
    "\t\tself.LOSS.apply_topk = False\n",
    "\t\t# hyperparameter for batch_size decay rate for topk training \\in [0,1]\n",
    "\t\tself.LOSS.topk_gamma = \"N/A\"\n",
    "\t\t# hyperparameter for the inf of the number of topk samples \\in [0,1],\n",
    "\t\t# inf_batch_size = int(topk_nu*batch_size)\n",
    "\t\tself.LOSS.topk_nu = \"N/A\"\n",
    "\t\t# strength lambda for infoGAN loss in case of discrete c (typically 0.1)\n",
    "\t\tself.LOSS.infoGAN_loss_discrete_lambda = \"N/A\"\n",
    "\t\t# strength lambda for infoGAN loss in case of continuous c (typically 1)\n",
    "\t\tself.LOSS.infoGAN_loss_conti_lambda = \"N/A\"\n",
    "\t\t# whether to apply LeCam regularization or not\n",
    "\t\tself.LOSS.apply_lecam = False\n",
    "\t\t# strength of the LeCam regularization\n",
    "\t\tself.LOSS.lecam_lambda = \"N/A\"\n",
    "\t\t# start iteration for EMALosses in src/utils/EMALosses\n",
    "\t\tself.LOSS.lecam_ema_start_iter = \"N/A\"\n",
    "\t\t# decay rate for the EMALosses\n",
    "\t\tself.LOSS.lecam_ema_decay = \"N/A\"\n",
    "\n",
    "\t\t# -----------------------------------------------------------------------------\n",
    "\t\t# optimizer settings\n",
    "\t\t# -----------------------------------------------------------------------------\n",
    "\t\tself.OPTIMIZATION = empty_object()\n",
    "\n",
    "\t\t# type of the optimizer for GAN training \\in [\"SGD\", RMSprop, \"Adam\"]\n",
    "\t\tself.OPTIMIZATION.type_ = \"Adam\"\n",
    "\t\t# number of batch size for GAN training,\n",
    "\t\t# typically {CIFAR10: 64, CIFAR100: 64, Tiny_ImageNet: 1024, \"CUB200\": 256, ImageNet: 512(batch_size) * 4(accm_step)\"}\n",
    "\t\tself.OPTIMIZATION.batch_size = 64\n",
    "\t\t# acuumulation steps for large batch training (batch_size = batch_size*accm_step)\n",
    "\t\tself.OPTIMIZATION.acml_steps = 1\n",
    "\t\t# learning rate for generator update\n",
    "\t\tself.OPTIMIZATION.g_lr = 0.0002\n",
    "\t\t# learning rate for discriminator update\n",
    "\t\tself.OPTIMIZATION.d_lr = 0.0002\n",
    "\t\t# weight decay strength for the generator update\n",
    "\t\tself.OPTIMIZATION.g_weight_decay = 0.0\n",
    "\t\t# weight decay strength for the discriminator update\n",
    "\t\tself.OPTIMIZATION.d_weight_decay = 0.0\n",
    "\t\t# momentum value for SGD and RMSprop optimizers\n",
    "\t\tself.OPTIMIZATION.momentum = \"N/A\"\n",
    "\t\t# nesterov value for SGD optimizer\n",
    "\t\tself.OPTIMIZATION.nesterov = \"N/A\"\n",
    "\t\t# alpha value for RMSprop optimizer\n",
    "\t\tself.OPTIMIZATION.alpha = \"N/A\"\n",
    "\t\t# beta values for Adam optimizer\n",
    "\t\tself.OPTIMIZATION.beta1 = 0.5\n",
    "\t\tself.OPTIMIZATION.beta2 = 0.999\n",
    "\t\t# whether to optimize discriminator first,\n",
    "\t\t# if True: optimize D -> optimize G\n",
    "\t\tself.OPTIMIZATION.d_first = True\n",
    "\t\t# the number of generator updates per step\n",
    "\t\tself.OPTIMIZATION.g_updates_per_step = 1\n",
    "\t\t# the number of discriminator updates per step\n",
    "\t\tself.OPTIMIZATION.d_updates_per_step = 5\n",
    "\t\t# the total number of steps for GAN training\n",
    "\t\tself.OPTIMIZATION.total_steps = 100000\n",
    "\n",
    "\t\t# -----------------------------------------------------------------------------\n",
    "\t\t# preprocessing settings\n",
    "\t\t# -----------------------------------------------------------------------------\n",
    "\t\tself.PRE = empty_object()\n",
    "\n",
    "\t\t# whether to apply random flip preprocessing before training\n",
    "\t\tself.PRE.apply_rflip = True\n",
    "\n",
    "\t\t# -----------------------------------------------------------------------------\n",
    "\t\t# differentiable augmentation settings\n",
    "\t\t# -----------------------------------------------------------------------------\n",
    "\t\tself.AUG = empty_object()\n",
    "\n",
    "\t\t# whether to apply differentiable augmentations for limited data training\n",
    "\t\tself.AUG.apply_diffaug = False\n",
    "\n",
    "\t\t# whether to apply adaptive discriminator augmentation (ADA)\n",
    "\t\tself.AUG.apply_ada = False\n",
    "\t\t# initial value of augmentation probability.\n",
    "\t\tself.AUG.ada_initial_augment_p = \"N/A\"\n",
    "\t\t# target probability for adaptive differentiable augmentations, None = fixed p (keep ada_initial_augment_p)\n",
    "\t\tself.AUG.ada_target = \"N/A\"\n",
    "\t\t# ADA adjustment speed, measured in how many kimg it takes for p to increase/decrease by one unit.\n",
    "\t\tself.AUG.ada_kimg = \"N/A\"\n",
    "\t\t# how often to perform ada adjustment\n",
    "\t\tself.AUG.ada_interval = \"N/A\"\n",
    "\t\t# whether to apply adaptive pseudo augmentation (APA)\n",
    "\t\tself.AUG.apply_apa = False\n",
    "\t\t# initial value of augmentation probability.\n",
    "\t\tself.AUG.apa_initial_augment_p = \"N/A\"\n",
    "\t\t# target probability for adaptive pseudo augmentations, None = fixed p (keep ada_initial_augment_p)\n",
    "\t\tself.AUG.apa_target = \"N/A\"\n",
    "\t\t# APA adjustment speed, measured in how many kimg it takes for p to increase/decrease by one unit.\n",
    "\t\tself.AUG.apa_kimg = \"N/A\"\n",
    "\t\t# how often to perform apa adjustment\n",
    "\t\tself.AUG.apa_interval = \"N/A\"\n",
    "\t\t# type of differentiable augmentation for cr, bcr, or limited data training\n",
    "\t\t# \\in [\"W/O\", \"cr\", \"bcr\", \"diffaug\", \"simclr_basic\", \"simclr_hq\", \"simclr_hq_cutout\", \"byol\",\n",
    "\t\t# \"blit\", \"geom\", \"color\", \"filter\", \"noise\", \"cutout\", \"bg\", \"bgc\", \"bgcf\", \"bgcfn\", \"bgcfnc\"]\n",
    "\t\t# cr (bcr, diffaugment, ada, simclr, byol) indicates differentiable augmenations used in the original paper\n",
    "\t\tself.AUG.cr_aug_type = \"W/O\"\n",
    "\t\tself.AUG.bcr_aug_type = \"W/O\"\n",
    "\t\tself.AUG.diffaug_type = \"W/O\"\n",
    "\t\tself.AUG.ada_aug_type = \"W/O\"\n",
    "\n",
    "\t\tself.STYLEGAN = empty_object()\n",
    "\n",
    "\t\t# type of generator used in stylegan3, stylegan3-t : translatino equiv., stylegan3-r : translation & rotation equiv.\n",
    "\t\t# \\ in [\"stylegan3-t\", \"stylegan3-r\"]\n",
    "\t\tself.STYLEGAN.stylegan3_cfg = \"N/A\"\n",
    "\t\t# conditioning types that utilize embedding proxies for conditional stylegan2, stylegan3\n",
    "\t\tself.STYLEGAN.cond_type = [\"PD\", \"SPD\", \"2C\", \"D2DCE\"]\n",
    "\t\t# lazy regularization interval for generator, default 4\n",
    "\t\tself.STYLEGAN.g_reg_interval = \"N/A\"\n",
    "\t\t# lazy regularization interval for discriminator, default 16\n",
    "\t\tself.STYLEGAN.d_reg_interval = \"N/A\"\n",
    "\t\t# number of layers for the mapping network, default 8 except for cifar (2)\n",
    "\t\tself.STYLEGAN.mapping_network = \"N/A\"\n",
    "\t\t# style_mixing_p in stylegan generator, default 0.9 except for cifar (0)\n",
    "\t\tself.STYLEGAN.style_mixing_p = \"N/A\"\n",
    "\t\t# half-life of the exponential moving average (EMA) of generator weights default 500\n",
    "\t\tself.STYLEGAN.g_ema_kimg = \"N/A\"\n",
    "\t\t# EMA ramp-up coefficient, defalt \"N/A\" except for cifar 0.05\n",
    "\t\tself.STYLEGAN.g_ema_rampup = \"N/A\"\n",
    "\t\t# whether to apply path length regularization, default is True except cifar\n",
    "\t\tself.STYLEGAN.apply_pl_reg = False\n",
    "\t\t# pl regularization strength, default 2\n",
    "\t\tself.STYLEGAN.pl_weight = \"N/A\"\n",
    "\t\t# discriminator architecture for STYLEGAN. 'resnet' except for cifar10 ('orig')\n",
    "\t\tself.STYLEGAN.d_architecture = \"N/A\"\n",
    "\t\t# group size for the minibatch standard deviation layer, None = entire minibatch.\n",
    "\t\tself.STYLEGAN.d_epilogue_mbstd_group_size = \"N/A\"\n",
    "\t\t# Whether to blur the images seen by the discriminator. Only used for stylegan3-r with value 10\n",
    "\t\tself.STYLEGAN.blur_init_sigma = \"N/A\"\n",
    "\n",
    "\t\t# -----------------------------------------------------------------------------\n",
    "\t\t# run settings\n",
    "\t\t# -----------------------------------------------------------------------------\n",
    "\t\tself.RUN = empty_object()\n",
    "\t\tself.RUN.distributed_data_parallel = False\n",
    "\t\tself.RUN.mixed_precision = False\n",
    "\n",
    "\t\t# -----------------------------------------------------------------------------\n",
    "\t\t# run settings\n",
    "\t\t# -----------------------------------------------------------------------------\n",
    "\t\tself.MISC = empty_object()\n",
    "\n",
    "\t\tself.MISC.no_proc_data = [\"CIFAR10\", \"CIFAR100\", \"Tiny_ImageNet\"]\n",
    "\t\tself.MISC.base_folders = [\"checkpoints\", \"figures\", \"logs\", \"moments\", \"samples\", \"values\"]\n",
    "\t\tself.MISC.classifier_based_GAN = [\"AC\", \"2C\", \"D2DCE\"]\n",
    "\t\tself.MISC.info_params = [\"info_discrete_linear\", \"info_conti_mu_linear\", \"info_conti_var_linear\"]\n",
    "\t\tself.MISC.cas_setting = {\n",
    "\t\t\t\"CIFAR10\": {\n",
    "\t\t\t\t\"batch_size\": 128,\n",
    "\t\t\t\t\"epochs\": 90,\n",
    "\t\t\t\t\"depth\": 32,\n",
    "\t\t\t\t\"lr\": 0.1,\n",
    "\t\t\t\t\"momentum\": 0.9,\n",
    "\t\t\t\t\"weight_decay\": 1e-4,\n",
    "\t\t\t\t\"print_freq\": 1,\n",
    "\t\t\t\t\"bottleneck\": True\n",
    "\t\t\t},\n",
    "\t\t\t\"Tiny_ImageNet\": {\n",
    "\t\t\t\t\"batch_size\": 128,\n",
    "\t\t\t\t\"epochs\": 90,\n",
    "\t\t\t\t\"depth\": 34,\n",
    "\t\t\t\t\"lr\": 0.1,\n",
    "\t\t\t\t\"momentum\": 0.9,\n",
    "\t\t\t\t\"weight_decay\": 1e-4,\n",
    "\t\t\t\t\"print_freq\": 1,\n",
    "\t\t\t\t\"bottleneck\": True\n",
    "\t\t\t},\n",
    "\t\t\t\"ImageNet\": {\n",
    "\t\t\t\t\"batch_size\": 128,\n",
    "\t\t\t\t\"epochs\": 90,\n",
    "\t\t\t\t\"depth\": 34,\n",
    "\t\t\t\t\"lr\": 0.1,\n",
    "\t\t\t\t\"momentum\": 0.9,\n",
    "\t\t\t\t\"weight_decay\": 1e-4,\n",
    "\t\t\t\t\"print_freq\": 1,\n",
    "\t\t\t\t\"bottleneck\": True\n",
    "\t\t\t},\n",
    "\t\t}\n",
    "\n",
    "\t\t# -----------------------------------------------------------------------------\n",
    "\t\t# Module settings\n",
    "\t\t# -----------------------------------------------------------------------------\n",
    "\t\tself.MODULES = empty_object()\n",
    "\n",
    "\t\tself.super_cfgs = {\n",
    "\t\t\t\"DATA\": self.DATA,\n",
    "\t\t\t\"MODEL\": self.MODEL,\n",
    "\t\t\t\"LOSS\": self.LOSS,\n",
    "\t\t\t\"OPTIMIZATION\": self.OPTIMIZATION,\n",
    "\t\t\t\"PRE\": self.PRE,\n",
    "\t\t\t\"AUG\": self.AUG,\n",
    "\t\t\t\"RUN\": self.RUN,\n",
    "\t\t\t\"STYLEGAN\": self.STYLEGAN\n",
    "\t\t}\n",
    "\n",
    "\tdef update_cfgs(self, cfgs, super=\"RUN\"):\n",
    "\t\tfor attr, value in cfgs.items():\n",
    "\t\t\tsetattr(self.super_cfgs[super], attr, value)\n",
    "\n",
    "\tdef _overwrite_cfgs(self, config):\n",
    "\t\tfor mode, attribute in config.items():\n",
    "\t\t\tfor key, val in attribute.items():\n",
    "\t\t\t\tif hasattr(self.super_cfgs[mode], key):\n",
    "\t\t\t\t\tsetattr(self.super_cfgs[mode], key, val)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\traise AttributeError(\"Doesn't exist\")\n",
    "\n",
    "\tdef define_losses(self):\n",
    "\t\tif self.MODEL.d_cond_mtd == \"MH\" and self.LOSS.adv_loss == \"MH\":\n",
    "\t\t\tself.LOSS.g_loss = crammer_singer_loss\n",
    "\t\t\tself.LOSS.d_loss = crammer_singer_loss\n",
    "\t\telse:\n",
    "\t\t\tg_losses = {\n",
    "\t\t\t\t\"vanilla\": g_vanilla,\n",
    "\t\t\t\t\"logistic\": g_logistic,\n",
    "\t\t\t\t\"least_square\": g_ls,\n",
    "\t\t\t\t\"hinge\": g_hinge,\n",
    "\t\t\t\t\"wasserstein\": g_wasserstein,\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\td_losses = {\n",
    "\t\t\t\t\"vanilla\": d_vanilla,\n",
    "\t\t\t\t\"logistic\": d_logistic,\n",
    "\t\t\t\t\"least_square\": d_ls,\n",
    "\t\t\t\t\"hinge\": d_hinge,\n",
    "\t\t\t\t\"wasserstein\": d_wasserstein,\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\tself.LOSS.g_loss = g_losses[self.LOSS.adv_loss]\n",
    "\t\t\tself.LOSS.d_loss = d_losses[self.LOSS.adv_loss]\n",
    "\n",
    "\tdef define_modules(self):\n",
    "\t\tif self.MODEL.apply_g_sn:\n",
    "\t\t\tself.MODULES.g_conv2d = snconv2d\n",
    "\t\t\tself.MODULES.g_deconv2d = sndeconv2d\n",
    "\t\t\tself.MODULES.g_linear = snlinear\n",
    "\t\t\tself.MODULES.g_embedding = sn_embedding\n",
    "\t\telse:\n",
    "\t\t\tself.MODULES.g_conv2d = conv2d\n",
    "\t\t\tself.MODULES.g_deconv2d = deconv2d\n",
    "\t\t\tself.MODULES.g_linear = linear\n",
    "\t\t\tself.MODULES.g_embedding = embedding\n",
    "\n",
    "\t\tif self.MODEL.apply_d_sn:\n",
    "\t\t\tself.MODULES.d_conv2d = snconv2d\n",
    "\t\t\tself.MODULES.d_deconv2d = sndeconv2d\n",
    "\t\t\tself.MODULES.d_linear = snlinear\n",
    "\t\t\tself.MODULES.d_embedding = sn_embedding\n",
    "\t\telse:\n",
    "\t\t\tself.MODULES.d_conv2d = conv2d\n",
    "\t\t\tself.MODULES.d_deconv2d = deconv2d\n",
    "\t\t\tself.MODULES.d_linear = linear\n",
    "\t\t\tself.MODULES.d_embedding = embedding\n",
    "\n",
    "\t\tif self.MODEL.g_cond_mtd == \"cBN\" or self.MODEL.g_info_injection == \"cBN\" or self.MODEL.backbone == \"big_resnet\":\n",
    "\t\t\tself.MODULES.g_bn = ConditionalBatchNorm2d\n",
    "\t\telif self.MODEL.g_cond_mtd == \"W/O\":\n",
    "\t\t\tself.MODULES.g_bn = batchnorm_2d\n",
    "\t\telif self.MODEL.g_cond_mtd == \"cAdaIN\":\n",
    "\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\traise NotImplementedError\n",
    "\n",
    "\t\tif not self.MODEL.apply_d_sn:\n",
    "\t\t\tself.MODULES.d_bn = batchnorm_2d\n",
    "\n",
    "\t\tif self.MODEL.g_act_fn == \"ReLU\":\n",
    "\t\t\tself.MODULES.g_act_fn = nn.ReLU(inplace=True)\n",
    "\t\telif self.MODEL.g_act_fn == \"Leaky_ReLU\":\n",
    "\t\t\tself.MODULES.g_act_fn = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "\t\telif self.MODEL.g_act_fn == \"ELU\":\n",
    "\t\t\tself.MODULES.g_act_fn = nn.ELU(alpha=1.0, inplace=True)\n",
    "\t\telif self.MODEL.g_act_fn == \"GELU\":\n",
    "\t\t\tself.MODULES.g_act_fn = nn.GELU()\n",
    "\t\telif self.MODEL.g_act_fn == \"Auto\":\n",
    "\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\traise NotImplementedError\n",
    "\n",
    "\t\tif self.MODEL.d_act_fn == \"ReLU\":\n",
    "\t\t\tself.MODULES.d_act_fn = nn.ReLU(inplace=True)\n",
    "\t\telif self.MODEL.d_act_fn == \"Leaky_ReLU\":\n",
    "\t\t\tself.MODULES.d_act_fn = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "\t\telif self.MODEL.d_act_fn == \"ELU\":\n",
    "\t\t\tself.MODULES.d_act_fn = nn.ELU(alpha=1.0, inplace=True)\n",
    "\t\telif self.MODEL.d_act_fn == \"GELU\":\n",
    "\t\t\tself.MODULES.d_act_fn = nn.GELU()\n",
    "\t\telif self.MODEL.g_act_fn == \"Auto\":\n",
    "\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\traise NotImplementedError\n",
    "\t\treturn self.MODULES\n",
    "\n",
    "\tdef define_optimizer(self, Gen, Dis):\n",
    "\t\tGen_params, Dis_params = [], []\n",
    "\t\tfor g_name, g_param in Gen.named_parameters():\n",
    "\t\t\tGen_params.append(g_param)\n",
    "\t\tif self.MODEL.info_type in [\"discrete\", \"both\"]:\n",
    "\t\t\tfor info_name, info_param in Dis.info_discrete_linear.named_parameters():\n",
    "\t\t\t\tGen_params.append(info_param)\n",
    "\t\tif self.MODEL.info_type in [\"continuous\", \"both\"]:\n",
    "\t\t\tfor info_name, info_param in Dis.info_conti_mu_linear.named_parameters():\n",
    "\t\t\t\tGen_params.append(info_param)\n",
    "\t\t\tfor info_name, info_param in Dis.info_conti_var_linear.named_parameters():\n",
    "\t\t\t\tGen_params.append(info_param)\n",
    "\n",
    "\t\tfor d_name, d_param in Dis.named_parameters():\n",
    "\t\t\tif self.MODEL.info_type in [\"discrete\", \"continuous\", \"both\"]:\n",
    "\t\t\t\tif \"info_discrete\" in d_name or \"info_conti\" in d_name:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tDis_params.append(d_param)\n",
    "\t\t\telse:\n",
    "\t\t\t\tDis_params.append(d_param)\n",
    "\n",
    "\t\tif self.OPTIMIZATION.type_ == \"SGD\":\n",
    "\t\t\tself.OPTIMIZATION.g_optimizer = torch.optim.SGD(params=Gen_params,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlr=self.OPTIMIZATION.g_lr,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tweight_decay=self.OPTIMIZATION.g_weight_decay,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmomentum=self.OPTIMIZATION.momentum,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnesterov=self.OPTIMIZATION.nesterov)\n",
    "\t\t\tself.OPTIMIZATION.d_optimizer = torch.optim.SGD(params=Dis_params,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlr=self.OPTIMIZATION.d_lr,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tweight_decay=self.OPTIMIZATION.d_weight_decay,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmomentum=self.OPTIMIZATION.momentum,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnesterov=self.OPTIMIZATION.nesterov)\n",
    "\t\telif self.OPTIMIZATION.type_ == \"RMSprop\":\n",
    "\t\t\tself.OPTIMIZATION.g_optimizer = torch.optim.RMSprop(params=Gen_params,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlr=self.OPTIMIZATION.g_lr,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tweight_decay=self.OPTIMIZATION.g_weight_decay,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmomentum=self.OPTIMIZATION.momentum,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\talpha=self.OPTIMIZATION.alpha)\n",
    "\t\t\tself.OPTIMIZATION.d_optimizer = torch.optim.RMSprop(params=Dis_params,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlr=self.OPTIMIZATION.d_lr,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tweight_decay=self.OPTIMIZATION.d_weight_decay,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmomentum=self.OPTIMIZATION.momentum,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\talpha=self.OPTIMIZATION.alpha)\n",
    "\t\telif self.OPTIMIZATION.type_ == \"Adam\":\n",
    "\t\t\tif self.MODEL.backbone in [\"stylegan2\", \"stylegan3\"]:\n",
    "\t\t\t\tg_ratio = (self.STYLEGAN.g_reg_interval / (self.STYLEGAN.g_reg_interval + 1)) if self.STYLEGAN.g_reg_interval != 1 else 1\n",
    "\t\t\t\td_ratio = (self.STYLEGAN.d_reg_interval / (self.STYLEGAN.d_reg_interval + 1)) if self.STYLEGAN.d_reg_interval != 1 else 1\n",
    "\t\t\t\tself.OPTIMIZATION.g_lr *= g_ratio\n",
    "\t\t\t\tself.OPTIMIZATION.d_lr *= d_ratio\n",
    "\t\t\t\tbetas_g = [self.OPTIMIZATION.beta1**g_ratio, self.OPTIMIZATION.beta2**g_ratio]\n",
    "\t\t\t\tbetas_d = [self.OPTIMIZATION.beta1**d_ratio, self.OPTIMIZATION.beta2**d_ratio]\n",
    "\t\t\t\teps_ = 1e-8\n",
    "\t\t\telse:\n",
    "\t\t\t\tbetas_g = betas_d = [self.OPTIMIZATION.beta1, self.OPTIMIZATION.beta2]\n",
    "\t\t\t\teps_ = 1e-6\n",
    "\n",
    "\t\t\tself.OPTIMIZATION.g_optimizer = torch.optim.Adam(params=Gen_params,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t lr=self.OPTIMIZATION.g_lr,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t betas=betas_g,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t weight_decay=self.OPTIMIZATION.g_weight_decay,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t eps=eps_)\n",
    "\t\t\tself.OPTIMIZATION.d_optimizer = torch.optim.Adam(params=Dis_params,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t lr=self.OPTIMIZATION.d_lr,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t betas=betas_d,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t weight_decay=self.OPTIMIZATION.d_weight_decay,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t eps=eps_)\n",
    "\t\telse:\n",
    "\t\t\traise NotImplementedError\n",
    "\n",
    "\tdef define_augments(self, device):\n",
    "\t\tself.AUG.series_augment = identity\n",
    "\t\tada_augpipe = {\n",
    "\t\t\t'blit':   dict(xflip=1, rotate90=1, xint=1),\n",
    "\t\t\t'geom':   dict(scale=1, rotate=1, aniso=1, xfrac=1),\n",
    "\t\t\t'color':  dict(brightness=1, contrast=1, lumaflip=1, hue=1, saturation=1),\n",
    "\t\t\t'filter': dict(imgfilter=1),\n",
    "\t\t\t'noise':  dict(noise=1),\n",
    "\t\t\t'cutout': dict(cutout=1),\n",
    "\t\t\t'bg':     dict(xflip=1, rotate90=1, xint=1, scale=1, rotate=1, aniso=1, xfrac=1),\n",
    "\t\t\t'bgc':    dict(xflip=1, rotate90=1, xint=1, scale=1, rotate=1, aniso=1, xfrac=1, brightness=1, contrast=1, lumaflip=1, hue=1, saturation=1),\n",
    "\t\t\t'bgcf':   dict(xflip=1, rotate90=1, xint=1, scale=1, rotate=1, aniso=1, xfrac=1, brightness=1, contrast=1, lumaflip=1, hue=1, saturation=1, imgfilter=1),\n",
    "\t\t\t'bgcfn':  dict(xflip=1, rotate90=1, xint=1, scale=1, rotate=1, aniso=1, xfrac=1, brightness=1, contrast=1, lumaflip=1, hue=1, saturation=1, imgfilter=1, noise=1),\n",
    "\t\t\t'bgcfnc': dict(xflip=1, rotate90=1, xint=1, scale=1, rotate=1, aniso=1, xfrac=1, brightness=1, contrast=1, lumaflip=1, hue=1, saturation=1, imgfilter=1, noise=1, cutout=1),\n",
    "\t\t}\n",
    "\t\tif self.AUG.apply_diffaug:\n",
    "\t\t\tassert self.AUG.diffaug_type != \"W/O\", \"Please select diffentiable augmentation type!\"\n",
    "\t\t\tif self.AUG.diffaug_type == \"diffaug\":\n",
    "\t\t\t\tself.AUG.series_augment = apply_diffaug\n",
    "\t\t\telse:\n",
    "\t\t\t\traise NotImplementedError\n",
    "\n",
    "\tdef check_compatability(self):\n",
    "\t\tif self.RUN.distributed_data_parallel and self.RUN.mixed_precision:\n",
    "\t\t\tprint(\"-\"*120)\n",
    "\t\t\tprint(\"Please use standing statistics (-std_stat) with -std_max and -std_step options for reliable evaluation!\")\n",
    "\t\t\tprint(\"-\"*120)\n",
    "\n",
    "\t\tif len(self.RUN.eval_metrics):\n",
    "\t\t\tfor item in self.RUN.eval_metrics:\n",
    "\t\t\t\tassert item in [\"is\", \"fid\", \"prdc\", \"none\"], \"-metrics option can only contain is, fid, prdc or none for skipping evaluation.\"\n",
    "\n",
    "\t\tif self.RUN.load_data_in_memory:\n",
    "\t\t\tassert self.RUN.load_train_hdf5, \"load_data_in_memory option is appliable with the load_train_hdf5 (-hdf5) option.\"\n",
    "\n",
    "\t\tif self.MODEL.backbone == \"deep_conv\":\n",
    "\t\t\tassert self.DATA.img_size == 32, \"StudioGAN does not support the deep_conv backbone for the dataset whose spatial resolution is not 32.\"\n",
    "\n",
    "\t\tif self.MODEL.backbone in [\"big_resnet_deep_legacy\", \"big_resnet_deep_studiogan\"]:\n",
    "\t\t\tmsg = \"StudioGAN does not support the big_resnet_deep backbones without applying spectral normalization to the generator and discriminator.\"\n",
    "\t\t\tassert self.MODEL.g_cond_mtd and self.MODEL.d_cond_mtd, msg\n",
    "\n",
    "\t\tif self.RUN.langevin_sampling or self.LOSS.apply_lo:\n",
    "\t\t\tassert self.RUN.langevin_sampling * self.LOSS.apply_lo == 0, \"Langevin sampling and latent optmization cannot be used simultaneously.\"\n",
    "\n",
    "\t\tif isinstance(self.MODEL.g_depth, int) or isinstance(self.MODEL.d_depth, int):\n",
    "\t\t\tassert self.MODEL.backbone in [\"big_resnet_deep_legacy\", \"big_resnet_deep_studiogan\"], \\\n",
    "\t\t\t\t\"MODEL.g_depth and MODEL.d_depth are hyperparameters for big_resnet_deep backbones.\"\n",
    "\n",
    "\t\tif self.RUN.langevin_sampling:\n",
    "\t\t\tmsg = \"Langevin sampling cannot be used for training only.\"\n",
    "\t\t\tassert self.RUN.vis_fake_images + \\\n",
    "\t\t\t\tself.RUN.k_nearest_neighbor + \\\n",
    "\t\t\t\tself.RUN.interpolation + \\\n",
    "\t\t\t\tself.RUN.frequency_analysis + \\\n",
    "\t\t\t\tself.RUN.tsne_analysis + \\\n",
    "\t\t\t\tself.RUN.intra_class_fid + \\\n",
    "\t\t\t\tself.RUN.semantic_factorization + \\\n",
    "\t\t\t\tself.RUN.GAN_train + \\\n",
    "\t\t\t\tself.RUN.GAN_test != 0, \\\n",
    "\t\t\tmsg\n",
    "\n",
    "\t\tif self.RUN.langevin_sampling:\n",
    "\t\t\tassert self.MODEL.z_prior == \"gaussian\", \"Langevin sampling is defined only if z_prior is gaussian.\"\n",
    "\n",
    "\t\tif self.RUN.freezeD > -1:\n",
    "\t\t\tmsg = \"Freezing discriminator needs a pre-trained model. Please specify the checkpoint directory (using -ckpt) for loading a pre-trained discriminator.\"\n",
    "\t\t\tassert self.RUN.ckpt_dir is not None, msg\n",
    "\n",
    "\t\tif not self.RUN.train and self.RUN.eval_metrics != \"none\":\n",
    "\t\t\tassert self.RUN.ckpt_dir is not None, \"Specify -ckpt CHECKPOINT_FOLDER to evaluate GAN without training.\"\n",
    "\n",
    "\t\tif self.RUN.GAN_train + self.RUN.GAN_test > 1:\n",
    "\t\t\tmsg = \"Please turn off -DDP option to calculate CAS. It is possible to train a GAN using the DDP option and then compute CAS using DP.\"\n",
    "\t\t\tassert not self.RUN.distributed_data_parallel, msg\n",
    "\n",
    "\t\tif self.RUN.distributed_data_parallel:\n",
    "\t\t\tmsg = \"StudioGAN does not support image visualization, k_nearest_neighbor, interpolation, frequency, tsne analysis, DDLS, SeFa, and CAS with DDP. \" + \\\n",
    "\t\t\t\t\"Please change DDP with a single GPU training or DataParallel instead.\"\n",
    "\t\t\tassert self.RUN.vis_fake_images + \\\n",
    "\t\t\t\tself.RUN.k_nearest_neighbor + \\\n",
    "\t\t\t\tself.RUN.interpolation + \\\n",
    "\t\t\t\tself.RUN.frequency_analysis + \\\n",
    "\t\t\t\tself.RUN.tsne_analysis + \\\n",
    "\t\t\t\tself.RUN.semantic_factorization + \\\n",
    "\t\t\t\tself.RUN.langevin_sampling + \\\n",
    "\t\t\t\tself.RUN.GAN_train + \\\n",
    "\t\t\t\tself.RUN.GAN_test == 0, \\\n",
    "\t\t\tmsg\n",
    "\n",
    "\t\tif self.RUN.intra_class_fid:\n",
    "\t\t\tassert self.RUN.load_data_in_memory*self.RUN.load_train_hdf5 or not self.RUN.load_train_hdf5, \\\n",
    "\t\t\t\"StudioGAN does not support calculating iFID using hdf5 data format without load_data_in_memory option.\"\n",
    "\n",
    "\t\tif self.RUN.vis_fake_images + self.RUN.k_nearest_neighbor + self.RUN.interpolation + self.RUN.intra_class_fid + \\\n",
    "\t\t\t\tself.RUN.GAN_train + self.RUN.GAN_test >= 1:\n",
    "\t\t\tassert self.OPTIMIZATION.batch_size % 8 == 0, \"batch_size should be divided by 8.\"\n",
    "\n",
    "\t\tif self.MODEL.aux_cls_type != \"W/O\":\n",
    "\t\t\tassert self.MODEL.d_cond_mtd in self.MISC.classifier_based_GAN, \\\n",
    "\t\t\t\"TAC and ADC are only applicable to classifier-based GANs.\"\n",
    "\n",
    "\t\tif self.MODEL.d_cond_mtd == \"MH\" or self.LOSS.adv_loss == \"MH\":\n",
    "\t\t\tassert self.MODEL.d_cond_mtd == \"MH\" and self.LOSS.adv_loss == \"MH\", \\\n",
    "\t\t\t\"To train a GAN with Multi-Hinge loss, both d_cond_mtd and adv_loss must be 'MH'.\"\n",
    "\n",
    "\t\tif self.MODEL.d_cond_mtd == \"MH\" or self.LOSS.adv_loss == \"MH\":\n",
    "\t\t\tassert not self.LOSS.apply_topk, \"StudioGAN does not support Topk training for MHGAN.\"\n",
    "\n",
    "\t\tif self.RUN.train * self.RUN.standing_statistics:\n",
    "\t\t\tprint(\"StudioGAN does not support standing_statistics during training\")\n",
    "\t\t\tprint(\"After training is done, StudioGAN will accumulate batchnorm statistics to evaluate GAN.\")\n",
    "\n",
    "\t\tif self.OPTIMIZATION.world_size > 1 and self.RUN.synchronized_bn:\n",
    "\t\t\tassert not self.RUN.batch_statistics, \"batch_statistics cannot be used with synchronized_bn.\"\n",
    "\n",
    "\t\tif self.DATA.name in [\"CIFAR10\", \"CIFAR100\"]:\n",
    "\t\t\tassert self.RUN.ref_dataset in [\"train\", \"test\"], \"There is no data for validation.\"\n",
    "\n",
    "\t\tif self.RUN.interpolation:\n",
    "\t\t\tassert self.MODEL.backbone in [\"big_resnet\", \"big_resnet_deep_legacy\", \"big_resnet_deep_studiogan\"], \\\n",
    "\t\t\t\t\"StudioGAN does not support interpolation analysis except for biggan and big_resnet_deep backbones.\"\n",
    "\n",
    "\t\tif self.RUN.semantic_factorization:\n",
    "\t\t\tassert self.RUN.num_semantic_axis > 0, \"To apply sefa, please set num_semantic_axis to a natual number greater than 0.\"\n",
    "\n",
    "\t\tif self.OPTIMIZATION.world_size == 1:\n",
    "\t\t\tassert not self.RUN.distributed_data_parallel, \"Cannot perform distributed training with a single gpu.\"\n",
    "\n",
    "\t\tif self.MODEL.backbone == \"stylegan3\":\n",
    "\t\t\tassert self.STYLEGAN.stylegan3_cfg in [\"stylegan3-t\", \"stylegan3-r\"], \"You must choose which type of stylegan3 generator (-r or -t)\"\n",
    "\n",
    "\t\tif self.MODEL.g_cond_mtd == \"cAdaIN\":\n",
    "\t\t\tassert self.MODEL.backbone in [\"stylegan2\", \"stylegan3\"], \"cAdaIN is only applicable to stylegan2, stylegan3.\"\n",
    "\n",
    "\t\tif self.MODEL.d_cond_mtd == \"SPD\":\n",
    "\t\t\tassert self.MODEL.backbone in [\"stylegan2\", \"stylegan3\"], \"SytleGAN Projection Discriminator (SPD) is only applicable to stylegan2, stylegan3.\"\n",
    "\n",
    "\t\tif self.MODEL.backbone in [\"stylegan2\", \"stylegan3\"]:\n",
    "\t\t\tassert self.MODEL.g_act_fn == \"Auto\" and self.MODEL.d_act_fn == \"Auto\", \\\n",
    "\t\t\t\t\"g_act_fn and d_act_fn should be 'Auto' to build StyleGAN2, StyleGAN3 generator and discriminator.\"\n",
    "\n",
    "\t\tif self.MODEL.backbone in [\"stylegan2\", \"stylegan3\"]:\n",
    "\t\t\tassert not self.MODEL.apply_g_sn and not self.MODEL.apply_d_sn, \\\n",
    "\t\t\t\t\"StudioGAN does not support spectral normalization on stylegan2, stylegan3.\"\n",
    "\n",
    "\t\tif self.MODEL.backbone in [\"stylegan2\", \"stylegan3\"]:\n",
    "\t\t\tassert self.MODEL.g_cond_mtd in [\"W/O\", \"cAdaIN\"], \\\n",
    "\t\t\t\t\"stylegan2 and stylegan3 only supports 'W/O' or 'cAdaIN' as g_cond_mtd.\"\n",
    "\n",
    "\t\tif self.LOSS.apply_r1_reg and self.MODEL.backbone in [\"stylegan2\", \"stylegan3\"]:\n",
    "\t\t\tassert self.LOSS.r1_place in [\"inside_loop\", \"outside_loop\"], \"LOSS.r1_place should be one of ['inside_loop', 'outside_loop']\"\n",
    "\n",
    "\t\tif self.MODEL.g_act_fn == \"Auto\" or self.MODEL.d_act_fn == \"Auto\":\n",
    "\t\t\tassert self.MODEL.backbone in [\"stylegan2\", \"stylegan3\"], \\\n",
    "\t\t\t\t\"StudioGAN does not support the act_fn auto selection options except for stylegan2, stylegan3.\"\n",
    "\n",
    "\t\tif self.MODEL.backbone == \"stylegan3\" and self.STYLEGAN.stylegan3_cfg == \"stylegan3-r\":\n",
    "\t\t\tassert self.STYLEGAN.blur_init_sigma != \"N/A\", \"With stylegan3-r, you need to specify blur_init_sigma.\"\n",
    "\n",
    "\t\tif self.MODEL.backbone in [\"stylegan2\", \"stylegan3\"] and self.MODEL.apply_g_ema:\n",
    "\t\t\tassert self.MODEL.g_ema_decay == \"N/A\" and self.MODEL.g_ema_start == \"N/A\", \\\n",
    "\t\t\t\t\"Please specify g_ema parameters to STYLEGAN.g_ema_kimg and STYLEGAN.g_ema_rampup instead of MODEL.g_ema_decay and MODEL.g_ema_start.\"\n",
    "\n",
    "\t\tif self.MODEL.backbone in [\"stylegan2\", \"stylegan3\"]:\n",
    "\t\t\tassert self.STYLEGAN.d_epilogue_mbstd_group_size <= (self.OPTIMIZATION.batch_size / self.OPTIMIZATION.world_size),\\\n",
    "\t\t\t\t\"Number of imgs that goes to each GPU must be bigger than d_epilogue_mbstd_group_size\"\n",
    "\n",
    "\t\tif self.MODEL.backbone not in [\"stylegan2\", \"stylegan3\"] and self.MODEL.apply_g_ema:\n",
    "\t\t\tassert isinstance(self.MODEL.g_ema_decay, float) and isinstance(self.MODEL.g_ema_start, int), \\\n",
    "\t\t\t\t\"Please specify g_ema parameters to MODEL.g_ema_decay and MODEL.g_ema_start.\"\n",
    "\t\t\tassert self.STYLEGAN.g_ema_kimg == \"N/A\" and self.STYLEGAN.g_ema_rampup == \"N/A\", \\\n",
    "\t\t\t\t\"g_ema_kimg, g_ema_rampup hyperparameters are only valid for stylegan2 backbone.\"\n",
    "\n",
    "\t\tif isinstance(self.MODEL.g_shared_dim, int):\n",
    "\t\t\tassert self.MODEL.backbone in [\"big_resnet\", \"big_resnet_deep_legacy\", \"big_resnet_deep_studiogan\"], \\\n",
    "\t\t\t\"hierarchical embedding is only applicable to big_resnet or big_resnet_deep backbones.\"\n",
    "\n",
    "\t\tif isinstance(self.MODEL.g_conv_dim, int) or isinstance(self.MODEL.d_conv_dim, int):\n",
    "\t\t\tassert self.MODEL.backbone in [\"resnet\", \"big_resnet\", \"big_resnet_deep_legacy\", \"big_resnet_deep_studiogan\"], \\\n",
    "\t\t\t\"g_conv_dim and d_conv_dim are hyperparameters for controlling dimensions of resnet, big_resnet, and big_resnet_deeps.\"\n",
    "\n",
    "\t\tif self.MODEL.backbone in [\"stylegan2\", \"stylegan3\"]:\n",
    "\t\t\tassert self.LOSS.apply_fm + \\\n",
    "\t\t\t\tself.LOSS.apply_gp + \\\n",
    "\t\t\t\tself.LOSS.apply_dra + \\\n",
    "\t\t\t\tself.LOSS.apply_maxgp + \\\n",
    "\t\t\t\tself.LOSS.apply_zcr + \\\n",
    "\t\t\t\tself.LOSS.apply_lo + \\\n",
    "\t\t\t\tself.RUN.synchronized_bn + \\\n",
    "\t\t\t\tself.RUN.batch_statistics + \\\n",
    "\t\t\t\tself.RUN.standing_statistics + \\\n",
    "\t\t\t\tself.RUN.freezeD + \\\n",
    "\t\t\t\tself.RUN.langevin_sampling + \\\n",
    "\t\t\t\tself.RUN.interpolation + \\\n",
    "\t\t\t\tself.RUN.semantic_factorization == -1, \\\n",
    "\t\t\t\t\"StudioGAN does not support some options for stylegan2, stylegan3. Please refer to config.py for more details.\"\n",
    "\n",
    "\t\tif self.MODEL.backbone in [\"stylegan2\", \"stylegan3\"]:\n",
    "\t\t\tassert not self.MODEL.apply_attn, \"cannot apply attention layers to the stylegan2 generator.\"\n",
    "\n",
    "\t\tif self.RUN.GAN_train or self.RUN.GAN_test:\n",
    "\t\t\tassert not self.MODEL.d_cond_mtd == \"W/O\", \\\n",
    "\t\t\t\t\"Classifier Accuracy Score (CAS) is defined only when the GAN is trained by a class-conditioned way.\"\n",
    "\n",
    "\t\tif self.MODEL.info_type == \"N/A\":\n",
    "\t\t\tassert self.MODEL.info_num_discrete_c == \"N/A\" and self.MODEL.info_num_conti_c == \"N/A\" and self.MODEL.info_dim_discrete_c == \"N/A\" and\\\n",
    "\t\t\t\tself.MODEL.g_info_injection == \"N/A\" and self.LOSS.infoGAN_loss_discrete_lambda == \"N/A\" and self.LOSS.infoGAN_loss_conti_lambda == \"N/A\",\\\n",
    "\t\t\t\"MODEL.info_num_discrete_c, MODEL.info_num_conti_c, MODEL.info_dim_discrete_c, LOSS.infoGAN_loss_discrete_lambda, and LOSS.infoGAN_loss_conti_lambda should be 'N/A'.\"\n",
    "\t\telif self.MODEL.info_type == \"continuous\":\n",
    "\t\t\tassert self.MODEL.info_num_conti_c != \"N/A\" and self.LOSS.infoGAN_loss_conti_lambda != \"N/A\",\\\n",
    "\t\t\t\t\"MODEL.info_num_conti_c and LOSS.infoGAN_loss_conti_lambda should be integer and float.\"\n",
    "\t\telif self.MODEL.info_type == \"discrete\":\n",
    "\t\t\tassert self.MODEL.info_num_discrete_c != \"N/A\" and self.MODEL.info_dim_discrete_c != \"N/A\" and self.LOSS.infoGAN_loss_discrete_lambda != \"N/A\",\\\n",
    "\t\t\t\"MODEL.info_num_discrete_c, MODEL.info_dim_discrete_c, and LOSS.infoGAN_loss_discrete_lambda should be integer, integer, and float, respectively.\"\n",
    "\t\telif self.MODEL.info_type == \"both\":\n",
    "\t\t\tassert self.MODEL.info_num_discrete_c != \"N/A\" and self.MODEL.info_num_conti_c != \"N/A\" and self.MODEL.info_dim_discrete_c != \"N/A\" and\\\n",
    "\t\t\t\tself.LOSS.infoGAN_loss_discrete_lambda != \"N/A\" and self.LOSS.infoGAN_loss_conti_lambda != \"N/A\",\\\n",
    "\t\t\t\"MODEL.info_num_discrete_c, MODEL.info_num_conti_c, MODEL.info_dim_discrete_c, LOSS.infoGAN_loss_discrete_lambda, and LOSS.infoGAN_loss_conti_lambda should not be 'N/A'.\"\n",
    "\t\telse:\n",
    "\t\t\traise NotImplementedError\n",
    "\n",
    "\t\tif self.MODEL.info_type in [\"discrete\", \"both\"]:\n",
    "\t\t\tassert self.MODEL.info_num_discrete_c > 0 and self.MODEL.info_dim_discrete_c > 0,\\\n",
    "\t\t\t\t\"MODEL.info_num_discrete_c and MODEL.info_dim_discrete_c should be over 0.\"\n",
    "\n",
    "\t\tif self.MODEL.info_type in [\"continuous\", \"both\"]:\n",
    "\t\t\tassert self.MODEL.info_num_conti_c > 0, \"MODEL.info_num_conti_c should be over 0.\"\n",
    "\n",
    "\t\tif self.MODEL.info_type in [\"discrete\", \"continuous\", \"both\"] and self.MODEL.backbone in [\"stylegan2\", \"stylegan3\"]:\n",
    "\t\t\tassert self.MODEL.g_info_injection == \"concat\", \"StyleGAN2, StyleGAN3 only allows concat as g_info_injection method\"\n",
    "\n",
    "\t\tif self.MODEL.info_type in [\"discrete\", \"continuous\", \"both\"]:\n",
    "\t\t\tassert self.MODEL.g_info_injection in [\"concat\", \"cBN\"], \"MODEL.g_info_injection should be 'concat' or 'cBN'.\"\n",
    "\n",
    "\t\tif self.AUG.apply_ada and self.AUG.apply_apa:\n",
    "\t\t\tassert self.AUG.ada_initial_augment_p == self.AUG.apa_initial_augment_p and \\\n",
    "\t\t\t\tself.AUG.ada_target == self.AUG.apa_target and \\\n",
    "\t\t\t\tself.AUG.ada_kimg == self.AUG.apa_kimg and \\\n",
    "\t\t\t\tself.AUG.ada_interval == self.AUG.apa_interval, \\\n",
    "\t\t\t\t\"ADA and APA specifications should be the completely same.\"\n",
    "\n",
    "\t\tassert self.RUN.eval_backbone in [\"InceptionV3_tf\", \"InceptionV3_torch\", \"ResNet50_torch\", \"SwAV_torch\", \"DINO_torch\", \"Swin-T_torch\"], \\\n",
    "\t\t\t\"eval_backbone should be in [InceptionV3_tf, InceptionV3_torch, ResNet50_torch, SwAV_torch, DINO_torch, Swin-T_torch]\"\n",
    "\n",
    "\t\tassert self.RUN.post_resizer in [\"legacy\", \"clean\", \"friendly\"], \"resizing flag should be in [legacy, clean, friendly]\"\n",
    "\n",
    "\t\tassert self.RUN.data_dir is not None or self.RUN.save_fake_images, \"Please specify data_dir if dataset is prepared. \\\n",
    "\t\t\t\\nIn the case of CIFAR10 or CIFAR100, just specify the directory where you want \\\n",
    "\t\t\tdataset to be downloaded.\"\n",
    "\n",
    "\t\tassert self.RUN.batch_statistics*self.RUN.standing_statistics == 0, \\\n",
    "\t\t\t\"You can't turn on batch_statistics and standing_statistics simultaneously.\"\n",
    "\n",
    "\t\tassert self.OPTIMIZATION.batch_size % self.OPTIMIZATION.world_size == 0, \\\n",
    "\t\t\t\"Batch_size should be divided by the number of gpus.\"\n",
    "\n",
    "\t\tassert int(self.LOSS.apply_cr)*int(self.LOSS.apply_bcr) == 0 and \\\n",
    "\t\t\tint(self.LOSS.apply_cr)*int(self.LOSS.apply_zcr) == 0, \\\n",
    "\t\t\t\"You can't simultaneously turn on consistency reg. and improved consistency reg.\"\n",
    "\n",
    "\t\tassert int(self.LOSS.apply_gp)*int(self.LOSS.apply_dra)*(self.LOSS.apply_maxgp) == 0, \\\n",
    "\t\t\t\"You can't simultaneously apply gradient penalty regularization, deep regret analysis, and max gradient penalty.\"\n",
    "\n",
    "\t\tassert self.RUN.save_freq % self.RUN.print_freq == 0, \\\n",
    "\t\t\t\"RUN.save_freq should be divided by RUN.print_freq for wandb logging.\"\n",
    "\n",
    "\t\tassert self.RUN.pre_resizer in [\"wo_resize\", \"nearest\", \"bilinear\", \"bicubic\", \"lanczos\"], \\\n",
    "\t\t\t\"The interpolation filter for pre-precessing should be \\in ['wo_resize', 'nearest', 'bilinear', 'bicubic', 'lanczos']\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model **Baseline**: <code>DCGAN</code>\n",
    "Let us begin with a simple model, `DCGAN`, which <strong>utilizes the Deep Convolutional</strong> layers in both the Generator and Discriminator networks, as opposed to the original vanilla GAN that **uses dense layers**. \n",
    "\n",
    "The original DCGAN is not conditional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN:\n",
    "\tclass GenBlock(nn.Module):\n",
    "\t\tdef __init__(self, in_channels, out_channels, g_cond_mtd, g_info_injection, affine_input_dim, MODULES):\n",
    "\t\t\tsuper(DCGAN.GenBlock, self).__init__()\n",
    "\t\t\tself.g_cond_mtd = g_cond_mtd\n",
    "\t\t\tself.g_info_injection = g_info_injection\n",
    "\n",
    "\t\t\tself.deconv0 = MODULES.g_deconv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "\t\t\tif self.g_cond_mtd == \"W/O\" and self.g_info_injection in [\"N/A\", \"concat\"]:\n",
    "\t\t\t\tself.bn0 = MODULES.g_bn(in_features=out_channels)\n",
    "\t\t\telif self.g_cond_mtd == \"cBN\" or self.g_info_injection == \"cBN\":\n",
    "\t\t\t\tself.bn0 = MODULES.g_bn(affine_input_dim, out_channels, MODULES)\n",
    "\t\t\telse:\n",
    "\t\t\t\traise NotImplementedError\n",
    "\n",
    "\t\t\tself.activation = MODULES.g_act_fn\n",
    "\n",
    "\t\tdef forward(self, x, affine):\n",
    "\t\t\tx = self.deconv0(x)\n",
    "\t\t\tif self.g_cond_mtd == \"W/O\" and self.g_info_injection in [\"N/A\", \"concat\"]:\n",
    "\t\t\t\tx = self.bn0(x)\n",
    "\t\t\telif self.g_cond_mtd == \"cBN\" or self.g_info_injection == \"cBN\":\n",
    "\t\t\t\tx = self.bn0(x, affine)\n",
    "\t\t\tout = self.activation(x)\n",
    "\t\t\treturn out\n",
    "\n",
    "\n",
    "\tclass Generator(nn.Module):\n",
    "\t\tdef __init__(self, z_dim, g_shared_dim, img_size, g_conv_dim, apply_attn, attn_g_loc, g_cond_mtd, num_classes, g_init, g_depth,\n",
    "\t\t\t\t\tMODULES, MODEL):\n",
    "\t\t\tsuper(DCGAN.Generator, self).__init__()\n",
    "\t\t\tself.in_dims = [512, 256, 128]\n",
    "\t\t\tself.out_dims = [256, 128, 64]\n",
    "\n",
    "\t\t\tself.z_dim = z_dim\n",
    "\t\t\tself.num_classes = num_classes\n",
    "\t\t\tself.g_cond_mtd = g_cond_mtd\n",
    "\t\t\t# self.mixed_precision = mixed_precision\n",
    "\t\t\tself.MODEL = MODEL\n",
    "\t\t\tself.affine_input_dim = 0\n",
    "\n",
    "\t\t\tinfo_dim = 0\n",
    "\t\t\tif self.MODEL.info_type in [\"discrete\", \"both\"]:\n",
    "\t\t\t\tinfo_dim += self.MODEL.info_num_discrete_c*self.MODEL.info_dim_discrete_c\n",
    "\t\t\tif self.MODEL.info_type in [\"continuous\", \"both\"]:\n",
    "\t\t\t\tinfo_dim += self.MODEL.info_num_conti_c\n",
    "\n",
    "\t\t\tself.g_info_injection = self.MODEL.g_info_injection\n",
    "\t\t\tif self.MODEL.info_type != \"N/A\":\n",
    "\t\t\t\tif self.g_info_injection == \"concat\":\n",
    "\t\t\t\t\tself.info_mix_linear = MODULES.g_linear(in_features=self.z_dim + info_dim, out_features=self.z_dim, bias=True)\n",
    "\t\t\t\telif self.g_info_injection == \"cBN\":\n",
    "\t\t\t\t\tself.affine_input_dim += self.z_dim\n",
    "\t\t\t\t\tself.info_proj_linear = MODULES.g_linear(in_features=info_dim, out_features=self.z_dim, bias=True)\n",
    "\n",
    "\t\t\tif self.g_cond_mtd != \"W/O\" and self.g_cond_mtd == \"cBN\":\n",
    "\t\t\t\tself.affine_input_dim += self.num_classes\n",
    "\n",
    "\t\t\tself.linear0 = MODULES.g_linear(in_features=self.z_dim, out_features=self.in_dims[0]*4*4, bias=True)\n",
    "\n",
    "\t\t\tself.blocks = []\n",
    "\t\t\tfor index in range(len(self.in_dims)):\n",
    "\t\t\t\tself.blocks += [[\n",
    "\t\t\t\t\tDCGAN.GenBlock(in_channels=self.in_dims[index],\n",
    "\t\t\t\t\t\t\tout_channels=self.out_dims[index],\n",
    "\t\t\t\t\t\t\tg_cond_mtd=self.g_cond_mtd,\n",
    "\t\t\t\t\t\t\tg_info_injection=self.g_info_injection,\n",
    "\t\t\t\t\t\t\taffine_input_dim=self.affine_input_dim,\n",
    "\t\t\t\t\t\t\tMODULES=MODULES)\n",
    "\t\t\t\t]]\n",
    "\n",
    "\t\t\t\tif index + 1 in attn_g_loc and apply_attn:\n",
    "\t\t\t\t\tself.blocks += [[SelfAttention(self.out_dims[index], is_generator=True, MODULES=MODULES)]]\n",
    "\n",
    "\t\t\tself.blocks = nn.ModuleList([nn.ModuleList(block) for block in self.blocks])\n",
    "\n",
    "\t\t\tself.conv4 = MODULES.g_conv2d(in_channels=self.out_dims[-1], out_channels=3, kernel_size=3, stride=1, padding=1)\n",
    "\t\t\tself.tanh = nn.Tanh()\n",
    "\n",
    "\t\t\tinit_weights(self.modules, g_init)\n",
    "\n",
    "\t\tdef forward(self, z, label, shared_label=None, eval=False):\n",
    "\t\t\taffine_list = []\n",
    "\t\t\tif self.g_cond_mtd != \"W/O\":\n",
    "\t\t\t\tlabel = F.one_hot(label, num_classes=self.num_classes).to(torch.float32)\n",
    "\t\t\twith torch.cuda.amp.autocast() as mp:\n",
    "\t\t\t\tif self.MODEL.info_type != \"N/A\":\n",
    "\t\t\t\t\tif self.g_info_injection == \"concat\":\n",
    "\t\t\t\t\t\tz = self.info_mix_linear(z)\n",
    "\t\t\t\t\telif self.g_info_injection == \"cBN\":\n",
    "\t\t\t\t\t\tz, z_info = z[:, :self.z_dim], z[:, self.z_dim:]\n",
    "\t\t\t\t\t\taffine_list.append(self.info_proj_linear(z_info))\n",
    "\n",
    "\t\t\t\tif self.g_cond_mtd != \"W/O\":\n",
    "\t\t\t\t\taffine_list.append(label)\n",
    "\t\t\t\tif len(affine_list) > 0:\n",
    "\t\t\t\t\taffines = torch.cat(affine_list, 1)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\taffines = None\n",
    "\n",
    "\t\t\t\tact = self.linear0(z)\n",
    "\t\t\t\tact = act.view(-1, self.in_dims[0], 4, 4)\n",
    "\t\t\t\tfor index, blocklist in enumerate(self.blocks):\n",
    "\t\t\t\t\tfor block in blocklist:\n",
    "\t\t\t\t\t\tif isinstance(block, SelfAttention):\n",
    "\t\t\t\t\t\t\tact = block(act)\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tact = block(act, affines)\n",
    "\n",
    "\t\t\t\tact = self.conv4(act)\n",
    "\t\t\t\tout = self.tanh(act)\n",
    "\t\t\treturn out\n",
    "\n",
    "\tclass DiscBlock(nn.Module):\n",
    "\t\tdef __init__(self, in_channels, out_channels, apply_d_sn, MODULES):\n",
    "\t\t\tsuper(DCGAN.DiscBlock, self).__init__()\n",
    "\t\t\tself.apply_d_sn = apply_d_sn\n",
    "\n",
    "\t\t\tself.conv0 = MODULES.d_conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\t\t\tself.conv1 = MODULES.d_conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "\t\t\tif not apply_d_sn:\n",
    "\t\t\t\tself.bn0 = MODULES.d_bn(in_features=out_channels)\n",
    "\t\t\t\tself.bn1 = MODULES.d_bn(in_features=out_channels)\n",
    "\n",
    "\t\t\tself.activation = MODULES.d_act_fn\n",
    "\n",
    "\t\tdef forward(self, x):\n",
    "\t\t\tx = self.conv0(x)\n",
    "\t\t\tif not self.apply_d_sn:\n",
    "\t\t\t\tx = self.bn0(x)\n",
    "\t\t\tx = self.activation(x)\n",
    "\n",
    "\t\t\tx = self.conv1(x)\n",
    "\t\t\tif not self.apply_d_sn:\n",
    "\t\t\t\tx = self.bn1(x)\n",
    "\t\t\tout = self.activation(x)\n",
    "\t\t\treturn out\n",
    "\n",
    "\tclass Discriminator(nn.Module):\n",
    "\t\tdef __init__(self, img_size, d_conv_dim, apply_d_sn, apply_attn, attn_d_loc, d_cond_mtd, aux_cls_type, d_embed_dim, normalize_d_embed,\n",
    "\t\t\t\t\tnum_classes, d_init, d_depth, MODULES, MODEL):\n",
    "\t\t\tsuper(DCGAN.Discriminator, self).__init__()\n",
    "\t\t\tself.in_dims = [3] + [64, 128]\n",
    "\t\t\tself.out_dims = [64, 128, 256]\n",
    "\n",
    "\t\t\tself.apply_d_sn = apply_d_sn\n",
    "\t\t\tself.d_cond_mtd = d_cond_mtd\n",
    "\t\t\tself.aux_cls_type = aux_cls_type\n",
    "\t\t\tself.normalize_d_embed = normalize_d_embed\n",
    "\t\t\tself.num_classes = num_classes\n",
    "\t\t\t# self.mixed_precision = mixed_precision\n",
    "\t\t\tself.MODEL= MODEL\n",
    "\n",
    "\t\t\tself.blocks = []\n",
    "\t\t\tfor index in range(len(self.in_dims)):\n",
    "\t\t\t\tself.blocks += [[\n",
    "\t\t\t\t\tDCGAN.DiscBlock(in_channels=self.in_dims[index], out_channels=self.out_dims[index], apply_d_sn=self.apply_d_sn, MODULES=MODULES)\n",
    "\t\t\t\t]]\n",
    "\n",
    "\t\t\t\tif index + 1 in attn_d_loc and apply_attn:\n",
    "\t\t\t\t\tself.blocks += [[SelfAttention(self.out_dims[index], is_generator=False, MODULES=MODULES)]]\n",
    "\n",
    "\t\t\tself.blocks = nn.ModuleList([nn.ModuleList(block) for block in self.blocks])\n",
    "\n",
    "\t\t\tself.activation = MODULES.d_act_fn\n",
    "\t\t\tself.conv1 = MODULES.d_conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "\t\t\tif not self.apply_d_sn:\n",
    "\t\t\t\tself.bn1 = MODULES.d_bn(in_features=512)\n",
    "\n",
    "\t\t\t# linear layer for adversarial training\n",
    "\t\t\tif self.d_cond_mtd == \"MH\":\n",
    "\t\t\t\tself.linear1 = MODULES.d_linear(in_features=512, out_features=1 + num_classes, bias=True)\n",
    "\t\t\telif self.d_cond_mtd == \"MD\":\n",
    "\t\t\t\tself.linear1 = MODULES.d_linear(in_features=512, out_features=num_classes, bias=True)\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.linear1 = MODULES.d_linear(in_features=512, out_features=1, bias=True)\n",
    "\n",
    "\t\t\t# double num_classes for Auxiliary Discriminative Classifier\n",
    "\t\t\tif self.aux_cls_type == \"ADC\":\n",
    "\t\t\t\tnum_classes = num_classes * 2\n",
    "\n",
    "\t\t\t# linear and embedding layers for discriminator conditioning\n",
    "\t\t\tif self.d_cond_mtd == \"AC\":\n",
    "\t\t\t\tself.linear2 = MODULES.d_linear(in_features=512, out_features=num_classes, bias=False)\n",
    "\t\t\telif self.d_cond_mtd == \"PD\":\n",
    "\t\t\t\tself.embedding = MODULES.d_embedding(num_classes, 512)\n",
    "\t\t\telif self.d_cond_mtd in [\"2C\", \"D2DCE\"]:\n",
    "\t\t\t\tself.linear2 = MODULES.d_linear(in_features=512, out_features=d_embed_dim, bias=True)\n",
    "\t\t\t\tself.embedding = MODULES.d_embedding(num_classes, d_embed_dim)\n",
    "\t\t\telse:\n",
    "\t\t\t\tpass\n",
    "\n",
    "\t\t\t# linear and embedding layers for evolved classifier-based GAN\n",
    "\t\t\tif self.aux_cls_type == \"TAC\":\n",
    "\t\t\t\tif self.d_cond_mtd == \"AC\":\n",
    "\t\t\t\t\tself.linear_mi = MODULES.d_linear(in_features=512, out_features=num_classes, bias=False)\n",
    "\t\t\t\telif self.d_cond_mtd in [\"2C\", \"D2DCE\"]:\n",
    "\t\t\t\t\tself.linear_mi = MODULES.d_linear(in_features=512, out_features=d_embed_dim, bias=True)\n",
    "\t\t\t\t\tself.embedding_mi = MODULES.d_embedding(num_classes, d_embed_dim)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\traise NotImplementedError\n",
    "\n",
    "\t\t\t# Q head network for infoGAN\n",
    "\t\t\tif self.MODEL.info_type in [\"discrete\", \"both\"]:\n",
    "\t\t\t\tout_features = self.MODEL.info_num_discrete_c*self.MODEL.info_dim_discrete_c\n",
    "\t\t\t\tself.info_discrete_linear = MODULES.d_linear(in_features=512, out_features=out_features, bias=False)\n",
    "\t\t\tif self.MODEL.info_type in [\"continuous\", \"both\"]:\n",
    "\t\t\t\tout_features = self.MODEL.info_num_conti_c\n",
    "\t\t\t\tself.info_conti_mu_linear = MODULES.d_linear(in_features=512, out_features=out_features, bias=False)\n",
    "\t\t\t\tself.info_conti_var_linear = MODULES.d_linear(in_features=512, out_features=out_features, bias=False)\n",
    "\n",
    "\t\t\tif d_init:\n",
    "\t\t\t\tinit_weights(self.modules, d_init)\n",
    "\n",
    "\t\tdef forward(self, x, label, eval=False, adc_fake=False):\n",
    "\t\t\twith torch.cuda.amp.autocast() as mp:\n",
    "\t\t\t\tembed, proxy, cls_output = None, None, None\n",
    "\t\t\t\tmi_embed, mi_proxy, mi_cls_output = None, None, None\n",
    "\t\t\t\tinfo_discrete_c_logits, info_conti_mu, info_conti_var = None, None, None\n",
    "\t\t\t\th = x\n",
    "\t\t\t\tfor index, blocklist in enumerate(self.blocks):\n",
    "\t\t\t\t\tfor block in blocklist:\n",
    "\t\t\t\t\t\th = block(h)\n",
    "\t\t\t\th = self.conv1(h)\n",
    "\t\t\t\tif not self.apply_d_sn:\n",
    "\t\t\t\t\th = self.bn1(h)\n",
    "\t\t\t\tbottom_h, bottom_w = h.shape[2], h.shape[3]\n",
    "\t\t\t\th = self.activation(h)\n",
    "\t\t\t\th = torch.sum(h, dim=[2, 3])\n",
    "\n",
    "\t\t\t\t# adversarial training\n",
    "\t\t\t\tadv_output = torch.squeeze(self.linear1(h))\n",
    "\n",
    "\t\t\t\t# make class labels odd (for fake) or even (for real) for ADC\n",
    "\t\t\t\tif self.aux_cls_type == \"ADC\":\n",
    "\t\t\t\t\tif adc_fake:\n",
    "\t\t\t\t\t\tlabel = label*2 + 1\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tlabel = label*2\n",
    "\n",
    "\t\t\t\t# forward pass through InfoGAN Q head\n",
    "\t\t\t\tif self.MODEL.info_type in [\"discrete\", \"both\"]:\n",
    "\t\t\t\t\tinfo_discrete_c_logits = self.info_discrete_linear(h/(bottom_h*bottom_w))\n",
    "\t\t\t\tif self.MODEL.info_type in [\"continuous\", \"both\"]:\n",
    "\t\t\t\t\tinfo_conti_mu = self.info_conti_mu_linear(h/(bottom_h*bottom_w))\n",
    "\t\t\t\t\tinfo_conti_var = torch.exp(self.info_conti_var_linear(h/(bottom_h*bottom_w)))\n",
    "\n",
    "\t\t\t\t# class conditioning\n",
    "\t\t\t\tif self.d_cond_mtd == \"AC\":\n",
    "\t\t\t\t\tif self.normalize_d_embed:\n",
    "\t\t\t\t\t\tfor W in self.linear2.parameters():\n",
    "\t\t\t\t\t\t\tW = F.normalize(W, dim=1)\n",
    "\t\t\t\t\t\th = F.normalize(h, dim=1)\n",
    "\t\t\t\t\tcls_output = self.linear2(h)\n",
    "\t\t\t\telif self.d_cond_mtd == \"PD\":\n",
    "\t\t\t\t\tadv_output = adv_output + torch.sum(torch.mul(self.embedding(label), h), 1)\n",
    "\t\t\t\telif self.d_cond_mtd in [\"2C\", \"D2DCE\"]:\n",
    "\t\t\t\t\tembed = self.linear2(h)\n",
    "\t\t\t\t\tproxy = self.embedding(label)\n",
    "\t\t\t\t\tif self.normalize_d_embed:\n",
    "\t\t\t\t\t\tembed = F.normalize(embed, dim=1)\n",
    "\t\t\t\t\t\tproxy = F.normalize(proxy, dim=1)\n",
    "\t\t\t\telif self.d_cond_mtd == \"MD\":\n",
    "\t\t\t\t\tidx = torch.LongTensor(range(label.size(0))).to(label.device)\n",
    "\t\t\t\t\tadv_output = adv_output[idx, label]\n",
    "\t\t\t\telif self.d_cond_mtd in [\"W/O\", \"MH\"]:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\traise NotImplementedError\n",
    "\n",
    "\t\t\t\t# extra conditioning for TACGAN and ADCGAN\n",
    "\t\t\t\tif self.aux_cls_type == \"TAC\":\n",
    "\t\t\t\t\tif self.d_cond_mtd == \"AC\":\n",
    "\t\t\t\t\t\tif self.normalize_d_embed:\n",
    "\t\t\t\t\t\t\tfor W in self.linear_mi.parameters():\n",
    "\t\t\t\t\t\t\t\tW = F.normalize(W, dim=1)\n",
    "\t\t\t\t\t\tmi_cls_output = self.linear_mi(h)\n",
    "\t\t\t\t\telif self.d_cond_mtd in [\"2C\", \"D2DCE\"]:\n",
    "\t\t\t\t\t\tmi_embed = self.linear_mi(h)\n",
    "\t\t\t\t\t\tmi_proxy = self.embedding_mi(label)\n",
    "\t\t\t\t\t\tif self.normalize_d_embed:\n",
    "\t\t\t\t\t\t\tmi_embed = F.normalize(mi_embed, dim=1)\n",
    "\t\t\t\t\t\t\tmi_proxy = F.normalize(mi_proxy, dim=1)\n",
    "\n",
    "\t\t\treturn {\n",
    "\t\t\t\t\"h\": h,\n",
    "\t\t\t\t\"adv_output\": adv_output,\n",
    "\t\t\t\t\"embed\": embed,\n",
    "\t\t\t\t\"proxy\": proxy,\n",
    "\t\t\t\t\"cls_output\": cls_output,\n",
    "\t\t\t\t\"label\": label,\n",
    "\t\t\t\t\"mi_embed\": mi_embed,\n",
    "\t\t\t\t\"mi_proxy\": mi_proxy,\n",
    "\t\t\t\t\"mi_cls_output\": mi_cls_output,\n",
    "\t\t\t\t\"info_discrete_c_logits\": info_discrete_c_logits,\n",
    "\t\t\t\t\"info_conti_mu\": info_conti_mu,\n",
    "\t\t\t\t\"info_conti_var\": info_conti_var\n",
    "\t\t\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "[INFO] 2023-01-26 16:02:27 > Epoch: 6 | Step:    500 Progress: 50.0% Elapsed: 0:01:35 Gen_loss: 7.0 Dis_loss: 0.003394 Cls_loss: N/A \n",
      "[INFO] 2023-01-26 16:03:52 > Epoch: 12 | Step:   1000 Progress: 100.0% Elapsed: 0:02:59 Gen_loss: 9.255 Dis_loss: 0.003528 Cls_loss: N/A \n"
     ]
    }
   ],
   "source": [
    "cfg = Configurations({\n",
    "    'DATA': {\n",
    "\t\t'name': 'CIFAR10',\n",
    "\t\t'img_size': 32,\n",
    "\t\t'num_classes': 10\n",
    "\t},\n",
    "    'MODEL': {\n",
    "\t\t'backbone': 'deep_conv',\n",
    "\t\t'g_cond_mtd': 'cBN',\n",
    "\t\t'd_cond_mtd': 'PD',\n",
    "\t\t'g_conv_dim': 'N/A',\n",
    "\t\t'd_conv_dim': 'N/A'\n",
    "\t},\n",
    "    'OPTIMIZATION': {\n",
    "\t\t'd_updates_per_step': 2,\n",
    "\t\t'total_steps': 1000,\n",
    "        'batch_size': 300 \n",
    "\t}\n",
    "})\n",
    "\n",
    "trainer = Trainer()\n",
    "trainer.load(cfg, 'DCGAN', DCGAN)\n",
    "FID_Calculator = FIDCalculator(trainer.train_dataloader)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FID_Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = Configurations({\n",
    "#     'DATA': {\n",
    "# \t\t'name': 'CIFAR10',\n",
    "# \t\t'img_size': 32,\n",
    "# \t\t'num_classes': 10\n",
    "# \t},\n",
    "#     'MODEL': {\n",
    "# \t\t'backbone': 'deep_conv',\n",
    "# \t\t'g_cond_mtd': 'cBN',\n",
    "# \t\t'd_cond_mtd': 'PD',\n",
    "# \t\t'g_conv_dim': 'N/A',\n",
    "# \t\t'd_conv_dim': 'N/A'\n",
    "# \t},\n",
    "#     'OPTIMIZATION': {\n",
    "# \t\t'd_updates_per_step': 2,\n",
    "# \t\t'total_steps': 200000,\n",
    "#         'batch_size': 300\n",
    "# \t}\n",
    "# })\n",
    "\n",
    "# trainer = Trainer()\n",
    "# trainer.load(cfg, \"cDCGAN\", DCGAN)\n",
    "# trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying **Spectral Normalization**\n",
    "Spectral Normalization is a method that aims to enforce Lipschitz continuity onto the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch StudioGAN: https://github.com/POSTECH-CVLab/PyTorch-StudioGAN\n",
    "# The MIT License (MIT)\n",
    "# See license file or visit https://github.com/POSTECH-CVLab/PyTorch-StudioGAN for details\n",
    "\n",
    "# models/resnet.py\n",
    "\n",
    "class ResNet:\n",
    "\tclass GenBlock(nn.Module):\n",
    "\t\tdef __init__(self, in_channels, out_channels, g_cond_mtd, g_info_injection, affine_input_dim, MODULES):\n",
    "\t\t\tsuper(ResNet.GenBlock, self).__init__()\n",
    "\t\t\tself.g_cond_mtd = g_cond_mtd\n",
    "\t\t\tself.g_info_injection = g_info_injection\n",
    "\n",
    "\t\t\tif self.g_cond_mtd == \"W/O\" and self.g_info_injection in [\"N/A\", \"concat\"]:\n",
    "\t\t\t\tself.bn1 = MODULES.g_bn(in_features=in_channels)\n",
    "\t\t\t\tself.bn2 = MODULES.g_bn(in_features=out_channels)\n",
    "\t\t\telif self.g_cond_mtd == \"cBN\" or self.g_info_injection == \"cBN\":\n",
    "\t\t\t\tself.bn1 = MODULES.g_bn(affine_input_dim, in_channels, MODULES)\n",
    "\t\t\t\tself.bn2 = MODULES.g_bn(affine_input_dim, out_channels, MODULES)\n",
    "\t\t\telse:\n",
    "\t\t\t\traise NotImplementedError\n",
    "\n",
    "\t\t\tself.activation = MODULES.g_act_fn\n",
    "\t\t\tself.conv2d0 = MODULES.g_conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\t\t\tself.conv2d1 = MODULES.g_conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\t\t\tself.conv2d2 = MODULES.g_conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "\t\tdef forward(self, x, affine):\n",
    "\t\t\tx0 = x\n",
    "\t\t\tif self.g_cond_mtd == \"W/O\" and self.g_info_injection in [\"N/A\", \"concat\"]:\n",
    "\t\t\t\tx = self.bn1(x)\n",
    "\t\t\telif self.g_cond_mtd == \"cBN\" or self.g_info_injection == \"cBN\":\n",
    "\t\t\t\tx = self.bn1(x, affine)\n",
    "\t\t\telse:\n",
    "\t\t\t\traise NotImplementedError\n",
    "\t\t\tx = self.activation(x)\n",
    "\t\t\tx = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "\t\t\tx = self.conv2d1(x)\n",
    "\n",
    "\t\t\tif self.g_cond_mtd == \"W/O\" and self.g_info_injection in [\"N/A\", \"concat\"]:\n",
    "\t\t\t\tx = self.bn2(x)\n",
    "\t\t\telif self.g_cond_mtd == \"cBN\" or self.g_info_injection == \"cBN\":\n",
    "\t\t\t\tx = self.bn2(x, affine)\n",
    "\t\t\telse:\n",
    "\t\t\t\traise NotImplementedError\n",
    "\t\t\tx = self.activation(x)\n",
    "\t\t\tx = self.conv2d2(x)\n",
    "\n",
    "\t\t\tx0 = F.interpolate(x0, scale_factor=2, mode=\"nearest\")\n",
    "\t\t\tx0 = self.conv2d0(x0)\n",
    "\t\t\tout = x + x0\n",
    "\t\t\treturn out\n",
    "\n",
    "\n",
    "\tclass Generator(nn.Module):\n",
    "\t\tdef __init__(self, z_dim, g_shared_dim, img_size, g_conv_dim, apply_attn, attn_g_loc, g_cond_mtd, num_classes, g_init, g_depth,\n",
    "\t\t\t\t\tMODULES, MODEL):\n",
    "\t\t\tsuper(ResNet.Generator, self).__init__()\n",
    "\t\t\tg_in_dims_collection = {\n",
    "\t\t\t\t\"32\": [g_conv_dim * 4, g_conv_dim * 4, g_conv_dim * 4],\n",
    "\t\t\t\t\"64\": [g_conv_dim * 16, g_conv_dim * 8, g_conv_dim * 4, g_conv_dim * 2],\n",
    "\t\t\t\t\"128\": [g_conv_dim * 16, g_conv_dim * 16, g_conv_dim * 8, g_conv_dim * 4, g_conv_dim * 2],\n",
    "\t\t\t\t\"256\": [g_conv_dim * 16, g_conv_dim * 16, g_conv_dim * 8, g_conv_dim * 8, g_conv_dim * 4, g_conv_dim * 2],\n",
    "\t\t\t\t\"512\": [g_conv_dim * 16, g_conv_dim * 16, g_conv_dim * 8, g_conv_dim * 8, g_conv_dim * 4, g_conv_dim * 2, g_conv_dim]\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\tg_out_dims_collection = {\n",
    "\t\t\t\t\"32\": [g_conv_dim * 4, g_conv_dim * 4, g_conv_dim * 4],\n",
    "\t\t\t\t\"64\": [g_conv_dim * 8, g_conv_dim * 4, g_conv_dim * 2, g_conv_dim],\n",
    "\t\t\t\t\"128\": [g_conv_dim * 16, g_conv_dim * 8, g_conv_dim * 4, g_conv_dim * 2, g_conv_dim],\n",
    "\t\t\t\t\"256\": [g_conv_dim * 16, g_conv_dim * 8, g_conv_dim * 8, g_conv_dim * 4, g_conv_dim * 2, g_conv_dim],\n",
    "\t\t\t\t\"512\": [g_conv_dim * 16, g_conv_dim * 8, g_conv_dim * 8, g_conv_dim * 4, g_conv_dim * 2, g_conv_dim, g_conv_dim]\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\tbottom_collection = {\"32\": 4, \"64\": 4, \"128\": 4, \"256\": 4, \"512\": 4}\n",
    "\n",
    "\t\t\tself.z_dim = z_dim\n",
    "\t\t\tself.num_classes = num_classes\n",
    "\t\t\tself.g_cond_mtd = g_cond_mtd\n",
    "\t\t\tself.mixed_precision = False\n",
    "\t\t\tself.MODEL = MODEL\n",
    "\t\t\tself.in_dims = g_in_dims_collection[str(img_size)]\n",
    "\t\t\tself.out_dims = g_out_dims_collection[str(img_size)]\n",
    "\t\t\tself.bottom = bottom_collection[str(img_size)]\n",
    "\t\t\tself.num_blocks = len(self.in_dims)\n",
    "\t\t\tself.affine_input_dim = 0\n",
    "\n",
    "\t\t\tinfo_dim = 0\n",
    "\t\t\tif self.MODEL.info_type in [\"discrete\", \"both\"]:\n",
    "\t\t\t\tinfo_dim += self.MODEL.info_num_discrete_c*self.MODEL.info_dim_discrete_c\n",
    "\t\t\tif self.MODEL.info_type in [\"continuous\", \"both\"]:\n",
    "\t\t\t\tinfo_dim += self.MODEL.info_num_conti_c\n",
    "\n",
    "\t\t\tself.g_info_injection = self.MODEL.g_info_injection\n",
    "\t\t\tif self.MODEL.info_type != \"N/A\":\n",
    "\t\t\t\tif self.g_info_injection == \"concat\":\n",
    "\t\t\t\t\tself.info_mix_linear = MODULES.g_linear(in_features=self.z_dim + info_dim, out_features=self.z_dim, bias=True)\n",
    "\t\t\t\telif self.g_info_injection == \"cBN\":\n",
    "\t\t\t\t\tself.affine_input_dim += self.z_dim\n",
    "\t\t\t\t\tself.info_proj_linear = MODULES.g_linear(in_features=info_dim, out_features=self.z_dim, bias=True)\n",
    "\n",
    "\t\t\tself.linear0 = MODULES.g_linear(in_features=self.z_dim, out_features=self.in_dims[0] * self.bottom * self.bottom, bias=True)\n",
    "\n",
    "\t\t\tif self.g_cond_mtd != \"W/O\" and self.g_cond_mtd == \"cBN\":\n",
    "\t\t\t\tself.affine_input_dim += self.num_classes\n",
    "\n",
    "\t\t\tself.blocks = []\n",
    "\t\t\tfor index in range(self.num_blocks):\n",
    "\t\t\t\tself.blocks += [[\n",
    "\t\t\t\t\tResNet.GenBlock(in_channels=self.in_dims[index],\n",
    "\t\t\t\t\t\t\tout_channels=self.out_dims[index],\n",
    "\t\t\t\t\t\t\tg_cond_mtd=self.g_cond_mtd,\n",
    "\t\t\t\t\t\t\tg_info_injection=self.g_info_injection,\n",
    "\t\t\t\t\t\t\taffine_input_dim=self.affine_input_dim,\n",
    "\t\t\t\t\t\t\tMODULES=MODULES)\n",
    "\t\t\t\t]]\n",
    "\n",
    "\t\t\t\tif index + 1 in attn_g_loc and apply_attn:\n",
    "\t\t\t\t\tself.blocks += [[SelfAttention(self.out_dims[index], is_generator=True, MODULES=MODULES)]]\n",
    "\n",
    "\t\t\tself.blocks = nn.ModuleList([nn.ModuleList(block) for block in self.blocks])\n",
    "\n",
    "\t\t\tself.bn4 = batchnorm_2d(in_features=self.out_dims[-1])\n",
    "\t\t\tself.activation = MODULES.g_act_fn\n",
    "\t\t\tself.conv2d5 = MODULES.g_conv2d(in_channels=self.out_dims[-1], out_channels=3, kernel_size=3, stride=1, padding=1)\n",
    "\t\t\tself.tanh = nn.Tanh()\n",
    "\n",
    "\t\t\tinit_weights(self.modules, g_init)\n",
    "\n",
    "\t\tdef forward(self, z, label, shared_label=None, eval=False):\n",
    "\t\t\taffine_list = []\n",
    "\t\t\tif self.g_cond_mtd != \"W/O\":\n",
    "\t\t\t\tlabel = F.one_hot(label, num_classes=self.num_classes).to(torch.float32)\n",
    "\t\t\twith torch.cuda.amp.autocast() if self.mixed_precision and not eval else dummy_context_mgr() as mp:\n",
    "\t\t\t\tif self.MODEL.info_type != \"N/A\":\n",
    "\t\t\t\t\tif self.g_info_injection == \"concat\":\n",
    "\t\t\t\t\t\tz = self.info_mix_linear(z)\n",
    "\t\t\t\t\telif self.g_info_injection == \"cBN\":\n",
    "\t\t\t\t\t\tz, z_info = z[:, :self.z_dim], z[:, self.z_dim:]\n",
    "\t\t\t\t\t\taffine_list.append(self.info_proj_linear(z_info))\n",
    "\n",
    "\t\t\t\tif self.g_cond_mtd != \"W/O\":\n",
    "\t\t\t\t\taffine_list.append(label)\n",
    "\t\t\t\tif len(affine_list) > 0:\n",
    "\t\t\t\t\taffines = torch.cat(affine_list, 1)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\taffines = None\n",
    "\n",
    "\t\t\t\tact = self.linear0(z)\n",
    "\t\t\t\tact = act.view(-1, self.in_dims[0], self.bottom, self.bottom)\n",
    "\t\t\t\tfor index, blocklist in enumerate(self.blocks):\n",
    "\t\t\t\t\tfor block in blocklist:\n",
    "\t\t\t\t\t\tif isinstance(block, SelfAttention):\n",
    "\t\t\t\t\t\t\tact = block(act)\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tact = block(act, affines)\n",
    "\n",
    "\t\t\t\tact = self.bn4(act)\n",
    "\t\t\t\tact = self.activation(act)\n",
    "\t\t\t\tact = self.conv2d5(act)\n",
    "\t\t\t\tout = self.tanh(act)\n",
    "\t\t\treturn out\n",
    "\n",
    "\n",
    "\tclass DiscOptBlock(nn.Module):\n",
    "\t\tdef __init__(self, in_channels, out_channels, apply_d_sn, MODULES):\n",
    "\t\t\tsuper(ResNet.DiscOptBlock, self).__init__()\n",
    "\t\t\tself.apply_d_sn = apply_d_sn\n",
    "\n",
    "\t\t\tself.conv2d0 = MODULES.d_conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\t\t\tself.conv2d1 = MODULES.d_conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\t\t\tself.conv2d2 = MODULES.d_conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "\t\t\tif not apply_d_sn:\n",
    "\t\t\t\tself.bn0 = MODULES.d_bn(in_features=in_channels)\n",
    "\t\t\t\tself.bn1 = MODULES.d_bn(in_features=out_channels)\n",
    "\n",
    "\t\t\tself.activation = MODULES.d_act_fn\n",
    "\n",
    "\t\t\tself.average_pooling = nn.AvgPool2d(2)\n",
    "\n",
    "\t\tdef forward(self, x):\n",
    "\t\t\tx0 = x\n",
    "\t\t\tx = self.conv2d1(x)\n",
    "\t\t\tif not self.apply_d_sn:\n",
    "\t\t\t\tx = self.bn1(x)\n",
    "\t\t\tx = self.activation(x)\n",
    "\n",
    "\t\t\tx = self.conv2d2(x)\n",
    "\t\t\tx = self.average_pooling(x)\n",
    "\n",
    "\t\t\tx0 = self.average_pooling(x0)\n",
    "\t\t\tif not self.apply_d_sn:\n",
    "\t\t\t\tx0 = self.bn0(x0)\n",
    "\t\t\tx0 = self.conv2d0(x0)\n",
    "\t\t\tout = x + x0\n",
    "\t\t\treturn out\n",
    "\n",
    "\n",
    "\tclass DiscBlock(nn.Module):\n",
    "\t\tdef __init__(self, in_channels, out_channels, apply_d_sn, MODULES, downsample=True):\n",
    "\t\t\tsuper(ResNet.DiscBlock, self).__init__()\n",
    "\t\t\tself.apply_d_sn = apply_d_sn\n",
    "\t\t\tself.downsample = downsample\n",
    "\n",
    "\t\t\tself.activation = MODULES.d_act_fn\n",
    "\n",
    "\t\t\tself.ch_mismatch = False\n",
    "\t\t\tif in_channels != out_channels:\n",
    "\t\t\t\tself.ch_mismatch = True\n",
    "\n",
    "\t\t\tif self.ch_mismatch or downsample:\n",
    "\t\t\t\tself.conv2d0 = MODULES.d_conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\t\t\t\tif not apply_d_sn:\n",
    "\t\t\t\t\tself.bn0 = MODULES.d_bn(in_features=in_channels)\n",
    "\n",
    "\t\t\tself.conv2d1 = MODULES.d_conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\t\t\tself.conv2d2 = MODULES.d_conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "\t\t\tif not apply_d_sn:\n",
    "\t\t\t\tself.bn1 = MODULES.d_bn(in_features=in_channels)\n",
    "\t\t\t\tself.bn2 = MODULES.d_bn(in_features=out_channels)\n",
    "\n",
    "\t\t\tself.average_pooling = nn.AvgPool2d(2)\n",
    "\n",
    "\t\tdef forward(self, x):\n",
    "\t\t\tx0 = x\n",
    "\t\t\tif not self.apply_d_sn:\n",
    "\t\t\t\tx = self.bn1(x)\n",
    "\t\t\tx = self.activation(x)\n",
    "\t\t\tx = self.conv2d1(x)\n",
    "\n",
    "\t\t\tif not self.apply_d_sn:\n",
    "\t\t\t\tx = self.bn2(x)\n",
    "\t\t\tx = self.activation(x)\n",
    "\t\t\tx = self.conv2d2(x)\n",
    "\t\t\tif self.downsample:\n",
    "\t\t\t\tx = self.average_pooling(x)\n",
    "\n",
    "\t\t\tif self.downsample or self.ch_mismatch:\n",
    "\t\t\t\tif not self.apply_d_sn:\n",
    "\t\t\t\t\tx0 = self.bn0(x0)\n",
    "\t\t\t\tx0 = self.conv2d0(x0)\n",
    "\t\t\t\tif self.downsample:\n",
    "\t\t\t\t\tx0 = self.average_pooling(x0)\n",
    "\t\t\tout = x + x0\n",
    "\t\t\treturn out\n",
    "\n",
    "\n",
    "\tclass Discriminator(nn.Module):\n",
    "\t\tdef __init__(self, img_size, d_conv_dim, apply_d_sn, apply_attn, attn_d_loc, d_cond_mtd, aux_cls_type, d_embed_dim, normalize_d_embed,\n",
    "\t\t\t\t\tnum_classes, d_init, d_depth, MODULES, MODEL):\n",
    "\t\t\tsuper(ResNet.Discriminator, self).__init__()\n",
    "\t\t\td_in_dims_collection = {\n",
    "\t\t\t\t\"32\": [3] + [d_conv_dim * 2, d_conv_dim * 2, d_conv_dim * 2],\n",
    "\t\t\t\t\"64\": [3] + [d_conv_dim, d_conv_dim * 2, d_conv_dim * 4, d_conv_dim * 8],\n",
    "\t\t\t\t\"128\": [3] + [d_conv_dim, d_conv_dim * 2, d_conv_dim * 4, d_conv_dim * 8, d_conv_dim * 16],\n",
    "\t\t\t\t\"256\": [3] + [d_conv_dim, d_conv_dim * 2, d_conv_dim * 4, d_conv_dim * 8, d_conv_dim * 8, d_conv_dim * 16],\n",
    "\t\t\t\t\"512\": [3] + [d_conv_dim, d_conv_dim, d_conv_dim * 2, d_conv_dim * 4, d_conv_dim * 8, d_conv_dim * 8, d_conv_dim * 16]\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\td_out_dims_collection = {\n",
    "\t\t\t\t\"32\": [d_conv_dim * 2, d_conv_dim * 2, d_conv_dim * 2, d_conv_dim * 2],\n",
    "\t\t\t\t\"64\": [d_conv_dim, d_conv_dim * 2, d_conv_dim * 4, d_conv_dim * 8, d_conv_dim * 16],\n",
    "\t\t\t\t\"128\": [d_conv_dim, d_conv_dim * 2, d_conv_dim * 4, d_conv_dim * 8, d_conv_dim * 16, d_conv_dim * 16],\n",
    "\t\t\t\t\"256\": [d_conv_dim, d_conv_dim * 2, d_conv_dim * 4, d_conv_dim * 8, d_conv_dim * 8, d_conv_dim * 16, d_conv_dim * 16],\n",
    "\t\t\t\t\"512\":\n",
    "\t\t\t\t[d_conv_dim, d_conv_dim, d_conv_dim * 2, d_conv_dim * 4, d_conv_dim * 8, d_conv_dim * 8, d_conv_dim * 16, d_conv_dim * 16]\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\td_down = {\n",
    "\t\t\t\t\"32\": [True, True, False, False],\n",
    "\t\t\t\t\"64\": [True, True, True, True, False],\n",
    "\t\t\t\t\"128\": [True, True, True, True, True, False],\n",
    "\t\t\t\t\"256\": [True, True, True, True, True, True, False],\n",
    "\t\t\t\t\"512\": [True, True, True, True, True, True, True, False]\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\tself.d_cond_mtd = d_cond_mtd\n",
    "\t\t\tself.aux_cls_type = aux_cls_type\n",
    "\t\t\tself.normalize_d_embed = normalize_d_embed\n",
    "\t\t\tself.num_classes = num_classes\n",
    "\t\t\tself.mixed_precision = False\n",
    "\t\t\tself.in_dims = d_in_dims_collection[str(img_size)]\n",
    "\t\t\tself.out_dims = d_out_dims_collection[str(img_size)]\n",
    "\t\t\tself.MODEL = MODEL\n",
    "\t\t\tdown = d_down[str(img_size)]\n",
    "\n",
    "\t\t\tself.blocks = []\n",
    "\t\t\tfor index in range(len(self.in_dims)):\n",
    "\t\t\t\tif index == 0:\n",
    "\t\t\t\t\tself.blocks += [[\n",
    "\t\t\t\t\t\tResNet.DiscOptBlock(in_channels=self.in_dims[index], out_channels=self.out_dims[index], apply_d_sn=apply_d_sn, MODULES=MODULES)\n",
    "\t\t\t\t\t]]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tself.blocks += [[\n",
    "\t\t\t\t\t\tResNet.DiscBlock(in_channels=self.in_dims[index],\n",
    "\t\t\t\t\t\t\t\tout_channels=self.out_dims[index],\n",
    "\t\t\t\t\t\t\t\tapply_d_sn=apply_d_sn,\n",
    "\t\t\t\t\t\t\t\tMODULES=MODULES,\n",
    "\t\t\t\t\t\t\t\tdownsample=down[index])\n",
    "\t\t\t\t\t]]\n",
    "\n",
    "\t\t\t\tif index + 1 in attn_d_loc and apply_attn:\n",
    "\t\t\t\t\tself.blocks += [[SelfAttention(self.out_dims[index], is_generator=False, MODULES=MODULES)]]\n",
    "\n",
    "\t\t\tself.blocks = nn.ModuleList([nn.ModuleList(block) for block in self.blocks])\n",
    "\n",
    "\t\t\tself.activation = MODULES.d_act_fn\n",
    "\n",
    "\t\t\t# linear layer for adversarial training\n",
    "\t\t\tif self.d_cond_mtd == \"MH\":\n",
    "\t\t\t\tself.linear1 = MODULES.d_linear(in_features=self.out_dims[-1], out_features=1 + num_classes, bias=True)\n",
    "\t\t\telif self.d_cond_mtd == \"MD\":\n",
    "\t\t\t\tself.linear1 = MODULES.d_linear(in_features=self.out_dims[-1], out_features=num_classes, bias=True)\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.linear1 = MODULES.d_linear(in_features=self.out_dims[-1], out_features=1, bias=True)\n",
    "\n",
    "\t\t\t# double num_classes for Auxiliary Discriminative Classifier\n",
    "\t\t\tif self.aux_cls_type == \"ADC\":\n",
    "\t\t\t\tnum_classes = num_classes * 2\n",
    "\n",
    "\t\t\t# linear and embedding layers for discriminator conditioning\n",
    "\t\t\tif self.d_cond_mtd == \"AC\":\n",
    "\t\t\t\tself.linear2 = MODULES.d_linear(in_features=self.out_dims[-1], out_features=num_classes, bias=False)\n",
    "\t\t\telif self.d_cond_mtd == \"PD\":\n",
    "\t\t\t\tself.embedding = MODULES.d_embedding(num_classes, self.out_dims[-1])\n",
    "\t\t\telif self.d_cond_mtd in [\"2C\", \"D2DCE\"]:\n",
    "\t\t\t\tself.linear2 = MODULES.d_linear(in_features=self.out_dims[-1], out_features=d_embed_dim, bias=True)\n",
    "\t\t\t\tself.embedding = MODULES.d_embedding(num_classes, d_embed_dim)\n",
    "\t\t\telse:\n",
    "\t\t\t\tpass\n",
    "\n",
    "\t\t\t# linear and embedding layers for evolved classifier-based GAN\n",
    "\t\t\tif self.aux_cls_type == \"TAC\":\n",
    "\t\t\t\tif self.d_cond_mtd == \"AC\":\n",
    "\t\t\t\t\tself.linear_mi = MODULES.d_linear(in_features=self.out_dims[-1], out_features=num_classes, bias=False)\n",
    "\t\t\t\telif self.d_cond_mtd in [\"2C\", \"D2DCE\"]:\n",
    "\t\t\t\t\tself.linear_mi = MODULES.d_linear(in_features=self.out_dims[-1], out_features=d_embed_dim, bias=True)\n",
    "\t\t\t\t\tself.embedding_mi = MODULES.d_embedding(num_classes, d_embed_dim)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\traise NotImplementedError\n",
    "\n",
    "\t\t\t# Q head network for infoGAN\n",
    "\t\t\tif self.MODEL.info_type in [\"discrete\", \"both\"]:\n",
    "\t\t\t\tout_features = self.MODEL.info_num_discrete_c*self.MODEL.info_dim_discrete_c\n",
    "\t\t\t\tself.info_discrete_linear = MODULES.d_linear(in_features=self.out_dims[-1], out_features=out_features, bias=False)\n",
    "\t\t\tif self.MODEL.info_type in [\"continuous\", \"both\"]:\n",
    "\t\t\t\tout_features = self.MODEL.info_num_conti_c\n",
    "\t\t\t\tself.info_conti_mu_linear = MODULES.d_linear(in_features=self.out_dims[-1], out_features=out_features, bias=False)\n",
    "\t\t\t\tself.info_conti_var_linear = MODULES.d_linear(in_features=self.out_dims[-1], out_features=out_features, bias=False)\n",
    "\n",
    "\t\t\tif d_init:\n",
    "\t\t\t\tinit_weights(self.modules, d_init)\n",
    "\n",
    "\t\tdef forward(self, x, label, eval=False, adc_fake=False):\n",
    "\t\t\twith torch.cuda.amp.autocast() if self.mixed_precision and not eval else dummy_context_mgr() as mp:\n",
    "\t\t\t\tembed, proxy, cls_output = None, None, None\n",
    "\t\t\t\tmi_embed, mi_proxy, mi_cls_output = None, None, None\n",
    "\t\t\t\tinfo_discrete_c_logits, info_conti_mu, info_conti_var = None, None, None\n",
    "\t\t\t\th = x\n",
    "\t\t\t\tfor index, blocklist in enumerate(self.blocks):\n",
    "\t\t\t\t\tfor block in blocklist:\n",
    "\t\t\t\t\t\th = block(h)\n",
    "\t\t\t\tbottom_h, bottom_w = h.shape[2], h.shape[3]\n",
    "\t\t\t\th = self.activation(h)\n",
    "\t\t\t\th = torch.sum(h, dim=[2, 3])\n",
    "\n",
    "\t\t\t\t# adversarial training\n",
    "\t\t\t\tadv_output = torch.squeeze(self.linear1(h))\n",
    "\n",
    "\t\t\t\t# make class labels odd (for fake) or even (for real) for ADC\n",
    "\t\t\t\tif self.aux_cls_type == \"ADC\":\n",
    "\t\t\t\t\tif adc_fake:\n",
    "\t\t\t\t\t\tlabel = label*2 + 1\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tlabel = label*2\n",
    "\n",
    "\t\t\t\t# forward pass through InfoGAN Q head\n",
    "\t\t\t\tif self.MODEL.info_type in [\"discrete\", \"both\"]:\n",
    "\t\t\t\t\tinfo_discrete_c_logits = self.info_discrete_linear(h/(bottom_h*bottom_w))\n",
    "\t\t\t\tif self.MODEL.info_type in [\"continuous\", \"both\"]:\n",
    "\t\t\t\t\tinfo_conti_mu = self.info_conti_mu_linear(h/(bottom_h*bottom_w))\n",
    "\t\t\t\t\tinfo_conti_var = torch.exp(self.info_conti_var_linear(h/(bottom_h*bottom_w)))\n",
    "\n",
    "\t\t\t\t# class conditioning\n",
    "\t\t\t\tif self.d_cond_mtd == \"AC\":\n",
    "\t\t\t\t\tif self.normalize_d_embed:\n",
    "\t\t\t\t\t\tfor W in self.linear2.parameters():\n",
    "\t\t\t\t\t\t\tW = F.normalize(W, dim=1)\n",
    "\t\t\t\t\t\th = F.normalize(h, dim=1)\n",
    "\t\t\t\t\tcls_output = self.linear2(h)\n",
    "\t\t\t\telif self.d_cond_mtd == \"PD\":\n",
    "\t\t\t\t\tadv_output = adv_output + torch.sum(torch.mul(self.embedding(label), h), 1)\n",
    "\t\t\t\telif self.d_cond_mtd in [\"2C\", \"D2DCE\"]:\n",
    "\t\t\t\t\tembed = self.linear2(h)\n",
    "\t\t\t\t\tproxy = self.embedding(label)\n",
    "\t\t\t\t\tif self.normalize_d_embed:\n",
    "\t\t\t\t\t\tembed = F.normalize(embed, dim=1)\n",
    "\t\t\t\t\t\tproxy = F.normalize(proxy, dim=1)\n",
    "\t\t\t\telif self.d_cond_mtd == \"MD\":\n",
    "\t\t\t\t\tidx = torch.LongTensor(range(label.size(0))).to(label.device)\n",
    "\t\t\t\t\tadv_output = adv_output[idx, label]\n",
    "\t\t\t\telif self.d_cond_mtd in [\"W/O\", \"MH\"]:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\traise NotImplementedError\n",
    "\n",
    "\t\t\t\t# extra conditioning for TACGAN and ADCGAN\n",
    "\t\t\t\tif self.aux_cls_type == \"TAC\":\n",
    "\t\t\t\t\tif self.d_cond_mtd == \"AC\":\n",
    "\t\t\t\t\t\tif self.normalize_d_embed:\n",
    "\t\t\t\t\t\t\tfor W in self.linear_mi.parameters():\n",
    "\t\t\t\t\t\t\t\tW = F.normalize(W, dim=1)\n",
    "\t\t\t\t\t\tmi_cls_output = self.linear_mi(h)\n",
    "\t\t\t\t\telif self.d_cond_mtd in [\"2C\", \"D2DCE\"]:\n",
    "\t\t\t\t\t\tmi_embed = self.linear_mi(h)\n",
    "\t\t\t\t\t\tmi_proxy = self.embedding_mi(label)\n",
    "\t\t\t\t\t\tif self.normalize_d_embed:\n",
    "\t\t\t\t\t\t\tmi_embed = F.normalize(mi_embed, dim=1)\n",
    "\t\t\t\t\t\t\tmi_proxy = F.normalize(mi_proxy, dim=1)\n",
    "\t\t\treturn {\n",
    "\t\t\t\t\"h\": h,\n",
    "\t\t\t\t\"adv_output\": adv_output,\n",
    "\t\t\t\t\"embed\": embed,\n",
    "\t\t\t\t\"proxy\": proxy,\n",
    "\t\t\t\t\"cls_output\": cls_output,\n",
    "\t\t\t\t\"label\": label,\n",
    "\t\t\t\t\"mi_embed\": mi_embed,\n",
    "\t\t\t\t\"mi_proxy\": mi_proxy,\n",
    "\t\t\t\t\"mi_cls_output\": mi_cls_output,\n",
    "\t\t\t\t\"info_discrete_c_logits\": info_discrete_c_logits,\n",
    "\t\t\t\t\"info_conti_mu\": info_conti_mu,\n",
    "\t\t\t\t\"info_conti_var\": info_conti_var\n",
    "\t\t\t}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BigResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch StudioGAN: https://github.com/POSTECH-CVLab/PyTorch-StudioGAN\n",
    "# The MIT License (MIT)\n",
    "# See license file or visit https://github.com/POSTECH-CVLab/PyTorch-StudioGAN for details\n",
    "\n",
    "# models/big_resnet.py\n",
    "\n",
    "class BigResNet:\n",
    "\tclass GenBlock(nn.Module):\n",
    "\t\tdef __init__(self, in_channels, out_channels, g_cond_mtd, affine_input_dim, MODULES):\n",
    "\t\t\tsuper(BigResNet.GenBlock, self).__init__()\n",
    "\t\t\tself.g_cond_mtd = g_cond_mtd\n",
    "\n",
    "\t\t\tself.bn1 = MODULES.g_bn(affine_input_dim, in_channels, MODULES)\n",
    "\t\t\tself.bn2 = MODULES.g_bn(affine_input_dim, out_channels, MODULES)\n",
    "\n",
    "\t\t\tself.activation = MODULES.g_act_fn\n",
    "\t\t\tself.conv2d0 = MODULES.g_conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\t\t\tself.conv2d1 = MODULES.g_conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\t\t\tself.conv2d2 = MODULES.g_conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "\t\tdef forward(self, x, affine):\n",
    "\t\t\tx0 = x\n",
    "\t\t\tx = self.bn1(x, affine)\n",
    "\t\t\tx = self.activation(x)\n",
    "\t\t\tx = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "\t\t\tx = self.conv2d1(x)\n",
    "\n",
    "\t\t\tx = self.bn2(x, affine)\n",
    "\t\t\tx = self.activation(x)\n",
    "\t\t\tx = self.conv2d2(x)\n",
    "\n",
    "\t\t\tx0 = F.interpolate(x0, scale_factor=2, mode=\"nearest\")\n",
    "\t\t\tx0 = self.conv2d0(x0)\n",
    "\t\t\tout = x + x0\n",
    "\t\t\treturn out\n",
    "\n",
    "\n",
    "\tclass Generator(nn.Module):\n",
    "\t\tdef __init__(self, z_dim, g_shared_dim, img_size, g_conv_dim, apply_attn, attn_g_loc, g_cond_mtd, num_classes, g_init, g_depth,\n",
    "\t\t\t\t\tMODULES, MODEL):\n",
    "\t\t\tsuper(BigResNet.Generator, self).__init__()\n",
    "\t\t\tg_in_dims_collection = {\n",
    "\t\t\t\t\"32\": [g_conv_dim * 4, g_conv_dim * 4, g_conv_dim * 4],\n",
    "\t\t\t\t\"64\": [g_conv_dim * 16, g_conv_dim * 8, g_conv_dim * 4, g_conv_dim * 2],\n",
    "\t\t\t\t\"128\": [g_conv_dim * 16, g_conv_dim * 16, g_conv_dim * 8, g_conv_dim * 4, g_conv_dim * 2],\n",
    "\t\t\t\t\"256\": [g_conv_dim * 16, g_conv_dim * 16, g_conv_dim * 8, g_conv_dim * 8, g_conv_dim * 4, g_conv_dim * 2],\n",
    "\t\t\t\t\"512\": [g_conv_dim * 16, g_conv_dim * 16, g_conv_dim * 8, g_conv_dim * 8, g_conv_dim * 4, g_conv_dim * 2, g_conv_dim]\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\tg_out_dims_collection = {\n",
    "\t\t\t\t\"32\": [g_conv_dim * 4, g_conv_dim * 4, g_conv_dim * 4],\n",
    "\t\t\t\t\"64\": [g_conv_dim * 8, g_conv_dim * 4, g_conv_dim * 2, g_conv_dim],\n",
    "\t\t\t\t\"128\": [g_conv_dim * 16, g_conv_dim * 8, g_conv_dim * 4, g_conv_dim * 2, g_conv_dim],\n",
    "\t\t\t\t\"256\": [g_conv_dim * 16, g_conv_dim * 8, g_conv_dim * 8, g_conv_dim * 4, g_conv_dim * 2, g_conv_dim],\n",
    "\t\t\t\t\"512\": [g_conv_dim * 16, g_conv_dim * 8, g_conv_dim * 8, g_conv_dim * 4, g_conv_dim * 2, g_conv_dim, g_conv_dim]\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\tbottom_collection = {\"32\": 4, \"64\": 4, \"128\": 4, \"256\": 4, \"512\": 4}\n",
    "\n",
    "\t\t\tself.z_dim = z_dim\n",
    "\t\t\tself.g_shared_dim = g_shared_dim\n",
    "\t\t\tself.g_cond_mtd = g_cond_mtd\n",
    "\t\t\tself.num_classes = num_classes\n",
    "\t\t\tself.MODEL = MODEL\n",
    "\t\t\tself.in_dims = g_in_dims_collection[str(img_size)]\n",
    "\t\t\tself.out_dims = g_out_dims_collection[str(img_size)]\n",
    "\t\t\tself.bottom = bottom_collection[str(img_size)]\n",
    "\t\t\tself.mixed_precision = False\n",
    "\t\t\tself.num_blocks = len(self.in_dims)\n",
    "\t\t\tself.chunk_size = z_dim // (self.num_blocks + 1)\n",
    "\t\t\tself.affine_input_dim = self.chunk_size\n",
    "\t\t\tassert self.z_dim % (self.num_blocks + 1) == 0, \"z_dim should be divided by the number of blocks\"\n",
    "\n",
    "\t\t\tinfo_dim = 0\n",
    "\t\t\tif self.MODEL.info_type in [\"discrete\", \"both\"]:\n",
    "\t\t\t\tinfo_dim += self.MODEL.info_num_discrete_c*self.MODEL.info_dim_discrete_c\n",
    "\t\t\tif self.MODEL.info_type in [\"continuous\", \"both\"]:\n",
    "\t\t\t\tinfo_dim += self.MODEL.info_num_conti_c\n",
    "\n",
    "\t\t\tif self.MODEL.info_type != \"N/A\":\n",
    "\t\t\t\tif self.MODEL.g_info_injection == \"concat\":\n",
    "\t\t\t\t\tself.info_mix_linear = MODULES.g_linear(in_features=self.z_dim + info_dim, out_features=self.z_dim, bias=True)\n",
    "\t\t\t\telif self.MODEL.g_info_injection == \"cBN\":\n",
    "\t\t\t\t\tself.affine_input_dim += self.g_shared_dim\n",
    "\t\t\t\t\tself.info_proj_linear = MODULES.g_linear(in_features=info_dim, out_features=self.g_shared_dim, bias=True)\n",
    "\n",
    "\t\t\tself.linear0 = MODULES.g_linear(in_features=self.chunk_size, out_features=self.in_dims[0]*self.bottom*self.bottom, bias=True)\n",
    "\n",
    "\t\t\tif self.g_cond_mtd != \"W/O\":\n",
    "\t\t\t\tself.affine_input_dim += self.g_shared_dim\n",
    "\t\t\t\tself.shared = embedding(num_embeddings=self.num_classes, embedding_dim=self.g_shared_dim)\n",
    "\n",
    "\t\t\tself.blocks = []\n",
    "\t\t\tfor index in range(self.num_blocks):\n",
    "\t\t\t\tself.blocks += [[\n",
    "\t\t\t\t\tBigResNet.GenBlock(in_channels=self.in_dims[index],\n",
    "\t\t\t\t\t\t\tout_channels=self.out_dims[index],\n",
    "\t\t\t\t\t\t\tg_cond_mtd=self.g_cond_mtd,\n",
    "\t\t\t\t\t\t\taffine_input_dim=self.affine_input_dim,\n",
    "\t\t\t\t\t\t\tMODULES=MODULES)\n",
    "\t\t\t\t]]\n",
    "\n",
    "\t\t\t\tif index + 1 in attn_g_loc and apply_attn:\n",
    "\t\t\t\t\tself.blocks += [[SelfAttention(self.out_dims[index], is_generator=True, MODULES=MODULES)]]\n",
    "\n",
    "\t\t\tself.blocks = nn.ModuleList([nn.ModuleList(block) for block in self.blocks])\n",
    "\n",
    "\t\t\tself.bn4 = batchnorm_2d(in_features=self.out_dims[-1])\n",
    "\t\t\tself.activation = MODULES.g_act_fn\n",
    "\t\t\tself.conv2d5 = MODULES.g_conv2d(in_channels=self.out_dims[-1], out_channels=3, kernel_size=3, stride=1, padding=1)\n",
    "\t\t\tself.tanh = nn.Tanh()\n",
    "\n",
    "\t\t\tinit_weights(self.modules, g_init)\n",
    "\n",
    "\t\tdef forward(self, z, label, shared_label=None, eval=False):\n",
    "\t\t\taffine_list = []\n",
    "\t\t\twith torch.cuda.amp.autocast() if self.mixed_precision and not eval else dummy_context_mgr() as mp:\n",
    "\t\t\t\tif self.MODEL.info_type != \"N/A\":\n",
    "\t\t\t\t\tif self.MODEL.g_info_injection == \"concat\":\n",
    "\t\t\t\t\t\tz = self.info_mix_linear(z)\n",
    "\t\t\t\t\telif self.MODEL.g_info_injection == \"cBN\":\n",
    "\t\t\t\t\t\tz, z_info = z[:, :self.z_dim], z[:, self.z_dim:]\n",
    "\t\t\t\t\t\taffine_list.append(self.info_proj_linear(z_info))\n",
    "\n",
    "\t\t\t\tzs = torch.split(z, self.chunk_size, 1)\n",
    "\t\t\t\tz = zs[0]\n",
    "\t\t\t\tif self.g_cond_mtd != \"W/O\":\n",
    "\t\t\t\t\tif shared_label is None:\n",
    "\t\t\t\t\t\tshared_label = self.shared(label)\n",
    "\t\t\t\t\taffine_list.append(shared_label)\n",
    "\t\t\t\tif len(affine_list) == 0:\n",
    "\t\t\t\t\taffines = [item for item in zs[1:]]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\taffines = [torch.cat(affine_list + [item], 1) for item in zs[1:]]\n",
    "\n",
    "\t\t\t\tact = self.linear0(z)\n",
    "\t\t\t\tact = act.view(-1, self.in_dims[0], self.bottom, self.bottom)\n",
    "\t\t\t\tcounter = 0\n",
    "\t\t\t\tfor index, blocklist in enumerate(self.blocks):\n",
    "\t\t\t\t\tfor block in blocklist:\n",
    "\t\t\t\t\t\tif isinstance(block, SelfAttention):\n",
    "\t\t\t\t\t\t\tact = block(act)\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tact = block(act, affines[counter])\n",
    "\t\t\t\t\t\t\tcounter += 1\n",
    "\n",
    "\t\t\t\tact = self.bn4(act)\n",
    "\t\t\t\tact = self.activation(act)\n",
    "\t\t\t\tact = self.conv2d5(act)\n",
    "\t\t\t\tout = self.tanh(act)\n",
    "\t\t\treturn out\n",
    "\n",
    "\n",
    "\tclass DiscOptBlock(nn.Module):\n",
    "\t\tdef __init__(self, in_channels, out_channels, apply_d_sn, MODULES):\n",
    "\t\t\tsuper(BigResNet.DiscOptBlock, self).__init__()\n",
    "\t\t\tself.apply_d_sn = apply_d_sn\n",
    "\n",
    "\t\t\tself.conv2d0 = MODULES.d_conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\t\t\tself.conv2d1 = MODULES.d_conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\t\t\tself.conv2d2 = MODULES.d_conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "\t\t\tif not apply_d_sn:\n",
    "\t\t\t\tself.bn0 = MODULES.d_bn(in_features=in_channels)\n",
    "\t\t\t\tself.bn1 = MODULES.d_bn(in_features=out_channels)\n",
    "\n",
    "\t\t\tself.activation = MODULES.d_act_fn\n",
    "\t\t\tself.average_pooling = nn.AvgPool2d(2)\n",
    "\n",
    "\t\tdef forward(self, x):\n",
    "\t\t\tx0 = x\n",
    "\t\t\tx = self.conv2d1(x)\n",
    "\t\t\tif not self.apply_d_sn:\n",
    "\t\t\t\tx = self.bn1(x)\n",
    "\t\t\tx = self.activation(x)\n",
    "\n",
    "\t\t\tx = self.conv2d2(x)\n",
    "\t\t\tx = self.average_pooling(x)\n",
    "\n",
    "\t\t\tx0 = self.average_pooling(x0)\n",
    "\t\t\tif not self.apply_d_sn:\n",
    "\t\t\t\tx0 = self.bn0(x0)\n",
    "\t\t\tx0 = self.conv2d0(x0)\n",
    "\t\t\tout = x + x0\n",
    "\t\t\treturn out\n",
    "\n",
    "\n",
    "\tclass DiscBlock(nn.Module):\n",
    "\t\tdef __init__(self, in_channels, out_channels, apply_d_sn, MODULES, downsample=True):\n",
    "\t\t\tsuper(BigResNet.DiscBlock, self).__init__()\n",
    "\t\t\tself.apply_d_sn = apply_d_sn\n",
    "\t\t\tself.downsample = downsample\n",
    "\n",
    "\t\t\tself.activation = MODULES.d_act_fn\n",
    "\n",
    "\t\t\tself.ch_mismatch = False\n",
    "\t\t\tif in_channels != out_channels:\n",
    "\t\t\t\tself.ch_mismatch = True\n",
    "\n",
    "\t\t\tif self.ch_mismatch or downsample:\n",
    "\t\t\t\tself.conv2d0 = MODULES.d_conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\t\t\t\tif not apply_d_sn:\n",
    "\t\t\t\t\tself.bn0 = MODULES.d_bn(in_features=in_channels)\n",
    "\n",
    "\t\t\tself.conv2d1 = MODULES.d_conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\t\t\tself.conv2d2 = MODULES.d_conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "\t\t\tif not apply_d_sn:\n",
    "\t\t\t\tself.bn1 = MODULES.d_bn(in_features=in_channels)\n",
    "\t\t\t\tself.bn2 = MODULES.d_bn(in_features=out_channels)\n",
    "\n",
    "\t\t\tself.average_pooling = nn.AvgPool2d(2)\n",
    "\n",
    "\t\tdef forward(self, x):\n",
    "\t\t\tx0 = x\n",
    "\t\t\tif not self.apply_d_sn:\n",
    "\t\t\t\tx = self.bn1(x)\n",
    "\t\t\tx = self.activation(x)\n",
    "\t\t\tx = self.conv2d1(x)\n",
    "\n",
    "\t\t\tif not self.apply_d_sn:\n",
    "\t\t\t\tx = self.bn2(x)\n",
    "\t\t\tx = self.activation(x)\n",
    "\t\t\tx = self.conv2d2(x)\n",
    "\t\t\tif self.downsample:\n",
    "\t\t\t\tx = self.average_pooling(x)\n",
    "\n",
    "\t\t\tif self.downsample or self.ch_mismatch:\n",
    "\t\t\t\tif not self.apply_d_sn:\n",
    "\t\t\t\t\tx0 = self.bn0(x0)\n",
    "\t\t\t\tx0 = self.conv2d0(x0)\n",
    "\t\t\t\tif self.downsample:\n",
    "\t\t\t\t\tx0 = self.average_pooling(x0)\n",
    "\t\t\tout = x + x0\n",
    "\t\t\treturn out\n",
    "\n",
    "\n",
    "\tclass Discriminator(nn.Module):\n",
    "\t\tdef __init__(self, img_size, d_conv_dim, apply_d_sn, apply_attn, attn_d_loc, d_cond_mtd, aux_cls_type, d_embed_dim, normalize_d_embed,\n",
    "\t\t\t\t\tnum_classes, d_init, d_depth, MODULES, MODEL):\n",
    "\t\t\tsuper(BigResNet.Discriminator, self).__init__()\n",
    "\t\t\td_in_dims_collection = {\n",
    "\t\t\t\t\"32\": [3] + [d_conv_dim * 2, d_conv_dim * 2, d_conv_dim * 2],\n",
    "\t\t\t\t\"64\": [3] + [d_conv_dim, d_conv_dim * 2, d_conv_dim * 4, d_conv_dim * 8],\n",
    "\t\t\t\t\"128\": [3] + [d_conv_dim, d_conv_dim * 2, d_conv_dim * 4, d_conv_dim * 8, d_conv_dim * 16],\n",
    "\t\t\t\t\"256\": [3] + [d_conv_dim, d_conv_dim * 2, d_conv_dim * 4, d_conv_dim * 8, d_conv_dim * 8, d_conv_dim * 16],\n",
    "\t\t\t\t\"512\": [3] + [d_conv_dim, d_conv_dim, d_conv_dim * 2, d_conv_dim * 4, d_conv_dim * 8, d_conv_dim * 8, d_conv_dim * 16]\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\td_out_dims_collection = {\n",
    "\t\t\t\t\"32\": [d_conv_dim * 2, d_conv_dim * 2, d_conv_dim * 2, d_conv_dim * 2],\n",
    "\t\t\t\t\"64\": [d_conv_dim, d_conv_dim * 2, d_conv_dim * 4, d_conv_dim * 8, d_conv_dim * 16],\n",
    "\t\t\t\t\"128\": [d_conv_dim, d_conv_dim * 2, d_conv_dim * 4, d_conv_dim * 8, d_conv_dim * 16, d_conv_dim * 16],\n",
    "\t\t\t\t\"256\": [d_conv_dim, d_conv_dim * 2, d_conv_dim * 4, d_conv_dim * 8, d_conv_dim * 8, d_conv_dim * 16, d_conv_dim * 16],\n",
    "\t\t\t\t\"512\":\n",
    "\t\t\t\t[d_conv_dim, d_conv_dim, d_conv_dim * 2, d_conv_dim * 4, d_conv_dim * 8, d_conv_dim * 8, d_conv_dim * 16, d_conv_dim * 16]\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\td_down = {\n",
    "\t\t\t\t\"32\": [True, True, False, False],\n",
    "\t\t\t\t\"64\": [True, True, True, True, False],\n",
    "\t\t\t\t\"128\": [True, True, True, True, True, False],\n",
    "\t\t\t\t\"256\": [True, True, True, True, True, True, False],\n",
    "\t\t\t\t\"512\": [True, True, True, True, True, True, True, False]\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\tself.d_cond_mtd = d_cond_mtd\n",
    "\t\t\tself.aux_cls_type = aux_cls_type\n",
    "\t\t\tself.normalize_d_embed = normalize_d_embed\n",
    "\t\t\tself.num_classes = num_classes\n",
    "\t\t\tself.mixed_precision = False\n",
    "\t\t\tself.in_dims = d_in_dims_collection[str(img_size)]\n",
    "\t\t\tself.out_dims = d_out_dims_collection[str(img_size)]\n",
    "\t\t\tself.MODEL = MODEL\n",
    "\t\t\tdown = d_down[str(img_size)]\n",
    "\n",
    "\t\t\tself.blocks = []\n",
    "\t\t\tfor index in range(len(self.in_dims)):\n",
    "\t\t\t\tif index == 0:\n",
    "\t\t\t\t\tself.blocks += [[\n",
    "\t\t\t\t\t\tBigResNet.DiscOptBlock(in_channels=self.in_dims[index], out_channels=self.out_dims[index], apply_d_sn=apply_d_sn, MODULES=MODULES)\n",
    "\t\t\t\t\t]]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tself.blocks += [[\n",
    "\t\t\t\t\t\tBigResNet.DiscBlock(in_channels=self.in_dims[index],\n",
    "\t\t\t\t\t\t\t\tout_channels=self.out_dims[index],\n",
    "\t\t\t\t\t\t\t\tapply_d_sn=apply_d_sn,\n",
    "\t\t\t\t\t\t\t\tMODULES=MODULES,\n",
    "\t\t\t\t\t\t\t\tdownsample=down[index])\n",
    "\t\t\t\t\t]]\n",
    "\n",
    "\t\t\t\tif index + 1 in attn_d_loc and apply_attn:\n",
    "\t\t\t\t\tself.blocks += [[SelfAttention(self.out_dims[index], is_generator=False, MODULES=MODULES)]]\n",
    "\n",
    "\t\t\tself.blocks = nn.ModuleList([nn.ModuleList(block) for block in self.blocks])\n",
    "\n",
    "\t\t\tself.activation = MODULES.d_act_fn\n",
    "\n",
    "\t\t\t# linear layer for adversarial training\n",
    "\t\t\tif self.d_cond_mtd == \"MH\":\n",
    "\t\t\t\tself.linear1 = MODULES.d_linear(in_features=self.out_dims[-1], out_features=1 + num_classes, bias=True)\n",
    "\t\t\telif self.d_cond_mtd == \"MD\":\n",
    "\t\t\t\tself.linear1 = MODULES.d_linear(in_features=self.out_dims[-1], out_features=num_classes, bias=True)\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.linear1 = MODULES.d_linear(in_features=self.out_dims[-1], out_features=1, bias=True)\n",
    "\n",
    "\t\t\t# double num_classes for Auxiliary Discriminative Classifier\n",
    "\t\t\tif self.aux_cls_type == \"ADC\":\n",
    "\t\t\t\tnum_classes = num_classes * 2\n",
    "\n",
    "\t\t\t# linear and embedding layers for discriminator conditioning\n",
    "\t\t\tif self.d_cond_mtd == \"AC\":\n",
    "\t\t\t\tself.linear2 = MODULES.d_linear(in_features=self.out_dims[-1], out_features=num_classes, bias=False)\n",
    "\t\t\telif self.d_cond_mtd == \"PD\":\n",
    "\t\t\t\tself.embedding = MODULES.d_embedding(num_classes, self.out_dims[-1])\n",
    "\t\t\telif self.d_cond_mtd in [\"2C\", \"D2DCE\"]:\n",
    "\t\t\t\tself.linear2 = MODULES.d_linear(in_features=self.out_dims[-1], out_features=d_embed_dim, bias=True)\n",
    "\t\t\t\tself.embedding = MODULES.d_embedding(num_classes, d_embed_dim)\n",
    "\n",
    "\t\t\t# linear and embedding layers for evolved classifier-based GAN\n",
    "\t\t\tif self.aux_cls_type == \"TAC\":\n",
    "\t\t\t\tif self.d_cond_mtd == \"AC\":\n",
    "\t\t\t\t\tself.linear_mi = MODULES.d_linear(in_features=self.out_dims[-1], out_features=num_classes, bias=False)\n",
    "\t\t\t\telif self.d_cond_mtd in [\"2C\", \"D2DCE\"]:\n",
    "\t\t\t\t\tself.linear_mi = MODULES.d_linear(in_features=self.out_dims[-1], out_features=d_embed_dim, bias=True)\n",
    "\t\t\t\t\tself.embedding_mi = MODULES.d_embedding(num_classes, d_embed_dim)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\traise NotImplementedError\n",
    "\n",
    "\t\t\t# Q head network for infoGAN\n",
    "\t\t\tif self.MODEL.info_type in [\"discrete\", \"both\"]:\n",
    "\t\t\t\tout_features = self.MODEL.info_num_discrete_c*self.MODEL.info_dim_discrete_c\n",
    "\t\t\t\tself.info_discrete_linear = MODULES.d_linear(in_features=self.out_dims[-1], out_features=out_features, bias=False)\n",
    "\t\t\tif self.MODEL.info_type in [\"continuous\", \"both\"]:\n",
    "\t\t\t\tout_features = self.MODEL.info_num_conti_c\n",
    "\t\t\t\tself.info_conti_mu_linear = MODULES.d_linear(in_features=self.out_dims[-1], out_features=out_features, bias=False)\n",
    "\t\t\t\tself.info_conti_var_linear = MODULES.d_linear(in_features=self.out_dims[-1], out_features=out_features, bias=False)\n",
    "\n",
    "\t\t\tif d_init:\n",
    "\t\t\t\tinit_weights(self.modules, d_init)\n",
    "\n",
    "\t\tdef forward(self, x, label, eval=False, adc_fake=False):\n",
    "\t\t\twith torch.cuda.amp.autocast() if self.mixed_precision and not eval else dummy_context_mgr() as mp:\n",
    "\t\t\t\tembed, proxy, cls_output = None, None, None\n",
    "\t\t\t\tmi_embed, mi_proxy, mi_cls_output = None, None, None\n",
    "\t\t\t\tinfo_discrete_c_logits, info_conti_mu, info_conti_var = None, None, None\n",
    "\t\t\t\th = x\n",
    "\t\t\t\tfor index, blocklist in enumerate(self.blocks):\n",
    "\t\t\t\t\tfor block in blocklist:\n",
    "\t\t\t\t\t\th = block(h)\n",
    "\t\t\t\tbottom_h, bottom_w = h.shape[2], h.shape[3]\n",
    "\t\t\t\th = self.activation(h)\n",
    "\t\t\t\th = torch.sum(h, dim=[2, 3])\n",
    "\n",
    "\t\t\t\t# adversarial training\n",
    "\t\t\t\tadv_output = torch.squeeze(self.linear1(h))\n",
    "\n",
    "\t\t\t\t# make class labels odd (for fake) or even (for real) for ADC\n",
    "\t\t\t\tif self.aux_cls_type == \"ADC\":\n",
    "\t\t\t\t\tif adc_fake:\n",
    "\t\t\t\t\t\tlabel = label*2 + 1\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tlabel = label*2\n",
    "\n",
    "\t\t\t\t# forward pass through InfoGAN Q head\n",
    "\t\t\t\tif self.MODEL.info_type in [\"discrete\", \"both\"]:\n",
    "\t\t\t\t\tinfo_discrete_c_logits = self.info_discrete_linear(h/(bottom_h*bottom_w))\n",
    "\t\t\t\tif self.MODEL.info_type in [\"continuous\", \"both\"]:\n",
    "\t\t\t\t\tinfo_conti_mu = self.info_conti_mu_linear(h/(bottom_h*bottom_w))\n",
    "\t\t\t\t\tinfo_conti_var = torch.exp(self.info_conti_var_linear(h/(bottom_h*bottom_w)))\n",
    "\n",
    "\t\t\t\t# class conditioning\n",
    "\t\t\t\tif self.d_cond_mtd == \"AC\":\n",
    "\t\t\t\t\tif self.normalize_d_embed:\n",
    "\t\t\t\t\t\tfor W in self.linear2.parameters():\n",
    "\t\t\t\t\t\t\tW = F.normalize(W, dim=1)\n",
    "\t\t\t\t\t\th = F.normalize(h, dim=1)\n",
    "\t\t\t\t\tcls_output = self.linear2(h)\n",
    "\t\t\t\telif self.d_cond_mtd == \"PD\":\n",
    "\t\t\t\t\tadv_output = adv_output + torch.sum(torch.mul(self.embedding(label), h), 1)\n",
    "\t\t\t\telif self.d_cond_mtd in [\"2C\", \"D2DCE\"]:\n",
    "\t\t\t\t\tembed = self.linear2(h)\n",
    "\t\t\t\t\tproxy = self.embedding(label)\n",
    "\t\t\t\t\tif self.normalize_d_embed:\n",
    "\t\t\t\t\t\tembed = F.normalize(embed, dim=1)\n",
    "\t\t\t\t\t\tproxy = F.normalize(proxy, dim=1)\n",
    "\t\t\t\telif self.d_cond_mtd == \"MD\":\n",
    "\t\t\t\t\tidx = torch.LongTensor(range(label.size(0))).to(label.device)\n",
    "\t\t\t\t\tadv_output = adv_output[idx, label]\n",
    "\t\t\t\telif self.d_cond_mtd in [\"W/O\", \"MH\"]:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\traise NotImplementedError\n",
    "\n",
    "\t\t\t\t# extra conditioning for TACGAN and ADCGAN\n",
    "\t\t\t\tif self.aux_cls_type == \"TAC\":\n",
    "\t\t\t\t\tif self.d_cond_mtd == \"AC\":\n",
    "\t\t\t\t\t\tif self.normalize_d_embed:\n",
    "\t\t\t\t\t\t\tfor W in self.linear_mi.parameters():\n",
    "\t\t\t\t\t\t\t\tW = F.normalize(W, dim=1)\n",
    "\t\t\t\t\t\tmi_cls_output = self.linear_mi(h)\n",
    "\t\t\t\t\telif self.d_cond_mtd in [\"2C\", \"D2DCE\"]:\n",
    "\t\t\t\t\t\tmi_embed = self.linear_mi(h)\n",
    "\t\t\t\t\t\tmi_proxy = self.embedding_mi(label)\n",
    "\t\t\t\t\t\tif self.normalize_d_embed:\n",
    "\t\t\t\t\t\t\tmi_embed = F.normalize(mi_embed, dim=1)\n",
    "\t\t\t\t\t\t\tmi_proxy = F.normalize(mi_proxy, dim=1)\n",
    "\t\t\treturn {\n",
    "\t\t\t\t\"h\": h,\n",
    "\t\t\t\t\"adv_output\": adv_output,\n",
    "\t\t\t\t\"embed\": embed,\n",
    "\t\t\t\t\"proxy\": proxy,\n",
    "\t\t\t\t\"cls_output\": cls_output,\n",
    "\t\t\t\t\"label\": label,\n",
    "\t\t\t\t\"mi_embed\": mi_embed,\n",
    "\t\t\t\t\"mi_proxy\": mi_proxy,\n",
    "\t\t\t\t\"mi_cls_output\": mi_cls_output,\n",
    "\t\t\t\t\"info_discrete_c_logits\": info_discrete_c_logits,\n",
    "\t\t\t\t\"info_conti_mu\": info_conti_mu,\n",
    "\t\t\t\t\"info_conti_var\": info_conti_var\n",
    "\t\t\t}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize the copied generator's parameters to be source parameters.\n",
      "Files already downloaded and verified\n",
      "[INFO] 2023-01-25 02:31:39 > Epoch: 6 | Step:    500 Progress: 0.2% Elapsed: 0:06:05 Gen_loss: 2.383 Dis_loss: 3.525 Cls_loss: 4.336 \n",
      "[INFO] 2023-01-25 02:37:49 > Epoch: 12 | Step:   1000 Progress: 0.5% Elapsed: 0:12:15 Gen_loss: 2.021 Dis_loss: 3.619 Cls_loss: 4.23 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m trainer \u001b[39m=\u001b[39m Trainer()\n\u001b[0;32m     45\u001b[0m trainer\u001b[39m.\u001b[39mload(cfg, \u001b[39m'\u001b[39m\u001b[39mReACGAN\u001b[39m\u001b[39m'\u001b[39m, BigResNet)\n\u001b[1;32m---> 46\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "Cell \u001b[1;32mIn[51], line 432\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[39mwhile\u001b[39;00m step \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfgs\u001b[39m.\u001b[39mOPTIMIZATION\u001b[39m.\u001b[39mtotal_steps:\n\u001b[0;32m    431\u001b[0m \t\u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfgs\u001b[39m.\u001b[39mOPTIMIZATION\u001b[39m.\u001b[39md_first:\n\u001b[1;32m--> 432\u001b[0m \t\treal_cond_loss, dis_acml_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_discriminator(current_step\u001b[39m=\u001b[39;49mstep)\n\u001b[0;32m    433\u001b[0m \t\tgen_acml_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_generator(current_step\u001b[39m=\u001b[39mstep)\n\u001b[0;32m    434\u001b[0m \t\u001b[39melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[51], line 47\u001b[0m, in \u001b[0;36mTrainer.train_discriminator\u001b[1;34m(self, current_step)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mOPTIMIZATION\u001b[39m.\u001b[39md_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     46\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast() \u001b[39mas\u001b[39;00m mpc:\n\u001b[1;32m---> 47\u001b[0m \treal_images \u001b[39m=\u001b[39m real_image_basket[batch_counter]\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m     48\u001b[0m \treal_labels \u001b[39m=\u001b[39m real_label_basket[batch_counter]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     50\u001b[0m \tfake_images, fake_labels, fake_images_eps, trsp_cost, ws, _, _ \u001b[39m=\u001b[39m generate_images(\n\u001b[0;32m     51\u001b[0m \t\tz_prior \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMODEL\u001b[39m.\u001b[39mz_prior,\n\u001b[0;32m     52\u001b[0m \t\ttruncation_factor \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1.0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \t\tcal_trsp_cost \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLOSS\u001b[39m.\u001b[39mapply_lo \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \t)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cfg = Configurations({\n",
    "    'DATA': {\n",
    "\t\t'name': 'CIFAR10',\n",
    "\t\t'img_size': 32,\n",
    "\t\t'num_classes': 10\n",
    "\t},\n",
    "    'MODEL': {\n",
    "\t\t'backbone': \"big_resnet\",\n",
    "\t\t'g_cond_mtd': \"cBN\",\n",
    "\t\t'd_cond_mtd': \"D2DCE\",\n",
    "\t\t'normalize_d_embed': True,\n",
    "\t\t'd_embed_dim': 512,\n",
    "\t\t'apply_g_sn': True,\n",
    "\t\t'apply_d_sn': True,\n",
    "\t\t'apply_attn': True,\n",
    "\t\t'attn_g_loc': [2],\n",
    "\t\t'attn_d_loc': [1],\n",
    "\t\t'z_dim': 80,\n",
    "\t\t'g_shared_dim': 128,\n",
    "\t\t'g_conv_dim': 96,\n",
    "\t\t'd_conv_dim': 96,\n",
    "\t\t'apply_g_ema': True,\n",
    "\t\t'g_ema_decay': 0.9999,\n",
    "\t\t'g_ema_start': 1000\n",
    "\t},\n",
    "\t'LOSS': {\n",
    "\t\t'adv_loss': 'hinge',\n",
    "\t\t'cond_lambda': 0.5,\n",
    "\t\t'm_p': 0.98,\n",
    "\t\t'temperature': 0.5\n",
    "\t},\n",
    "    'OPTIMIZATION': {\n",
    "\t\t'batch_size': 128,\n",
    "\t\t'g_lr': 0.00028284271,\n",
    "\t\t'd_lr': 0.00028284271,\n",
    "\t\t'total_steps': 200000\n",
    "\t},\n",
    "\t'AUG': {\n",
    "\t\t'apply_diffaug': True,\n",
    "\t\t'diffaug_type': 'diffaug'\n",
    "\t}\n",
    "})\n",
    "\n",
    "trainer = Trainer()\n",
    "trainer.load(cfg, 'ReACGAN', BigResNet)\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying **Self-Attention**\n",
    "What is self-attention? Traditional convolution layers are local, in the sense that the scope of interactions between inputs is restricted. If the kernel size is less than the image (which it usually is) then one can know that the top left region on an image will only have some interaction with the bottom right region after a certain number of layers. The attention mechanism introduces **global receptive field**. Specifically, it first identifies the dependencies between different tokens on an input, by performing a dot product between two weighted and position-encoded vector embeddings on the input. This leads to a weight matrix that's essentially **calculated on the fly**. The method of producing tokens are generally dependent on the architecture, with the novel Vision Transformer (ViT) using linear layers, while other transformer architectures such as Convolutional Vision Transformers use Convolution layers to generate tokens. Formally, attention is calculated by:\n",
    "$$\\text{Att}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "where $Q, K, V$ are the token matrices.\n",
    "\n",
    "The authors of the SAGAN (Self Attention GANs) paper showed that their networks were able to:\n",
    "> leveraging complementary features in distant portions of the image rather than local regions of fixed shape to generate consistent objects/scenarios.\n",
    "(Zhang et al., 2019)\n",
    "\n",
    "Additionally, a somewhat subtle component in the SAGAN architecture is **Conditional Batch Normalization**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying **Projection Discriminator**\n",
    "As of 2018, most frameworks that describe the architecture of cGANs simply performed some concatenation to add the conditional information $y$ to either the feature vector or at some middle layer (Miyato and Koyama, 2018).\n",
    "\n",
    "<img src=\"images/projection.jpg\" width=900><br />\n",
    "<em>(Miyato and Koyama, 2018)</em>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying **Two-Time Update Rule**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying **BigGAN Architecture**\n",
    "In fact the network we are currently using is similar to the BigGAN architecture. Thus, we attempt to "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying **MultiHinge Loss**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying <strong>Data-to-Data Cross Entropy</strong>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying <strong>DiffAugment</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying <strong>Auxiliary Discriminative Classifier</strong>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning with **Bayesian Optimization**\n",
    "**Why should we hyperparameter tune?**\n",
    "\n",
    "**Hyperparameter tuning GANs**\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Analysis** of Final Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1042389d7c82dd1bd8119cafb1c36337ac9bb25498b1a7ccb724fef191fad074"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
