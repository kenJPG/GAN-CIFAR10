{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== #\n",
    "# Author: Kenneth Chen #\n",
    "# Student ID: 2100072  #\n",
    "# ==================== #\n",
    "\n",
    "# Notebook Configuration\n",
    "GPU_COUNT = 1\n",
    "WINDOWS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas\n",
    "!pip install -q seaborn\n",
    "!pip install -q matplotlib\n",
    "!pip install -q neptune-client"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cifar10.PNG\" width=700>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks - CIFAR10\n",
    "> Can we create a strong generative model for CIFAR10 using the GAN architecture?\n",
    "\n",
    "## Background Research üìñ\n",
    "\n",
    "### Introduction üí°\n",
    "The ultimate goal of Deep Learning is to be able to create a function that can <strong>effectively model any form of data distribution</strong>. History has time and time again displayed the impressive success of discriminators, models that learn to divide the data distribution/map a high dimensional vector to one that is lower (Goodfellow et al., 2014). For instance, tasks such as Image Classification are one type of discriminative modelling, as the high dimensional images are mapped into low dimensional probabilities of labels.\n",
    "\n",
    "What about generative modelling? In generative modelling, the goal is instead given a data distribution to learn from, to produce or <strong>generate new examples</strong> that follow this distribution but still aim to be unique. Thus, a high performing generative model should be able to create examples that are both <strong>plausible</strong> (in that one can recognize what the generated example is supposed to be of) <strong>and indistinguishable</strong> from real data examples (Brownlee, 2019). Generative models can be Unsupervised or Semi-Supervised, depending on the exact task that one is trying to tackle. \n",
    "\n",
    "There are different approaches to network architectures when it comes to trying to achieve Generative Models:\n",
    "<ul>\n",
    "\t<li>Generative Adversarial Networks (GAN) </li>\n",
    "\t<li>Diffusion Models</li>\n",
    "\t<li>Variational Auto Encoders (VAE)</li>\n",
    "</ul>\n",
    "\n",
    "<strong>GANs ‚öîÔ∏è</strong> <br />\n",
    "GANs are the main focus of this notebook. Proposed by Ian Goodfellow in 2014, it became one of the more popular types of Generative Models used. For instance, the commonly known website <a href=\"thispersondoesnotexist.com\">thispersondoesnotexist.com</a> uses the StyleGAN2 architecture (Karras et al., 2020), to generate high fidelity images of humans. The idea for GANs is that <strong>there are two networks that work against each other in a game</strong>, where they try to one up each other. Thus, this leads to improvement in both networks. More details are discussed under \"What's inside a GAN? üîç\".\n",
    "\n",
    "<strong>Diffusion Models ‚ú®</strong><br /> \n",
    "These are the models that have been not only been successful, but widely popular as well. For instance, OpenAI Dall-E, Google Imagen, Stable Diffusion, Midjourney are models that fall under the category of Diffusion Models. (Muppalla and Hendryx, 2022). From a high level, it works like so:\n",
    "<ul>\n",
    "\t<li>Noise is added to original images</li>\n",
    "\t<li>Noise is procedurally added until image is all noise</li>\n",
    "\t<li>The model then learns to remove the noise</li>\n",
    "\t<li>Guidance can be added in the form of e.g. text-to-image, to provide direction of the generation process</li>\n",
    "</ul>\n",
    "\n",
    "<img src=\"images/diffusion1.jpg\" width=400/><br />\n",
    "*Noise is procedurally added to the image. Image Credit: (Muppalla and Hendryx, 2022)*\n",
    "\n",
    "<img src=\"images/diffusion2.jpg\" width=400/><br />\n",
    "*Model attempts to recreate the image. Image Credit: (Muppalla and Hendryx, 2022)*\n",
    "\n",
    "<strong>Variational Auto Encoders üé≤</strong><br />\n",
    "We first take a look at what Auto Encoders are. An Auto Encoder is trained for it to learn to copy the input to the output (Goodfellow, Bengio and Courville, 2016). This is done by having an <strong>encoder map the image</strong> to a compressed representation of the image (the inner nodes), to which the decoder <strong>uses this compressed representation</strong> to generate an image similar to the original. Note that the trick is to <strong>restrict the number of inner nodes</strong> inside the network, such that it is <strong>not able to generate a 1-to-1 copy</strong>. This way, it's forced to learn the most promiment of features to recognize, using the limited number of nodes.\n",
    "\n",
    "The idea with Variational Auto Encoders is that <strong>instead of the encoder just mapping the image to a compressed representation</strong> (aka latent vector), we instead <strong>learn the distribution that the latent vector can take on</strong>. Using this, we can then randomly sample from the learned latent distribution, for the decoder to give us a newly and controlled generated image.\n",
    "\n",
    "<img src=\"images/vae.PNG\" width=400/><br />\n",
    "*VAE Architecture. We note the learning of the latent distribution. Image Credit: (Rocca, 2019)*\n",
    "\n",
    "\n",
    "### What's inside a GAN? üîç\n",
    "In a GAN, there are in fact two networks, a <strong>generator</strong> and a <strong>discriminator</strong> that improve each other by competing in a game scenario (Goodfellow, Bengio and Courville, 2016). The aim is to use the well established field of discriminators to assist the generator. The goal of the generator is to <strong>create realistic images</strong> that appear to be from the distribution of the training images, where as the goal of the discriminator is to determine <strong>if a given image is from the data distribution</strong>. The process goes as follows:\n",
    "<ol>\n",
    "\t<li>Generator creates images</li>\n",
    "\t<li>Discriminator learns to distinguish real vs fake from a set of real images and these newly generated images</li>\n",
    "\t<li>Using the updated Discriminator, Generator learns to trick to trick the Discriminator</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "### Types of GANs üçê\n",
    "There many different types of GANs, however I believe the most differing pair is the Vanilla GAN and the Conditional GAN. The Vanilla GAN is what was proposed by Ian Goodfellow in 2014, which consists of the basic architecture with multi-layer perceptrons (MLPs). Conditional GANs are different in the aspect that one can provide additional information to the model, which could be thought of as a form of guidance similar to diffusion models.\n",
    "\n",
    "<img src=\"images/vanilla.jpg\" width=400><br/>\n",
    "*Vanilla GAN architecture (Tewari, N.d.)*\n",
    "\n",
    "<img src=\"images/conditional.jpg\" width=400><br/>\n",
    "\n",
    "*Conditional GAN architecture (Tewari, N.d.)*\n",
    "\n",
    "We observe that there is an extra component of `y`, which represents the extra information presented both to the Generator and Discriminator. This extra information is usually in the form of class labels to allow one to possess control over the output, but it can be extended to different modal data, even something such as text (in which case it needs some sort of text processor).\n",
    "\n",
    "One may think of Conditional GAN as a *\"supervised\" version* of Vanilla GANs. \n",
    "\n",
    "### Uses of GANs üß§\n",
    "\n",
    "As a GAN is a generative model, there are a large number of applications of GANs (Brownlee, 2019). Personally, I find it interesting how the idea of GANs can be adjusted for any modal of data, as long as the architecture for the encoder and decoders are adjusted accordingly. Here are a few areas of GANs I believe are quite intriguing:\n",
    "<ul>\n",
    "\t<li>Time Series</li>\n",
    "\t<li>Image Generation</li>\n",
    "\t<li>Music Generation</li>\n",
    "\t<li>Audio Generation</li>\n",
    "\t<li>Style Transfer (e.g. winter photo to summer photo, jazz to classical music)</li>\n",
    "</ul>\n",
    "\n",
    "Among them, I think Audio Generation stands out to me the most. The idea of using GANs in music composition sounds like a difficult challenge, but also an impressive feat if one could pull it off.\n",
    "\n",
    "### The difficulty with GANs üß©\n",
    "GANs are \n",
    "Convex.\n",
    " \n",
    "Mode collapse\n",
    "Stability - hyperparameter tuning is very important versus discriminator models where hyperparameters more often than not determine .\n",
    "\n",
    "### Loss Functions üèì\n",
    "Different Loss functions, main one being KL Divergence.\n",
    "Explain how KL Divergence works.\n",
    "There are papers claiming that novel loss functions improve stability.\n",
    "There are also papers https://arxiv.org/abs/1811.09567 that claim that loss functions don't really matter.\n",
    "This is an area of research I will attempt to explore in the notebook as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing GAN üíª\n",
    "### Objectives üñäÔ∏è\n",
    "We identify the tasks and objectives we want to meet, which will be used as a guide throughout the development of our GAN.\n",
    "<ol>\n",
    "\t<li>Explore the CIFAR10 dataset</li>\n",
    "\t<li>Implement and evaluate to find the best performing model</li>\n",
    "\t<li>Analyse the final model</li>\n",
    "</ol>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "The necessary libraries are imported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "We define some utility functions below that will ease and help us with our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc_data(data, loc):\n",
    "\tdatacopy = copy.deepcopy(data)\n",
    "\tarr = np.array(datacopy.loc[loc].drop('label'))\n",
    "\tlabel = datacopy.loc[loc]['label']\n",
    "\troot = int(len(arr) ** 0.5)\n",
    "\tarr.resize((root, root))\n",
    "\treturn label, arr\n",
    "\n",
    "def imshow(arr: list, label: list = None, figsize=None, shape = (32, 32, 3), is_int = None):\n",
    "\tif is_int == None:\n",
    "\t\tif type(arr[0]) == torch.Tensor:\n",
    "\t\t\tis_int = (arr[0].detach().cpu().numpy() > 1).sum() > 0\n",
    "\t\telse:\n",
    "\t\t\tis_int = (arr[0] > 1).sum() > 0\n",
    "\tif label == None:\n",
    "\t\tlabel = [''] * len(arr)\n",
    "\n",
    "\theight = int(len(arr) ** 0.5)\n",
    "\twidth = math.ceil(len(arr) / height)\n",
    "\n",
    "\tif figsize == None:\n",
    "\t\tfig = plt.figure()\n",
    "\telse:\n",
    "\t\tfig = plt.figure(figsize=figsize)\n",
    "\tfor i in range(height):\n",
    "\t\tfor j in range(width):\n",
    "\t\t\tax = fig.add_subplot(height, width, i * height + j + 1)\n",
    "\t\t\tax.grid(False)\n",
    "\t\t\tax.set_xticks([])\n",
    "\t\t\tax.set_yticks([])\n",
    "\t\t\tshow = arr[i * height + j]\n",
    "\t\t\tif type(arr[i * height + j]) != torch.Tensor:\n",
    "\t\t\t\tshow = torch.Tensor(show)\n",
    "\t\t\t\t# ax.imshow((arr[i * height + j].squeeze(0).cpu().permute(1, 2, 0) / 255).type(torch.uint8 if is_int else float))\n",
    "\t\t\t# if (show.shape[0] == 1):\n",
    "\t\t\t# \tax.imshow((show.squeeze(0).cpu()).type(torch.uint8 if is_int else torch.float), cmap='gray')\n",
    "\t\t\t# else:\n",
    "\t\t\tif len(show.squeeze(0).cpu().shape) == 2:\n",
    "\t\t\t\tax.imshow((show.squeeze(0).detach().cpu()).type(torch.uint8 if is_int else torch.float), cmap='gray')\n",
    "\t\t\telse:\n",
    "\t\t\t\tax.imshow((show.squeeze(0).detach().cpu().permute(1,2,0)).type(torch.uint8 if is_int else torch.float))\n",
    "\t\t\tax.set_title(label[i * height + j])\n",
    "\n",
    "def df_to_tensor(df, shape = (28, 28)):\n",
    "\treturn torch.tensor(df.values.reshape((-1, *shape)), dtype=torch.float32)\n",
    "\n",
    "def preprocess(df):\n",
    "\treturn df.copy() / 255"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:16:33) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d19efe0496419facc97fd7ea20d5d3a0b9c1bdc9473c8f87e1305d92cacb204"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
